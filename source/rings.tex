\chapter{Rings}\label{sec:rings}

\section{Basic Definitions}

\begin{definition}
    A ring is a quintuple \((R,+,\cdot,0,1)\),
    where \(R \neq \emptyset\) is a set
    equipped with two operations \(+,\cdot\),
    each with their identity \(0,1\) respectively,
    with the following three properties:
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item \((R,+,0)\) forms an abelian group;
        \item \((R - \{0\},\cdot,1)\) forms a monoid; and
        \item \(\forall \{a,x,y\} \subset R\),
            the distributive laws \(a(x+y) = ax + ay\)
            and \((x+y)a = xa + ya\) hold.
    \end{enumerate}
\end{definition}
\begin{proposition}
    In a nondegenerate ring \(R\), where \(R \neq \{0\}\), \(0 \neq 1\).
\end{proposition}
\begin{proof}
    For all \(x \in R\), we know \(0 = x + (-x)\).
    Suppose, by way of contradiction, that \(0 = 1\).
    Then we have \(x = 1x = (x+(-x))x = x^2 + (-x^2) = 0\),
    implying that \(R = \{0\}\).
\end{proof}

\begin{definition}
    Suppose \((R,+,\cdot,0,1)\) is a ring.
    Then \(S \subseteq R\) is a subring if:
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item \((S,+,0) \subseteq (R,+,0)\) is a subgroup; and
        \item \((S - \{0\},\cdot,1) \subseteq (R - \{0\},\cdot,1)\) is a submonoid.
    \end{enumerate}
\end{definition}

\begin{lemma}\label{lem:intersection-subring}
    Suppose \(R\) is a ring, and \(S_i \subseteq R\) subrings.
    Then \(\bigcap_i S_i\) is also a subring of \(R\).
\end{lemma}
\begin{proof}
    Lemma~\ref{lem:intersection-subgroup} gives us that
    the intersection of additive groups are subgroups,
    and the intersection of multiplicative monoids are submonoids.
\end{proof}

\begin{definition}
    Suppose \(R\) is a ring, and \(X \subseteq R\) is a subset.
    The subring generated by \(X\)
    is the intersection of all subrings \(R'\) that contain \(X\),
    that is, \(S = \bigcap_{R \supset R' \supset X} R'\).
\end{definition}
\begin{proposition}\label{prop:subset-generated-subring}
    The subring generated by a subset
    is the ring of all possible sums and products of elements of the subset.
\end{proposition}
\begin{proof}
    Apply Proposition~\ref{prop:subset-generated-subgroup} twice,
    to both the additive group and the multiplicative monoid.
\end{proof}

\begin{definition}
    A ring homomorphism is a function \(\func{f}{R_1}{R_2}\)
    where \(R_1,R_2\) are rings,
    with the properties that:
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item \(\func{f}{(R_1,+,0)}{(R_2,+,0)}\) is a group homomorphism; and
        \item \(\func{f}{(R_1 - \{0\},\cdot,1)}{(R_2 - \{0\},\cdot,1)}\)
            is a monoid homomorphism.
    \end{enumerate}
\end{definition}
\begin{proposition}
    For every ring \(R\),
    there exists a unique homomorphism \(\func{\zeta}{\bZ}{R}\).
\end{proposition}
\begin{proof}
    We attempt to define \(\zeta\) starting with the simplest requirements.
    We know that \(\zeta\) must map \(0 \mapsto 0_R\) and \(1 \mapsto 1_R\),
    By addition, and inversion,
    we can clearly define for any \(n \in \bZ\),
    \begin{equation*}
        \zeta(n) = \zeta(\underbrace{1+1+\cdots+1}_\textrm{\(n\) times})
    = \underbrace{1_R + 1_R + \cdots + 1_R}_\textrm{\(n\) times} = n_R
    \end{equation*}

    It is now sufficient to prove existence
    by checking that this homomorphism is valid under multiplication.
    \begin{equation*}
        \zeta(m)\zeta(n)
        = \underbrace{(1_R + 1_R + \cdots + 1_R)}_\textrm{\(m\) times}
        \underbrace{(1_R + 1_R + \cdots + 1_R)}_\textrm{\(n\) times}
        = \underbrace{1_R + 1_R + \cdots + 1_R}_\textrm{\(mn\) times}
        = \zeta(mn)
    \end{equation*}

    Now suppose, by way of contradiction,
    that there exists another homomorphism \(\func{\phi}{\bZ}{R}\).
    Then for them to be different,
    there exists some \(n \in \bZ\) such that \(\phi(n) \neq \zeta(n)\).
    But clearly, by definition of \(\zeta\),
    \begin{equation*}
        \phi(n) = \phi(\underbrace{1+1+\cdots+1}_\textrm{\(n\) times})
        = \underbrace{\phi(1)+\phi(1)+\cdots+\phi(1)}_\textrm{\(n\) times}
        = \underbrace{1_R + 1_R + \cdots + 1_R}_\textrm{\(n\) times}
        = \zeta(n)
    \end{equation*}
    which is a contradiction,
    since \(\phi(1) = 1_R\) by definition of ring homomorphism.
\end{proof}
\begin{definition}
    The homomorphism \(\func{\zeta}{\bZ}{R}\)
    is the canonical homomorphism.
\end{definition}
\begin{remark}
    In notation for rings,
    we might sometimes write some number \(n\)
    and an element \(x \in R\) as \(nx \in R\),
    which by strict definition means `adding up \(x\), \(n\) times',
    which makes \(nx = \zeta(n)x\).
\end{remark}

\begin{definition}
    If the multiplicative monoid \((R - \{0\},\cdot,1)\) is a group,
    i.e.\ there exists multiplicative inverses for every element,
    then \(R\) is a division ring.
\end{definition}
\begin{definition}
    If the multiplicative monoid \((R - \{0\},\cdot,1)\) is an abelian group,
    i.e.\ there exists multiplicative inverses,
    and multiplication is commutative,
    then \(R\) is a field.
\end{definition}
\begin{definition}
    If the multiplicative monoid \((R - \{0\},\cdot,1)\) is commutative,
    then \(R\) is a commutative ring.
\end{definition}
\begin{proposition}[Binomial Theorem]\label{prop:binom}
    In any commutative ring (and therefore a field),
    \({(x+y)}^n = \sum_{r=0}^n \binom{n}{r} x^r y^{n-r}\),
    where \(\binom{n}{r}\) is the usual binomial coefficient
    given by \(\binom{n}{r} = (n!)/(r!(n-r)!)\).
\end{proposition}
\begin{proof}
    % This is obvious and we will only give a sketch of the proof.
    % Clearly because of commutativity,
    % we can group all the terms with the same number of \(x\) and \(y\),
    % which add up to the binomial coefficient.
    % Use a combinatorics argument
    % for the definition of the coefficient itself.
    \sout{Simple proof by induction.}
    \textit{Fine, I'll give you the proof.}

    Clearly \(x+y = \binom{1}{0}x + \binom{1}{1}y\),
    as \(\binom{1}{0} = 1 = \binom{1}{1}\) by simple computation.

    Now, suppose the binomial theorem holds for case \(k\).
    Then we have
    \begin{align*}
        {(x+y)}^{k+1} &= x{(x+y)}^k + y{(x+y)}^k
        = \sum_{r=0}^k \binom{k}{r} x^{r+1} y^{k-r}
        + \sum_{r=0}^k \binom{k}{r} x^r y^{k-r+1} \\
        &= \sum_{r=1}^{k+1} \binom{k}{r} x^r y^{k-r+1}
        + \sum_{r=0}^k \binom{k}{r} x^r y^{k-r+1}
        = \sum_{r=0}^{k+1} \qty[\binom{k}{r}+\binom{k}{r-1}]x^r y^{k-r+1}
    \end{align*}
    since \(\binom{k}{r-1} = \binom{k}{k+1} = 0\) by usual definition.
    It is now sufficient to prove the sum of binomial coefficients.
    \begin{equation*}
        \binom{k}{r}+\binom{k}{r-1}
        = \frac{k!}{r!(k-r)!} + \frac{k!}{(r-1)!(k-r+1)!}
        = \frac{k!(k-r+1) + k!r}{r!(k-r+1)!}
        = \frac{k!(k+1)}{r!(k+1-r)!}
        = \frac{(k+1)!}{r!(k+1-r)!}
    \end{equation*}
\end{proof}

\begin{definition}
    Suppose \(R\) is a ring, and \(x \in R\) some element.
    \(x\) is a left zero divisor if there exists some \(y \neq 0\)
    such that \(xy = 0\);
    \(x\) is a right zero divisor if there exists some \(y \neq 0\)
    such that \(yx = 0\).
\end{definition}
\begin{definition}
    \(R\) is a domain if \(xy = 0\)
    implies \(x = 0\) or \(y = 0\) for all \(\{x,y\} \subset R\).
    That is, the only zero divisor is \(0 \in R\).
    If \(R\) is also a commutative ring,
    then it is an integral domain.
\end{definition}

\begin{definition}
    Suppose \(R\) is a ring, and \(x \in R\) some element.
    \(x\) is a unit if there exists \(y \in R\)
    such that \(xy = yx = 1_R\),
    i.e.\ there exists a multiplicative inverse.
    This implies that we can use cancellation when dealing with units,
    since we can multiply on both the left and the right
    its multiplicative inverse \(x^{-1} = y\).
\end{definition}
\begin{remark}
    The word `unit' here is significantly different
    as the word `unit' in analysis;
    units need not have a norm equal to 1 here,
    for example when in a field,
    every nonzero element has a multiplicative inverse by definition,
    and hence is a `unit' in algebra,
    but don't you dare write in your analysis homework
    that \(2\) is a unit.
\end{remark}
\begin{proposition}\label{prop:ring-unit-subgroup}
    \(R^\times\), the set of units in \(R\) is a subgroup
    of the multiplicative monoid \((R - \{0\},\cdot,1)\).
\end{proposition}
\begin{proof}
    Suppose \(x,y\) are units; then \(x^{-1},y^{-1}\) exist.
    By definition of a unit, \(0\) is not a unit,
    so all units are in \(R - \{0\}\).
    There is closure, since \({(xy)}^{-1} = y^{-1}x^{-1}\).
    Associativity is inherited,
    and \(1\) is its own inverse by definition.
    Lastly, it is obvious that \({(x^{-1})}^{-1} = x\),
    just by definition, so inverses exist.
\end{proof}

\begin{proposition}
    Suppose \(G\) is an abelian group.
    Then \(\End(G)\) the set of homomorphisms from \(G\) to \(G\)
    forms a ring.
\end{proposition}
\begin{proof}
    Suppose \(a,b,c \in \End(G)\).
    If we define \((a+b+c)(g) = a(g)+b(g)+c(g)\),
    then additive axioms immediately follow.
    Now let use define addition as function composition.
    Associativity immediately follows,
    and the identity homomorphism is obvious 1.
    Left distributivity can be seen by
    \begin{equation*}
        (a\circ(b+c))(g) = a(b(g)+c(g))
        = (a \circ b)(g) + (a \circ c)(g)
        = ((a \circ b) + (a \circ c))(g)
    \end{equation*}
    Right distributivity follows similarly.
\end{proof}


\section{Examples}\label{sec:matrix-rings}

\subsection*{Matrix Rings}

\begin{remark}
    We shall establish the following convention in this section:
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item \(a\) a lowercase symbol denotes a generic element in a set;
        \item \(\vec{a}\) an underlined symbol denotes a vector; and
        \item \(\vb{A}\) a bold symbol (usually uppercase)
            denotes a matrix quantity.
    \end{enumerate}
    Note that sometimes these definitions are blurry,
    as such quantities can be expressed in different ways;
    in that case, these notations are merely the way the author thinks of them.
\end{remark}

% \pagebreak

\begin{proposition}
    Suppose \(R\) is a ring.
    Then \(R^{n \times n}\),
    the set of \(n \times n\) matrices with entries in \(R\),
    forms a ring with entry-wise addition
    and the usual matrix multiplication.
\end{proposition}
\begin{proof}
    Check the ring axioms yourself.
\end{proof}
\begin{remark}
    We shall use bold fonts to represent matrices in this document.
    In particular, \(\vb{I} = \vb{I}_n\)
    is the \(n \times n\) identity matrix,
    and \(\vb{0} = \vb{0}_n\) is the \(n \times n\) zero matrix.
    \(\vb{e}_{ij}\) will represent a matrix
    with 1 at the \(i\)th row and \(j\)th column,
    with zeroes everywhere else;
    so we can write \((a_{ij}) = \sum_{ij} a_{ij}\vb{e}_{ij}\).
\end{remark}
\begin{remark}\label{rem:matrix-mult}
    For ease of reference,
    we shall write down the rules of matrix multiplication.
    Suppose \(\vb{A} = (a_{ij}) \in R^{m \times n}\)
    and \(\vb{B} = (b_{ij}) \in R^{n \times p}\),
    then \(\vb{C} = \vb{A}\vb{B} = (c_{ij}) \in R^{m \times p}\)
    is given by
    \begin{equation*}
        c_{ij} = \sum_{k=1}^n a_{ik}b_{kj}
    \end{equation*}
\end{remark}

\begin{definition}
    Suppose \(R\) is a commutative ring.
    A function \(\func{\det}{R^{n \times n}}{R}\) is called a determinant
    if it is defined by the following three rules:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(\det\vb{I} = 1\);
        \item If \(\sigma \in S_n\) is a row permutation,
            and \(\vb{A} \in R^{n \times n}\),
            then \(\det(\sigma\vb{A}) = \sgn(\sigma)\det(\vb{A})\); and
        \item \(\det\) is an \(n\)-linear function with respect to rows,
            that is, for some arbitrary \(i\)th row,
            assuming all other rows are the same,
            \begin{equation*}
                \det\threevector{\vdots}{au + bv}{\vdots}
                = a\det\threevector{\vdots}{u}{\vdots}
                + b\det\threevector{\vdots}{v}{\vdots}
            \end{equation*}
    \end{enumerate}
\end{definition}
\begin{proposition}[Uniqueness of the determinant]
    The determinant exists and is unique.
\end{proposition}
\begin{proof}
    We claim that if \(\vb{A} = (a_{ij})\), then
    \begin{equation*}
        \det(\vb{A}) = \sum_{\sigma \in S_n}\sgn(\sigma)
        \prod_{i=1}^n a_{i\sigma(i)}
        = \sum_{\sigma \in S_n}\sgn(\sigma)
        a_{1\sigma(1)}a_{2\sigma(2)} \hdots a_{n\sigma(n)}
    \end{equation*}
    is a determinant.

    We can first check that \(\det(\vb{I}) = 1\)
    since \(a_{ij} = 0\) if and only if \(i \neq j\),
    so any \(\sigma\) that is not the identity
    inside the sum produces 0.

    Now suppose we permute the rows by \(\tau \in S_n\).
    Then we are simply sending all \(a_{ij} \mapsto a_{\tau(i)j}\),
    so via a change of variables \(\upsilon\tau = \sigma\),
    and as the sum over the \(\tau(i)\) is the same as sum over \(i\),
    \begin{align*}
        \det(\tau\vb{A}) &= \sum_{\sigma \in S_n} \sgn(\sigma)
        \prod_{i=1}^n a_{\tau(i)\sigma(i)}
        = \sum_{\upsilon \in S_n} \sgn(\upsilon\tau)
        \prod_{i=1}^n a_{\tau(i)\upsilon\tau(i)} \\
        &= \sgn(\tau) \sum_{\upsilon \in S_n} \sgn(\upsilon)
        \prod_{i=1}^n a_{i\upsilon(i)}
        = \sgn(\tau) \det(\vb{A})
    \end{align*}

    The \(n\)-linear condition is obvious
    from the definition of the determinant,
    since every summand \(\prod a_{i\sigma(i)}\)
    contains exactly one term from the the \(i\)th row.
    Hence, when all other rows are kept the same,
    the determinant is a sum of a constant coefficient
    times the \(i\)th row term,
    which is by definition, linear.
    Combined together this proves existence.

    \medskip

    To prove uniqueness, suppose \(\vb{A} = (a_{ij})\) again.
    Then the \(i\)th row is a vector
    \(\vec{a}_{i\star} = a_{i1}\vec{e}_1 + \cdots + a_{in}\vec{e}_n\),
    where \(\vec{e}_j\) is a basis row vector for the \(j\)th coordinate.
    Then the determinant must be
    \begin{equation*}
        \det(\vb{A}) = \det\begin{bmatrix}
            \sum_t a_{1t}\vec{e}_t \\
            \sum_t a_{2t}\vec{e}_t \\
            \vdots \\
            \sum_t a_{nt}\vec{e}_t
        \end{bmatrix}
    \end{equation*}
    which by the \(n\)-linear condition means
    the determinant must be a linear combination of
    \(\det({\mqty[\vec{e}_{\alpha_1} & \hdots & \vec{e}_{\alpha_n}]}^\intercal)\),
    where \(\alpha_i\) is a choice between 1 to \(n\).
    Since we have to accomodate all permutations (by condition (b)),
    all combinations must be in there,
    and we need to sign of the permutation to be part of the coefficient.
    Lastly, since the identity must return 1,
    we are forced to conclude that we cannot have another factor in there,
    so the coefficient must only be the sign.
    % TODO: make this proof better
\end{proof}
\begin{remark}
    We shall discuss the determinant,
    especially with how it is algorithmically defined,
    further in Chapter~\ref{sec:linear-algebra} for linear algebra.
\end{remark}

\begin{definition}
    Suppose \(\vb{A} \in R^{n \times n}\).
    We denote the submatrix formed
    by removing the \(i\)th row and the \(j\)th column
    as \(\vb{M}_{ij}^{\vb{A}}\) or simply \(\vb{M}_{ij}\).
    The determinant of this submatrix is called a minor,
    or sometimes a first minor
    \(m_{ij} = m_{ij}^{\vb{A}} = \det\vb{M}_{ij}^{\vb{A}}\).
    We can similarly define a second, third, or \(k\)th minor
    by removing two, three, or \(k\) rows and columns from \(\vb{A}\).
\end{definition}
\begin{theorem}[Laplace Expansion for the Determinant]\label{thm:laplace-expansion-det}
    Suppose \(\vb{A} = (a_{ij}) \in R^{n \times n}\).
    Then
    \begin{equation*}
        \det\vb{A} = \sum_{j=1}^n {(-1)}^{i+j} a_{ij} m_{ij}
    \end{equation*}
    for any \(1 \leq i \leq n\).
\end{theorem}
\begin{proof}
    We shall first prove the case of expansion along the first row.
    We denote \(\tau_k = (1k)\) and \(tau_1 = 1 \in S_n\)
    as swapping the first row with the others,
    and \(S_{n-1}\) as the permutation of all rows except the first.
    Then we can clearly write the determinant as
    \begin{equation*}
        \det\vb{A} = \sum_{k=1}^n \sum_{\sigma \in S_{n-1}} \sgn(\tau_k \sigma)
        \prod_{i=1}^n a_{i\tau_k\sigma(i)}
        = \sum_{k=1}^n \sgn(\tau_k) a_{1k} \sum_{\sigma \in S_{n-1}} \sgn(\sigma)
        \prod_{i=2}^n a_{i\tau_k\sigma(i)}
    \end{equation*}
    But the second summation is effectively the definition of the determinant,
    but with the first row, and the \(k\)th column removed.
    Notice that there is a sign change
    since the effective odd/even column numbering has changed
    for the first \(k-1\) rows,
    which we know to be a \((k-1)\)-cycle
    that has a sign \({(-1)}^{k-2} = {(-1)}^k\).
    This completes the first part of the proof as
    \begin{equation*}
        \det\vb{A} = a_{11}m_{11}
        + \sum_{k=2}^n \sgn(\tau_k) a_{1k} {(-1)}^k m_{1k}
        = \sum_{k=1}^n {(-1)}^{1+k} a_{1k} m_{1k}
    \end{equation*}
    
    The expansion along the \(i\)th row is simply
    an expansion along the first row after shuffling the first \(i\) rows,
    which is a permutation of sign \(i-1\).
    Hence we multiply by \({(-1)}^{i-1}\) by the definition of the determinant
    and that yields our final answer.
    \begin{equation*}
        \det\vb{A} = {(-1)}^{i-1} \sum_{k=1}^n {(-1)}^{1+k} a_{ik} m_{ik}
        = \sum_{k=1}^n {(-1)}^{i+k} a_{ik} m_{ik}
    \end{equation*}
\end{proof}

\begin{definition}
    Suppose \(R\) is a commutative ring,
    and \(\vb{A} \in R^{n \times n}\) is a square matrix.
    We call the matrix \(\adj\vb{A} \in R^{n \times n}\)
    that makes \((\adj\vb{A})\vb{A} = \vb{A}(\adj\vb{A}) = (\det\vb{A})\vb{I}\)
    the adjugate matrix, or the classical adjoint.
    This is not to be confused with the more common notion of an adjoint,
    which is the conjugate transpose of a matrix.
\end{definition}
\begin{proposition}
    The adjugate is given by the transpose of the cofactor matrix \(\vb{C}\).
    More specifically, if \(\vb{C} = (c_{ij})\),
    then its entries are \(c_{ij} = {(-1)}^{i+j}m_{ij}\).
\end{proposition}
\begin{proof}
    Clearly \(\adj\vb{A} = (d_{ij})\)
    where \(d_{ij} = {(-1)}^{i+j}m_{ji}\).
    If \(\vb{A} = (a_{ij})\),
    then by definition of matrix multiplication we have
    \(\vb{A}(\adj\vb{A}) = (b_{ij})\), with
    \begin{equation*}
        b_{ij} = \sum_{k=1}^n a_{ik}d_{kj}
        = \sum_{k=1}^n a_{ik} {(-1)}^{k+j}m_{jk}
        = \sum_{k=1}^n {(-1)}^{j+k} a_{ik}m_{jk}
    \end{equation*}
    Notice that if \(i=j\),
    then this is exactly the definition of the determinant
    by \hyperref[thm:laplace-expansion-det]{Laplace's expansion}.
    Hence the entries on the main diagonal must be equal to \(\det(\vb{A})\).
    
    Now it suffices to prove that all off-diagonals are 0.
    Since \(i \neq j\), we can replace row \(j\) with row \(i\) in \(\vb{A}\),
    which we will denote this new matrix a \(\vb{A'}\);
    as the minor \(m_{jk}\) removes the \(j\)th row anyways,
    \(m_{jk}^{\vb{A}} = m_{jk}^{\vb{A'}}\).
    But since the \(i\)th row of the old matrix \(\vb{A}\)
    is the \(j\)th row of the new matrix \(\vb{A'}\),
    \(a_{ik} = a'_{jk}\), so \(b_{ij}\) is again,
    by \hyperref[thm:laplace-expansion-det]{Laplace's expansion},
    the determinant of the new matrix \(\det\vb{A'}\).
    But clearly \(\det\vb{A'} = 0\),
    as it has a duplicate row.

    Left multiplication by the adjugate yields the same result
    by simple computation.
\end{proof}

\begin{theorem}
    Suppose \(R\) is a commutative ring,
    and \(\vb{A} \in R^{n \times n}\).
    Then \(\vb{A}^{-1} \in R^{n \times n}\) exists
    if and only if \({(\det\vb{A})}^{-1} \in R\) exists.
\end{theorem}
\begin{proof}
    By the definition of the adjugate,
    \({(\det\vb{A})}^{-1} \adj\vb{A} = \vb{A}^{-1}\).
    It is now clear that one cannot exist without the other.
\end{proof}

\subsection*{Quaternions}

\begin{lemma}
    There exists solutions to polynomial equations with real-valued coefficients
    that do not reside in the reals.
    That is, \(\bR\) is not an algebraically closed field.
\end{lemma}
\begin{proof}
    The counterexample is \(x^2 + 1 = 0\),
    requiring \(\pm\sqrt{-1}\) as solutions.
\end{proof}
\begin{remark}
    This is historically the motivation
    as to constructing the complex numbers \(\bC\).
    However, it does not seem possible to construct
    even larger fields via this method.
\end{remark}
\begin{theorem}[Fundamental Theorem of Algebra]\label{thm:fta}
    For a polynomial equation of degree \(n\) with complex-valued coefficients,
    there exists exactly \(n\) solutions in \(\bC\) counted with multiplicity.
    That is, \(\bC\) is algebraically closed.
\end{theorem}
\begin{proof}
    % TODO: provide algebraic proof
    We shall first formalize the above statement.
    Suppose we have a polynomial \(p(z) = \sum_{i=0}^n a_i z^i\).
    We wish to prove that this has \(n\) solutions,
    but it is sufficient to prove that it has one solution,
    because by factoring out that solution,
    one would obtain a polynomial one degree lower,
    and by induction that would complete our argument.

    We can assume a monic polynomial \(a_n = 1\),
    since multiplication by a factor does not change the roots.
    Let \(\mu = \inf_{z\in\bC}\abs{p(z)}\).
    We wish to show that \(\mu = 0\),
    and that the infimum is attained at some \(z_0 \in \bC\).
    For some arbitrary \(\abs{z} = R\) on a circle,
    \begin{equation*}
        \abs{p(z)}
        = \abs{z}^n\abs{1 + \frac{a_{n-1}}{z} + \cdots + \frac{a_0}{z^n}}
        \geq \abs{z}^n\qty(1 - \frac{\abs{a_{n-1}}}{\abs{z}}
        - \cdots - \frac{\abs{a_0}}{\abs{z}^n})
        = R^n \qty(1 - \frac{\abs{a_{n-1}}}{\abs{z}}
        - \cdots - \frac{\abs{a_0}}{\abs{z}^n})
    \end{equation*}
    which tends to infinity as \(R \to \infty\).
    Hence we can conclude there exists some \(R_0 > 0\)
    such that \(\abs{p(z)} \geq \mu + 1\).
    Focusing on inside this circle of radius \(R_0\),
    \(\abs{p}\) is continuous, and \(\{z : \abs{z} \leq R_0\}\) is compact,
    so analysis tells us the infimum of \(\abs{p}\)
    inside this region is attained;
    tacked on the fact that the infimum outside of the circle must be larger,
    the global infimum \(\mu\) must be the same as the one inside the circle.
    
    Now suppose, by way of contradiction, that \(\mu > 0\).
    Let \(q(z) = p(z_0 + z)/p(z_0)\), and \(q(0) = 1\).
    As we know \(q(z)\) is also a polynomial of degree \(n\),
    we can write \(q(z) = 1 + b_k z^k + \cdots + b_n z^n\),
    where \(b_k\) is the first nonzero coefficient.
    We see that \(\abs{q(z)} = \abs{p(z_0+z)}/\mu \geq 1\);
    but on the other hand,
    \(\abs{q(z)} = \abs{1 + b_k z^k + \cdots + b_n z^n}
    \leq \abs{1 + b_k z^k} + \sum_{m=k+1}^n \abs{b_m}\abs{z}^m\).
    Writing \(b_k z^k = \abs{b_k}\frac{b_k}{\abs{b_k}}z^k\),
    where \(\frac{b_k}{\abs{b_k}} = e^{it}\) for some \(t\),
    and \(z = re^{i\theta}\),
    this yields us \(b_k z^k = \abs{b_k}r^k e^{i(t+k\theta)}\).
    We can choose a value \(\theta = (\pi-t)/k\),
    which evaluates to \(b_k z^k = \abs{b_k}r^k e^{i\pi} = -\abs{b_k}r^k\),
    which will be strictly within \(-1\) and \(0\) for \(r\) small enough.
    Then we have
    \begin{equation*}
        \abs{q(z)} = \abs{q(re^{i\theta})}
        \leq 1 - \abs{b_k}r^k + \sum_{m=k+1}^n \abs{b_k}r^m
        = 1 - r^k\qty(\abs{b_k} - \sum_{m=k+1}^n \abs{b_k}r^m) < 1
    \end{equation*}
    since the quantity inside the brackets is positive for \(r\) small enough.
    This is a contradiction,
    so \(\mu = 0\), and there must be a root.
\end{proof}
\begin{remark}
    Despite the name, the Fundamemtal Theorem of Algebra
    is not a fundamental theorem in the study of algebra,
    but rather a theorem in analysis.
    A purely analytic proof is always provided
    in any analysis textbook at the undergraduate level;
    however, a partially algebraic proof is possible
    if we assume some facts from analysis,
    such as continuity and the intermediate value theorem.
\end{remark}

\begin{definition}
    The quaternions are a noncommutative division ring
    \(\bH = \{\smqty[a&b \\ -\bar{b}&\bar{a}] : a,b \in \bC\}
    \subseteq \bC^{2 \times 2}\).
\end{definition}
\begin{remark}
    This might not be the usual way people define quaternions,
    but it is equivalent, and lends itself to obviously being a matrix ring.
    The inverse is given by \({\smqty[a&b \\ -\bar{b}&a]}^{-1}
    = \frac{1}{\abs{a}^2+\abs{b}^2}\smqty[\bar{a}&-b \\ \bar{b}&a]\).
\end{remark}
\begin{proposition}
    The quaternions \(\bH\) can also be thought of
    as a 4-dimensional vector space over the reals \(\bR^4\)
    with the unit vectors \(1,i,j,k\).
\end{proposition}
\begin{proof}
    We shall write a quaternion as \(\smqty[a+ib&c+id \\ -c+id&a-ib]\),
    with \(a,b,c,d \in \bR\);
    then by simple verification one can find
    \(a + bi + cj + dk\) with the multiplication table \(i^2=j^2=k^2=ijk=-1\),
    since we can represent the following:
    \begin{equation*}
        i = \twomatrix{\sqrt{-1}}{0}{0}{\sqrt{-1}} \qquad
        j = \twomatrix{0}{1}{-1}{0} \qquad
        k = \twomatrix{0}{\sqrt{-1}}{\sqrt{-1}}{0}
    \end{equation*}
\end{proof}

\begin{definition}
    Suppose \(q = \smqty[a&b \\ -\bar{b}&a] \in \bH\).
    The quaternion norm is defined as
    \(\Normalizer(q) = \abs{a}^2 + \abs{b}^2\).
\end{definition}


\section{Ideals}

\begin{definition}
    A ring homomorphism is a function \(\func{\phi}{R}{S}\),
    where \(R,S\) are rings,
    with the properties
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item \(\func{\phi}{(R,+,0)}{(S,+,0)}\) is a group homomorphism; and
        \item \(\func{\phi}{(R - \{0\},\cdot,1)}{(S - \{0\},\cdot,1)}\)
            is a monoid homomorphism.
    \end{enumerate}
\end{definition}
\begin{definition}
    The kernel of a ring homomorphism
    is the preimage of \(0\),
    that is, \(\ker\phi = \phi^{-1}(0)\).
    Note that this is a subgroup of \((R,+,0)\).
\end{definition}

\begin{definition}
    Suppose \(R\) is a ring, and \(I \subseteq R\).
    \(I\) is an ideal if it fits the following criteria:
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item \(I \subseteq (R,+,0)\) is a subgroup; and
        \item \(\forall x \in I, \forall y \in R, \{xy,yx\} \subset I\).
    \end{enumerate}
    A left ideal is defined with only \(yx \in I\),
    and a right ideal is defined with only \(xy \in I\).
\end{definition}
\begin{remark}
    Note that an ideal does not necessarily contain the unit \(1\),
    and therefore is not always a ring.
\end{remark}

\begin{definition}
    Suppose \(R\) is a ring, and \(r \in R\) some arbitrary element.
    The left principal ideal is \(Rr = \{yr : y \in R\}\),
    and the right principal ideal is \(rR = \{ry : y \in R\}\).
    These two definitions coincide with \(R\) is a commutative ring,
    and we call it the principal ideal.
\end{definition}
\begin{remark}
    In the rest of the chapter, we assume that all rings are commutative,
    but we remark that some of the proofs below indeed do not require commutativity.
\end{remark}

\begin{lemma}\label{lem:unit-mult-surjective}
    Suppose \(R\) is a ring, and \(r \in R\).
    % and \(I \subseteq R\) is a left (or right) principal ideal
    % generated by \(R\).
    If \(\vfunc{f}{R}{R}{x}{xr}\) (or \(rx\)) is a mapping,
    then \(f\) is surjective if and only if \(r\) is a unit.
\end{lemma}
\begin{proof}
    Suppose \(r\) is a unit.
    Then for some \(y \in R\),
    clearly \(yr^{-1} \mapsto yr^{-1}r = y\).
    This is therefore surjective.

    Suppose \(f\) is surjective.
    Then for all \(y \in R\),
    there exists an element \(x \in R\) such that \(xr = y\).
    In particular, let \(y = 1\) and we see that there is an inverse.
\end{proof}
\begin{proposition}\label{prop:unit-one-ideal}
    Suppose \(R\) is a ring.
    \(r \in R\) is a unit if and only if the ideals \((r) = (1)\).
\end{proposition}
\begin{proof}
    Suppose \(r\) is a unit.
    Consider the surjection \(x \mapsto xr\).
    The inverse map is \(x \mapsto xr^{-1}\), which obviously exists.

    Suppose \((r) = (1)\).
    Then the map \(x \mapsto xr\) must be surjective,
    so the \hyperref[lem:unit-mult-surjective]{lemma above} gives us that \(r\) is a unit.
\end{proof}

\begin{lemma}
    Suppose \(R\) is a ring, and \(I \subseteq R\) is an ideal.
    Then \(R/I\) forms a ring via \((x+I)(y+I) = (xy+I)\).
\end{lemma}
\begin{proof}
    Clearly \((R,+,0)/I\) readily forms an additive group,
    since \(I\) is an additive subgroup,
    and Proposition~\ref{prop:abelian-subgroup-normal}
    informs us that it is normal.

    Now suppose \(\{r,r'\} \subset I\).
    Clearly \((x+r)(y+r') = xy + xr' + ry + rr' \in I\),
    since the last three terms are all in \(I\);
    we are guaranteed closure.
    Next up the multiplicative unit is \(1+I\),
    which since \((1+I)(x+I) = 1x+I = x+I = x1+I = (x+I)(1+I)\)
    gives us the identity.
    Associativity is inherited.
\end{proof}
\begin{corollary}
    \(\vfunc{\pi}{R}{R/I}{x}{x+I}\) is the quotient homomorphism.
\end{corollary}
\begin{proof}
    All properties are inherited, and it forms a ring.
\end{proof}

\begin{theorem}\label{thm:field-props}
    Suppose \(R,S\) are nondegenerate rings.
    The following are equivalent:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(R\) is a field;
        \item \(R\) has only two distinct ideals \((0)\) and \((1)\); and
        \item Every ring homomorphism \(\func{\phi}{R}{S}\) is injective.
    \end{enumerate}
\end{theorem}
\begin{proof}
    We first prove that (a) implies (c).
    Let \(R\) be a field, and consider \(\phi(r) = 0\).
    Notice that if \(r\) has an inverse,
    then \(\phi(r)\phi(r^{-1}) = \phi(1) = 1\),
    which contradicts that \(\phi(r) = 0\).
    Hence \(r = 0\), and \(\phi\) is injective.

    We now prove that (c) implies (b), via contraposition.
    Consider a nonzero ideal \(I \subseteq R\).
    We have the quotient map \(\func{\pi}{R}{R/I}\),
    which is clearly not injective.
    
    We shall lastly prove (b) implies (a), again via contraposition.
    Suppose \(R\) is not a field.
    Then there exists some nonzero \(x \in R\) that is not a unit,
    so by definition \(x \notin (0)\),
    and \((x) \neq (1)\) by Proposition~\ref{prop:unit-one-ideal}.
\end{proof}

\begin{proposition}\label{prop:ideal-operations}
    Suppoose \(R\) is a ring,
    and \(I_i\) are ideals.
    Then the following are ideals:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item an arbitrary intersection \(\bigcap_i I_i\);
        \item an arbitrary sum with finite support \(\sum_i I_i = \{\sum_\text{finite} x_i : x_i \in I_i\}\); and
        \item a finite product \(\prod_{i=1}^n I_i = \pqty{\prod_{i=1}^n x_i : x_i \in I_i} \subseteq \bigcap_{i=1}^n I_i\).
    \end{enumerate}
    % \(I \cap J\), \(I + J = \{x+y : x \in I, y \in J\}\),
    % and \(IJ = \{\sum_i x_i y_i : x_i \in I, y_i \in J\}
    % \subseteq I \cap J\) are all ideals.
\end{proposition}
\begin{proof}
    Suppose \(\{x,y\} \subset \bigcap_i I_i\). Then clearly
    \hyperref[lem:intersection-subgroup]{the intersection is a subgroup}.
    Moreover, by closure of each of \(I_i\),
    \(xy\) is in each of \(I_i\).
    This proves \(\bigcap_i I_i\) is an ideal.

    % Suppose \(\{x,x'\} \subset I\), and \(\{y,y'\} \subset J\).
    Suppose \(\Bqty{\sum_i x_i, \sum_j y_j} \in I_i\).
    % Any \((x+y)+(x'+y') = (x+x')+(y+y') \in I+J\),
    the sum of two finite sum is still a finite sum,
    associativity is given by \(R\),
    % \(0 \in I\) and \(0 \in J\) so \(0 = 0+0 \in I+J\),
    the empty sum is in \(\sum_i I_i\),
    and inverse is clearly \(-\sum_i x_i \in \sum_i I_i\);
    \(\sum_i I_i\) is therefore an additive subgroup.
    Moreover, \(\pqty{\sum_i x_i}\pqty{\sum_j y_j} = \sum_{i,j} x_i y_j \in \sum_i I_i\),
    since \(x_i \sum_j y_j \in I_i\).
    This proves \(\sum_i I_i\) is an ideal.

    Lastly, we only need to prove the case of two ideals,
    since an inductive argument completes the proof.
    Suppose \(x \in I\) and \(y \in J\).
    We have closure simply by the summation definition,
    associativity is inherited from \(R\),
    the empty sum is \(0 \in IJ\),
    and the inverse is just \(\sum_i -x_i y_i\),
    which since \(-x_i \in I\), gives us the inverse;
    \(IJ\) is therefore an additive subgroup.
    It is also clear that this is in \(I\) and in \(J\).
    Moreover, \((\sum_i x_i y_i)(\sum_j x_j y_j)
    = \sum_{ij} x_i y_i x_j y_j\)
    and since \(y_i x_j y_j \in J\),
    we have closure under multiplication.
    This proves \(IJ\) is an ideal.
\end{proof}
\begin{proposition}\label{prop:ideal-distributive}
    Suppose \(I,J,K\) are three ideals.
    We have the two following distributive properties:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(I(J+K) = IJ + IK\); and
        \item \(I \cap (J+K) \supseteq I \cap J + I \cap K\),
            with equality only if \(J \subseteq I\) or \(K \subseteq I\).
    \end{enumerate}
\end{proposition}
\begin{proof}
    Suppoose \(x \in I\), \(y \in J\), and \(z \in K\).
    Then any \(x(y+z) = xy + xz\).
    This proves statement (a).

    Suppose \(x \in I \cap J\), and \(y \in I \cap K\).
    Then \(x+y \in J+K\) obviously,
    and under additive closure of \(I\), \(x+y \in I\).
    Now let us add the assumption of \(J \subseteq I\).
    Suppose \(z \in I \cap (J+K)\).
    Then \(z = x+y\) for some \(x \in J\) and \(y \in K\),
    If \(x \in I\), then \(y \in I\) by closure.
    This proves statement (b).
\end{proof}
\begin{proposition}\label{prop:nested-ideals}
    Suppose \(I_1 \subseteq I_2 \subseteq \cdots \subseteq
    I_n \subseteq \cdots\) is an ascending chain of ideals.
    Then \(I = \bigcup_{i=1}^\infty I_i\) is an ideal.
\end{proposition}
\begin{proof}
    Suppose \(\{x,y\} \subset I\).
    Then \(x \in I_m\) and \(y \in I_n\) for some \(m,n\).
    Without loss of generality let \(I_m \subseteq I_n\),
    then \(\{x,y\} \subset I_n\),
    so we have closure inside \(I_n\), and hence inside \(I\).
    Associativity is as per usual inherited from \(R\).
    The identity \(0 \in I_1 \subseteq I\).
    And lastly, if \(x \in I\), then \(x \in I_n\) for some \(n\),
    so \(-x \in I_n \subseteq I\).
    This proves that \(I\) is an additive subgroup.
    
    Now suppose \(x \in I\), and \(r \in R\).
    Clearly \(x \in I_n\) for some \(n\)
    and therefore \(xr \in I_n \subseteq I\).
\end{proof}


\begin{definition}
    Suppose \(R\) is a ring, and \(S \subseteq R\) a subset.
    The ideal generated by \(S\) is
    \((S) = \{\sum_i r_i s_i r_i' : s_i \in S, \{r_i,r_i'\} \subset R\}\).
    We call an ideal \(I\) finitely generated
    if \(I = (S)\) for some finite \(S\).
\end{definition}
\begin{remark}
    Ideals are typically generated by a set of generators \(x_i \in S\),
    so elements are of the form
    \(I = \{\sum_i r_i x_i : r_i \in R, x_i \in S\}\).
    A Noetherian ring is a ring with every ideal being finitely generated.
\end{remark}


\section{Isomorphism Theorems}

\begin{remark}
    The isomorphism theorems work in a lot of algebraic structures
    (formally, it works in any universal algebra).
    We will number them \hyperref[rmk:iso-numbering]{the same way as we did for groups}.
\end{remark}

\begin{theorem}[Universal Property of Quotient Rings]\label{thm:univ-prop-quotient-ring}
    Let \(R,S\) be commutative rings,
    and \(I \subseteq R\) be an ideal.
    Suppose \(\func{\pi}{R}{R/I}\) is the quotient homomorphism
    and \(\func{\phi}{R}{S}\) is any ring homomorphism
    with \(I \subseteq \ker\phi\).
    Then there exists a unique ring homomorphism
    \(\func{\bar{\phi}}{R/I}{S}\) such that \(\phi = \bar{\phi}\circ\pi\).

    This is represented by the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            R \arrow{r}{\phi} \arrow{d}{\pi} & S \\
            R/I \arrow[dashrightarrow]{ru}[swap]{\exists! \bar{\phi}}
        \end{tikzcd}
    \end{center}
\end{theorem}
\begin{proof}
    We wish to prove uniqueness by assuming existence.
    Suppose \(\phi = \bar{\phi}\circ\pi = \bar{\phi}'\circ\pi\).
    Since \(I \subseteq \ker\phi\),
    all elements \(x \in I\) obey \(\phi(x) = 0\).
    Knowing that all elements in \(R\) belong in some coset \(r+I\),
    all \(r' \in r+I\) maps to \(\phi(r') = \phi(r+x) = \phi(r)+\phi(x) = \phi(r)\),
    which tells us the image of each coset
    is a single element \(\phi(r+I) = \{\phi(r)\}\).
    Now, seeing that \(\pi \) maps \(r' \mapsto r+I\), its coset,
    by definition of a quotient mapping,
    if \(\bar{\phi} \neq \bar{\phi}'\),
    there must be one such coset \(r+I\)
    that \(\bar{\phi}(r+I) \neq \bar{\phi}'(r+I)\) disagrees on.
    However, this is a contradiction,
    because for all \(r' \in r+I \subseteq R\),
    \(\bar{\phi}'(r+I) = \bar{\phi}'(\pi(r')) = \phi(r')
    = \bar{\phi}(\pi(r')) = \bar{\phi}(r+I)\),
    contradicting with our assumed inequality above.
    Hence we have established uniqueness of \(\bar{\phi}\).

    We will now prove existence by constructing such a homomorphism.
    Let \(\vfunc{\bar{\phi}}{R/I}{S}{r+I}{\phi(r)}\),
    mapping all cosets \(r+I\) to the function output of its coset representative.
    Suppose some arbitrary element \(r' \in r+I \subseteq R\)
    in an arbitrary coset.
    Then we know that there exists \(x \in I\) such that \(r' = r+x\),
    which allows us to conclude that
    \begin{equation*}
        \phi(r') = \phi(r+x) = \phi(r) + \phi(x)
        = \phi(r) = \bar{\phi}(r+I) = \bar{\phi}(\pi(r'))
    \end{equation*}
\end{proof}

\begin{theorem}[First Isomorphism Theorem for Rings]\label{thm:iso-1-ring}
    Suppose \(\func{\phi}{R}{S}\) is a ring homomorphism,
    and \(I = \ker\phi\).
    We have:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(I \subseteq R\), the kernel is an ideal;
        \item \(\img\phi \subseteq S\), the image is a subring; and
        \item \(\img\phi \cong R/I\),
            the image is uniquely isomorphic to the quotient ring.
    \end{enumerate}

    This is represented by the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            R \arrow{rd}{\phi} \arrow{d}{\pi} & S \\
            R/I \arrow{r}{\cong} & \img\phi \arrow[dash]{u}
        \end{tikzcd}
    \end{center}
\end{theorem}
\begin{proof}
    By the simple fact that \(\phi\) is also an additive group homomorphism,
    the \hyperref[thm:iso-1-group]{first group isomorphism theorem}
    gives us that the kernel is an additive subgroup.

    Now suppose some arbitrary \(x \in \ker\phi\), and \(y \in R\).
    Then clearly \(\phi(xy) = \phi(x)\phi(y) = 0\phi(y) = 0= 
    \phi(y)0 = \phi(y)\phi(x) = \phi(yx)\).
    This proves statement (a).

    \medskip

    Secondly, the homomorphism inherits all its properties from \(R\),
    so its image must form a subring.
    In particular,
    the \hyperref[thm:iso-1-group]{first group (monoid) isomorphism theorem}
    guarantees us that the image will be an additive subgroup,
    and a multiplicative monoid.
    This proves statement (b).

    \medskip

    The \hyperref[thm:univ-prop-quotient-ring]{universal property}
    guarantees a unique homomorphism \(\func{\bar{\phi}}{R/I}{\img\phi}\)
    such that \(\phi = \bar{\phi}\circ\pi\),
    \(\pi\) being the quotient homomorphism.
    By the \hyperref[thm:iso-1-group]{first group isomorphism theorem},
    when applied to the additive group,
    we see that it is an isomorphism.
    This proves statement (c).
\end{proof}

\begin{theorem}[Second Isomorphism Theorem for Rings]\label{thm:iso-2-ring}
    Suppose \(R\) is a ring, \(S \subseteq R\) some subring,
    and \(I \subseteq R\) an ideal.
    We have:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(S + I = \{s+x : x \in S, x \in I\} \subseteq R\),
            the sum is a subring;
        \item \(I \subseteq S + I\), the ideal is also and ideal of the sum;
        \item \(S \cap I \subseteq S\),
            the intersection is an ideal of the subring; and
        \item \((S+I)/I \cong S/(S \cap I)\),
            these two quotients are isomorphic.
    \end{enumerate}

    This is represented by the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            R \arrow[dash]{d} \\
            S+I \arrow[dash]{d} \arrow{r}{\pi} \arrow[dash]{rd} &
            (S+I)/I \arrow[leftrightarrow]{rd}{\cong} \\
            I \arrow[dash]{rd} & S \arrow{r}{\pi'} \arrow[dash]{d} &
            S/(S \cap I) \\
            & S \cap I
        \end{tikzcd}
    \end{center}
\end{theorem}
\begin{proof}
    By the \hyperref[thm:iso-2-group]{second group isomorphism theorem},
    \(S+I\) forms an additive subgroup.
    For \(\{s,s'\} \subset S\) and \(\{x,x'\} \subset I\),
    we see that \((s+x)(s'+x') = ss' + xs' + sx' + xx' \in S+I\),
    since \(ss' \in S\) and \(xs' + sx' + xx' \in I\),
    which gives us closure.
    Associativity is inherited from \(R\),
    and the identity is \(0+1 \in S+I\).
    This proves statement (a).

    \medskip

    \(I\) already forms a group,
    and clearly \(I = 0+I \subseteq S+I\),
    so it is already a subgroup.
    By definition of an ideal,
    for all \(x \in I\) and \(y \in R\), \(\{xy,yx\} \subset I\),
    so more specifically this holds for \(y \in S+I \subseteq R\).
    This proves statement (b).

    \medskip

    Suppose \(\{x,x'\} \in S \cap I\).
    Then \(x+x' \in S \cap I\),
    since there is closure for both \(S\) as a group,
    and \(I\) as an ideal (which is also a group).
    Associativity is always inherited,
    and the identity is \(0 = \in S \cap I\)
    as \(0 \in S\) and \(0 \in I\) by definition.
    Lastly, additive inverses always exist in a subgroups \(S,I\),
    so \(-x \in S \cap I\).
    Hence \(S \cap I \subseteq S\) is a subgroup.

    Now, we need to check the multiplicative condition.
    Suppose \(x \in S \cap I\), and \(y \in S\).
    We see that by the definition of a subring, \(\{xy,yx\} \subset S\);
    but also \(\{xy,yx\} \subset I\) since \(I \subseteq R\) is an ideal,
    so it also works for elements \(y \in S \subseteq R\).
    This proves statement (c).

    \medskip

    By statements (b) \& (c), these two quotients are well-formed.
    We now attempt to construct a homomorphism
    \(\vfunc{\phi}{S}{(S+I)/I}{s}{s+I}\).
    We demonstrate that this a valid homomorphism,
    by showing that
    \begin{gather*}
        \phi(s_1 s_2) = (s_1 s_2)+I = (s_1 + I)(s_2 + I) = \phi(s_1)\phi(s_2) \\
        \phi(s_1+s_2) = s_1+s_2+I = (s_1+I) + (s_2+I) = \phi(s_1) + \phi(s_2)
    \end{gather*}
    since \(s_1, s_2\) are elements of \(R\),
    so same logic as the quotient ring epimorphism applies.

    We now want to show that \(\phi\) is surjective.
    The elements \(s+x \in S+I\) must belong in some coset \(s+x+I\),
    which we now see is equivalent to \(s+I\).
    By definition, \(\phi\) maps \(s \mapsto s+I\),
    so every coset is covered by \(\phi\),
    and therefore it is an epimorphism.

    We can then demonstrate that \(\ker\phi = S \cap I\).
    We can see that if \(x \in S \cap I\),
    then \(x \in I\), so \(\phi(x) = x+I = I\),
    which gives us \(S \cap I \subseteq \ker\phi\).
    On the other hand, if \(x \in \ker\phi\),
    then \(\phi(x) = x+I \subseteq I\),
    which requires \(x \in I\), giving us \(\ker\phi \subseteq S \cap I\).
    Hence \(\ker\phi = S \cap I\).

    Lastly, we apply the \hyperref[thm:iso-2-ring]{first isomorphism theorem},
    and prove that there exists a unique homomorphism
    between \(S/(S \cap I) \cong (S+I)/I\).
\end{proof}

\begin{theorem}[Third Isomorphism Theorem for Rings]\label{thm:iso-3-ring}
    Suppose \(R\) is a ring, \(I \subseteq R\) an ideal. Then:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item if \(S\) is a subring such that \(I \subseteq S \subseteq R\),
            then \(S/I \subseteq R/I\) is a subring;
        \item a subring of \(R/I\) must be of the form \(S/I\)
            such that \(S\) is a subring with \(I \subseteq S \subseteq R\);
        \item if \(J\) is an ideal such that \(I \subseteq J \subseteq R\),
            then \(J/I \subseteq R/I\) is an ideal;
        \item an ideal of \(R/I\) must be of the form \(J/I\)
            such that \(J \subseteq R\) is an ideal
            with \(I \subseteq J \subseteq R\); and
        \item if \(J \subseteq R\) is an ideal such that \(I \subseteq J \subseteq R\),
            then \((R/I)/(J/I) \cong R/J\).
    \end{enumerate}

    This is represented by the following commutative diagrams:
    \begin{center}
        \begin{tikzcd}
            R \arrow[dash]{d} \arrow{r} & R/I \arrow[dash]{d} \\
            % & G \arrow{d}{\pi'} \arrow{r}{\pi} & G/N \arrow{d}{\pi'} \\
            S \arrow{r} & S/I % & K \arrow{r}{\pi} & K/N
        \end{tikzcd} \qquad
        \begin{tikzcd}
            R \arrow{r} \arrow[dash]{d} \arrow{rd} &
            R/J \arrow[leftrightarrow]{rd}{\cong} \\
            J \arrow[dash]{rd} & R/I \arrow{r} \arrow{d} & (R/I)/(J/I) \\
            & J/I
        \end{tikzcd}
    \end{center}
\end{theorem}
\begin{proof}
    We first have to prove that \(I \subseteq S\),
    which is obvious because it is already an additive subgroup,
    and \(\{xy,yx\} \in I\) for all \(x \in I\) and \(y \in R\)
    can be restricted to \(y \in S \subseteq R\).

    It is now easy to see that
    with the quotient homomorphism \(\func{\pi}{R}{R/I}\),
    the image of the subring \(\pi(S) = S/I\),
    so by the \hyperref[thm:iso-1-group]{first isomorphism theorem}
    \(S/I\) forms a subring.
    This proves statement (a).

    \medskip

    Suppose \(S' \subseteq R/I\) is a subring.
    We can look at the preimage \(\pi^{-1}(S')\),
    which since \(0 \in S'\), we have \(S' \supseteq \ker\pi = I\).
    Notice that the preimage of a ring is still a ring,
    because the correspondence given by
    the \hyperref[thm:iso-4-group]{fourth group (monoid) isomorphism theorem}
    when applied to the additive group and the multiplicative monoid
    forms a ring.
    This proves statement (b).

    \medskip

    We now look at the quotient homomorphism \(\func{\pi}{R}{R/I}\),
    especially the image of the larger ideal \(\pi(J) = J/I\).
    By the \hyperref[thm:iso-1-group]{first group isomorphism theorem},
    \(\pi(J)\) forms an additive group,
    and if \(x \in J\) and \(r \in R\),
    \(\pi(x)\pi(r) = \pi(xr) \in \pi(J) = J/I\),
    and similarly this holds for \(\pi(r)\pi(x)\).
    Hence \(J/I\) is an ideal, proving statement (c).

    \medskip

    Suppose \(J' \subseteq R/I\) is an ideal.
    We can look at the preimage \(\pi^{-1}(J')\),
    which since \(0 \in J'\), we have \(\pi^{-1}(J') \supseteq \ker\pi = I\).
    Notice that the preimage \(\pi^{-1}(J') = J' + I\) is an ideal,
    which is given by Proposition~\ref{prop:ideal-operations}.
    Hence we have the preimage being an ideal \(J\),
    so \(J' = \pi(J) = J/I\),
    proving statement (d).

    \medskip

    We can now attempt to construct a homomorphism
    \(\vfunc{\phi}{R/I}{R/J}{r+I}{r+J}\).
    This is valid because
    by the \hyperref[thm:univ-prop-quotient-ring]{universal property},
    we have \(\func{\pi}{R}{R/I}\) and \(\func{\eta}{R}{R/J}\),
    so there is a unique homomorphism that makes \(\eta = \phi\circ\pi\).
    This is surjective by Theorem~\ref{thm:composite-surjective}.

    We claim the kernel is \(\ker\phi = J/I\).
    Observe that \(\ker\eta = J\) and \(\ker\pi = I\),
    so \(\phi\) must map all the \(I\)-cosets
    that are represented by elements of \(J\) into \(0\).

    Lastly, we invoke the \hyperref[thm:iso-1-ring]{first isomorphism theorem},
    which gives us \(\ker\phi = J/I \subseteq R/I\),
    making our quotient \((R/I)/(J/I)\) valid,
    and also that \((R/I)/\ker\phi = (R/I)/(J/I) \cong R/J\),
    proving statement (e).
\end{proof}

\begin{theorem}[Fourth Isomorphism Theorem for Rings]\label{thm:iso-4-ring}
    Suppose \(R\) is a ring, \(I \subseteq R\) some ideal,
    and \(\vfunc{\pi}{R}{R/I}{x}{x+I}\) the quotient homomorphism.
    Then \(\pi\) is a bijection between the subrings of \(R/I\)
    and the subrings of \(R\) containing \(I\);
    and is also a bijection between ideals of \(R/I\)
    and ideals (and subrings) of \(R\) containing \(I\).
\end{theorem}
\begin{proof}
    This is merely a corollary of
    the \hyperref[thm:iso-3-ring]{third isomorphism theorem}.
    Statements (a) and (b) prove the correspondence between subrings,
    while statements (c) and (d) prove the correspondence between ideals.
\end{proof}


\section{Classification of Ideals}

\subsection*{Maximal, Prime, and Radical Ideals}

\begin{definition}
    Suppose \(R\) is a ring.
    An element \(x \in R\) is nilpotent if there exists \(n > 0\) such that \(x^n = 0\).
    Any nonzero nilpotent element is a zero divisor.
    A ring \(R\) is reduced if the only nilpotent element is 0.
\end{definition}

\begin{definition}
    Suppose \(R\) is a ring, \(I \subseteq R\) an ideal.
    \(I\) is maximal if \(I \neq R\),
    and there does not exist any intermediate ideals \(J\)
    such that \(I \subseteq J \subseteq R\).
\end{definition}
\begin{theorem}\label{thm:maximal-quotient-field}
    Suppose \(R\) is a ring,
    and \(I \subseteq R\) an ideal.
    \(I\) is maximal if and only if \(R/I\) is a field.
\end{theorem}
\begin{proof}
    Since \(I\) maximal ideal,
    there does not exist any intermediate ideal \(J \subseteq R\).
    By the \hyperref[thm:iso-3-ring]{third isomorphism theorem}
    there is no intermediate ideal \(J/I\)
    between the quotient ring \(R/I\) and \(I/I = \{0\}\).

    Suppose \(R/I\) is a field.
    Then there are no nonzero proper ideals by Theorem~\ref{thm:field-props},
    and there are no intermediate ideals \(I \subseteq J \subseteq R\)
    due to the \hyperref[thm:iso-3-ring]{third isomorphism theorem}.
    \(I\) must be maximal.
\end{proof}

\begin{definition}
    Suppose \(R\) is a ring, \(I \subseteq R\) an ideal.
    \(I\) is prime if \(xy \in I\) implies \(x \in I\) or \(y \in I\);
    that is, the complement of \(I\) is closed under multiplication.
\end{definition}
\begin{theorem}\label{thm:prime-quotient-domain}
    Suppose \(R\) is a ring, \(I \subseteq R\) an ideal.
    \(I\) is prime if and only if \(R/I\) is a domain.
\end{theorem}
\begin{proof}
    For the foward direction, suppose \(I\) is a prime ideal.
    By way of contradiction, let \(R/I\) not be a domain,
    so there exists nonzero \(x+I, y+I\) such that \((x+I)(y+I) = xy+I = I\).
    But since \(xy \in I\),
    the product of the two generated ideals \((x)(y) \subseteq I\),
    so either \((x) \subseteq I\), which implies \(x \in I\),
    or \((y) \subseteq I\), which implies \(y \in I\),
    contradicting our assumption of nonzero \(x,y\).

    For the reverse direction, suppose \(R/I\) is an integral domain.
    Then for all \(\{x,y\} \subset R\), \((x+I)(y+I) = xy+I = I\),
    \(xy \in I\) implies either \(x \in I\) or \(y \in I\).
    % By way of contradiction, let \(I\) not be prime,
    % so there exists two ideals \(J,K\) such that \(JK \subseteq I\),
    % but neither \(J,K\) are subideals of \(I\).
    For any two ideals \(JK \subseteq I\),
    every \(jk \in I\) for \(j \in J\) and \(k \in K\).
    Without loss of generality, suppose \(J\) not a subideal of \(I\),
    so there exists some \(j_0 \in J\) such that \(j \notin I\).
    Then clearly since \(j_0 k \in I\) for all \(k \in K\),
    \(k \in I\), so \(K \subseteq I\).
    Hence \(I\) is prime.
\end{proof}

\begin{definition}
    Suppose \(R\) is a ring, \(I \subseteq R\) an ideal.
    \(I\) is radical if for all \(x \in R\) and \(x^n \in I\), then \(x \in I\).
\end{definition}
\begin{theorem}\label{thm:radical-quotient-reduced}
    Suppose \(R\) is a ring, \(I \subseteq R\) an ideal.
    \(I\) is radical if and only if \(R/I\) is reduced.
\end{theorem}
\begin{proof}
    Suppose \(I\) is not radical;
    then there exists some \(x \notin I\) such that \(x^n \in I\).
    Hence there exists some nonzero \([x] \in R/I\) such that \({[x]}^n = 0\),
    and \(R/I\) is not reduced.
    The reverse argument holds the same way since all those logical steps are bidirectional.
\end{proof}

\begin{theorem}
    Maximal ideals are prime, and prime ideals are radical.
\end{theorem}
\begin{proof}
    We first prove that maximal ideals are prime.
    Suppose \(I \subseteq R\) is maximal.
    Then \(R/I\) is a field by Theorem~\ref{thm:maximal-quotient-field},
    and because fields are domains,
    \(I\) is prime by Theorem~\ref{thm:prime-quotient-domain}.

    We then prove that prime ideals are radical.
    Suppose \(I \subseteq R\) is prime.
    Then \(R/I\) is a domain by Theorem~\ref{thm:prime-quotient-domain},
    and because domains are reduced (since any nilpotent element would be a zero divisor),
    \(I\) is reduced by Theorem~\ref{thm:radical-quotient-reduced}.
\end{proof}

\begin{theorem}[Krull's Theorem]\label{thm:maximal-ideal-exists}\label{thm:krull}
    Every ring has a maximal ideal.
\end{theorem}
\begin{proof}
    Let \(\Sigma\) be the set of all proper ideals of some ring \(R\).
    Let this set be partially ordered by \(\subseteq\), the subset relation.
    We first see that \(\Sigma\neq\emptyset\) due to \((0) \in \Sigma\).
    We then claim that every chain \(I_1 \subseteq I_2 \subseteq I_3 \subseteq \cdots\)
    is contained within the union \(I = \bigcup_i I_i \in \Sigma\).
    By Proposition~\ref{prop:nested-ideals}, \(I\) is an ideal,
    and by definition, \(I_i \subseteq I\).
    We also see that \(I \neq R\) because if \(1 \in I\),
    then \(1 \in I_i\) for some \(i\), which gives us a contradiction.
    Hence by \hyperref[ax:zorn]{Zorn's lemma}, there is a maximal element, a maximal ideal.
\end{proof}
\begin{corollary}\label{cor:proper-in-maximal}
    Every proper ideal lies in a maximal ideal.
\end{corollary}
\begin{proof}
    The \hyperref[thm:maximal-ideal-exists]{proof above} shows that
    every ideal lies in a chain,
    which is contained within a maximal ideal.
\end{proof}
\begin{corollary}
    Suppose \(R\) is a ring, and \(x \in R\) is not a unit.
    Then \(x\) lies in a maximal ideal.
\end{corollary}
\begin{proof}
    If \(x\) is not a unit,
    then \((x) \neq (1)\) by Proposition~\ref{prop:unit-one-ideal},
    so it must be a proper ideal,
    and the \hyperref[cor:proper-in-maximal]{corollary above} holds.
\end{proof}

\begin{definition}
    Suppose \(R\) is a ring.
    \(R\) is a local ring if it has a unique maximal ideal \(m \subseteq R\).
\end{definition}
\begin{theorem}
    Suppose \((R,m)\) forms a local ring.
    Then \(R-m\) is the group of units in \(R\).
\end{theorem}
\begin{proof}
    Suppose \(x \in R\) is not a unit.
    Then Proposition~\ref{prop:unit-one-ideal} tells us that \((x) \neq (1)\),
    so it must be a proper ideal,
    and Corollary~\ref{cor:proper-in-maximal} indicates \(x \in M\).
    Hence \(R-m\) is a subset of the group of units.

    If \(x \in R\) is a unit,
    then Proposition~\ref{prop:unit-one-ideal} tells us that \((x) = (1)\),
    so if \(x \in I\) some ideal, \((x) \subseteq I = (1)\),
    which cannot be maximal.
    Hence the group of units is a subset of \(R-m\).
\end{proof}
\begin{proposition}\label{prop:local-ring-units}
    Suppose \(R\) is a ring, and \(I \subseteq R\) is a proper ideal.
    If every \(x \in R-I\) is a unit, then \((R,I)\) is a local ring.
\end{proposition}
\begin{proof}
    Consider any arbitrary proper ideal \(J \subsetneq R\).
    \(J\) contains no units, so \(J \subseteq I\),
    and hence \(I\) is the unique maximal ideal.
\end{proof}
\begin{proposition}
    Suppose \(R\) is a ring, and \(m \subseteq R\) is a maximal ideal.
    If for every \(x \in m\), \(1+x\) is a unit, then \((R,m)\) is a local ring.
\end{proposition}
\begin{proof}
    Consider any \(x \in R-m\).
    Since \(m\) is a maximal ideal,
    the ideal generated by \(x\) and all the elements in \(m\)
    must be the whole ring \((x,m) = (1)\).
    Then by definition there exists \(r \in R\) and \(\mu \in m\) such that \(rx-\mu = 1\)
    (note that the choice of sign with \(-\mu\) is by convenience).
    But under our assumption, \(rx = 1+\mu\) is a unit,
    so there exists some \(y\) such that \((rx)y = (ry)x = 1\),
    and \(x\) is a unit.
    The \hyperref[prop:local-ring-units]{proposition above}
    then indicates that \((R,m)\) forms a local ring.
\end{proof}

\subsection*{Radicals and Nilradicals}

\begin{definition}
    Suppose \(R\) is a ring.
    The set of nilpotent elements \(\Nil(R)\) is called the nilradical of \(R\).
\end{definition}
\begin{lemma}
    The nilradical \(\Nil(R)\) is an ideal.
\end{lemma}
\begin{proof}
    Consider two nilpotent elements \(x,y\).
    Then there exists some \(m,n\) such that \(x^m = y^n = 0\).
    Then \({(x+y)}^{m+n} = \sum_{r=0}^{m+n} \binom{m+n}{r} x^r y^{m+n-r}\) is also nilpotent,
    since every term is there involves either \(x^m\) or \(y^n\).

    Consider nilpotent \(x\) and \(r \in R\).
    Then if \(x^n = 0\), then \({(rx)}^n = r^n x^n = 0\).
\end{proof}
\begin{theorem}
    Suppose \(R\) is a ring, and \(N = \Nil(R)\) its nilradical.
    Then \(R/N\) is reduced.
\end{theorem}
\begin{proof}
    Consider any \([x] \in R/N\) that is nilpotent \({[x]}^n = 0\).
    Then we know that \(x^n \in N\) is in the kernel,
    but by definition there exists some \(m\) such that \(x^{nm} = 0\).
    This shows that \(x \in N\), so \([x] = 0\).
\end{proof}
\begin{theorem}\label{thm:nilradical-intersection-prime}
    The nilradical is the intersection of all prime ideals.
\end{theorem}
\begin{proof}
    Let \(N = \Nil(R)\).
    Consider \(x \in N\). Then \(x^n = 0 \in P\) for any \(P\) prime ideal,
    which implies that \(x \in P\).
    This shows that \(N \subseteq \bigcap P\).

    On the other hand, if \(x \notin N\),
    then consider the set of all ideals that contains powers of \(x\),
    \(\Sigma = \{I \subseteq R: \forall n > 0,\, x^n \notin I\}\).
    Obviously \((0) \in \Sigma\).
    Now consider the any arbitrary chain of ideals \(I_1 \subseteq I_2 \subseteq \cdots\).
    We claim that this is bounded above by the union
    \(I = \bigcup_{i=1}^\infty I_i \in \Sigma\).
    Clearly \(x^n \notin I\), because if it is, \(x^n \in I_i\) for some \(i\).
    We then apply \hyperref[ax:zorn]{Zorn's lemma} to see that there is a maximal element of \(\Sigma\),
    which we shall call \(P\).

    We wish to show that \(P\) is prime.
    Consider any arbitrary \(y,z \notin P\).
    Then we see that \((y)+P\) and \((z)+P\) cannot be in \(\Sigma\)
    because it would otherwise violate maximality,
    which implies there exists some \(m,n\) such that \(x^m \in (y)+P\) and \(x^n \in (z)+P\).
    Hence we know that \(x^{m+n} \in (yz)+P\),
    and therefore \(yz \notin P\), so \(P\) is indeed prime.
    In particular, \(x \notin P\) since \(x\) cannot reside in any element of \(\Sigma\).
\end{proof}

\begin{definition}
    Suppose \(R\) is a ring, and \(I \subseteq R\) is an ideal.
    The radical of \(I\) is \(\sqrt{I} = \{x \in R: \exists n > 0,\, x^n \in I\}\).
\end{definition}
\begin{lemma}
    \(\sqrt{I}\) is an ideal.
\end{lemma}
\begin{proof}
    Consider any \(x,y \in \sqrt{I}\).
    Then there exists some \(x^m \in I\) and \(y^n \in I\).
    so by the same argument as the nilradical \({(x+y)}^{m+n} \in I\),
    and hence \(x+y \in \sqrt{I}\).

    Consider any \(x \in \sqrt{I}\) and \(r \in R\).
    Then there exists some \(x^n \in I\),
    and \({(rx)}^n = r^n x^n \in I\), so \(rx \in \sqrt{I}\).
\end{proof}
\begin{theorem}
    The radical \(\sqrt{I}\) is the intersection of all prime ideals that contain \(I\).
\end{theorem}
\begin{proof}
    By the \hyperref[thm:iso-3-ring]{third isomorphism theorem},
    there is an ideal \(\sqrt{I}/I \subseteq R/I\).
    Observe that \(\sqrt{I}/I = \{[x] \in R/I: \exists n > 0,\, x^n \in I = [0]\}\),
    so \(\sqrt{I}/I = \Nil(R/I)\) is the nilradical.
    By Theorem~\ref{thm:nilradical-intersection-prime},
    \(\sqrt{I}/I\) is in the intersection of all prime ideals in \(R/I\).

    It now suffices to prove that
    the \hyperref[thm:iso-3-ring]{third isomorphism theorem}
    preserves prime ideals an intersections.
    Suppose \(J \subseteq R\) is a prime ideal containing \(I\).
    Then if \(x,y \notin J\), then \(xy \notin J\), in particular \(x,y \notin I\);
    so \([x],[y] \notin J/I\), and \([xy] \notin J/I\).
    Hence \(J/I \subseteq R/I\) is a prime ideal.
    In reverse, let \(J/I \subseteq R/I\) be a prime ideal.
    If \([x],[y] \notin J/I\), and \([xy] \notin J/I\),
    then the preimages cannot be in \(J\),
    so \(J \subseteq R\) is a prime ideal.

    Similarly, suppose \({\{J_i\}}_i\) are ideals of \(R\) containing \(I\).
    If \(x \in J_i\) for all \(i\),
    then \([x] \in J_i/I\) for all \(i\),
    so it preserves intersection.
    For the reverse we can suppose if \({\{J_i/I\}}_i\) are ideals of \(R/I\),
    and if \([x] \in J_i/I\) for all \(i\),
    then the preimages must be in \(J_i\),
    so we are done.
\end{proof}

\begin{definition}
    Suppose \(R\) is a ring.
    The Jacobson radical of \(R\) is the intersection of all maximal ideals,
    \(\mca{R} = \bigcap m\).
\end{definition}
\begin{remark}
    The nilradical is inside the Jacobson radical.
\end{remark}
\begin{lemma}
    Suppose \(R\) is a ring, \(\mca{R}\) is its Jacobson radical.
    Any element \(x \in \mca{R}\) if and only if \(1 - xy\) is a unit for all \(y \in R\).
\end{lemma}
\begin{proof}
    Suppose \(x \in \mca{R}\),
    and by way of contradiction, that there is some \(y\) such that \(1 - xy\) is not a unit.
    Then \((1-xy) \neq (1)\) is a proper ideal by Proposition~\ref{prop:unit-one-ideal},
    so it must be contained within a maximal ideal \(m\) by Corollary~\ref{cor:proper-in-maximal}.
    But since \(x \in \mca{R} \subseteq m\), then \(xy \in m\), so \(1 \in m\),
    which is a contradiction.

    Now suppose for all \(y\), \(1 - xy\) are units,
    but by way of contradiction, that \(x \notin \mca{R}\).
    Then there is some maximal ideal \(m\) that \(x\) does not reside in.
    By maximality of this \(m\), we know that \(m + (x) = (1)\),
    In particular, there is some \(u \in m\) and \(y \in R\) such that \(u + xy = 1\),
    and we see that \(u = 1-xy \in m\) is a unit.
    But units cannot reside in \(m\), because otherwise \(1 \in m\),
    which is a contradiction.
\end{proof}

\subsection*{Coprimality}

\begin{definition}
    Suppose \(I,J\) are two ideals.
    They are coprime if \(I+J = (1)\).
\end{definition}
\begin{theorem}\label{thm:coprime-product-intersection}
    Suppose \({\{J_i\}}_{i=1}^n\) are pairwise coprime ideals.
    Then \(\bigcap_{i=1}^n J_i = \prod_{i=1}^n J_i\).
\end{theorem}
\begin{proof}
    From Proposition~\ref{prop:ideal-operations},
    the product is contained within the intersection,
    so it suffices to only prove the reverse inclusion.

    Let us first consider the case of \(n = 2\).
    Since we know that \(J_1+J_2 = (1)\),
    we can exploit this property using the \hyperref[prop:ideal-distributive]{distributive properties}.
    \begin{equation*}
        J_1 \cap J_2 = (J_1 \cap J_2)R = (J_1 \cap J_2)(J_1+J_2)
        = (J_1 \cap J_2)J_1 + (J_1 \cap J_2)J_2
        \subseteq J_2 J_1 + J_1 J_2
        \subseteq J_1 J_2
    \end{equation*}

    We now inductively prove this for \(n \geq 2\),
    by assuming that \(J = \bigcap_{i=1}^{n-1} J_i = \prod_{i=1}^{n-1} J_i\),
    and proving that \(J \cap J_n \subseteq J J_n\).
    But the exact same proof above works if we have \(J+J_n = (1)\).
    Consider, for each \(i\), some \(x_i \in J_i\).
    Then by pairwise coprimality,
    we can find some corresponding \(y_i \in J_n\) such that \(x_i+y_i = 1\).
    But we can then see that
    \begin{equation*}
        \prod_{i=1}^{n-1} x_i = \prod_{i=1}^{n-1} (1-y_i) = 1 - y
    \end{equation*}
    where \(y\) is a sum of products of \(y_i\),
    which is clearly in \(J_n\),
    so \(J+J_n = (1)\).
\end{proof}

\begin{definition}
    Suppose \({\{R_i\}}_{i \in S}\) is a family of rings.
    The direct product is \(\prod_{i \in S} R_i = \{(r_1,r_2,\hdots) : r_i \in R_i\}\).
\end{definition}
\begin{lemma}\label{lem:ideal-projections}
    Suppose \(R\) is a ring, and \(\{J_i\}\) is a family of ideals.
    Let us define the homomorphism \(\vfunc{\phi}{R}{\prod_i R/J_i}{r}{{(r+J_i)}_{i \in S}}\).
    Then we have \(\ker\phi = \bigcap_i J_i\),
    and \(\img\phi = R/\bigcap_i J_i\).
\end{lemma}
\begin{proof}
    Consider \(x \in \bigcap_i J_i\).
    Then clearly \(\phi(x) = {(x+J_i)}_i = {(J_i)}_i\)
    which are exactly the cosets of 0.
    We have \(\bigcap_i J_i \subseteq \ker\phi\).

    But now observe any element \(x \in \ker\phi\).
    Then we need \(\phi(x) = {(J_i)}_i\),
    so it must belong to the zero \(J_i\)-coset,
    hence \(x \in J_i\).
    We have the reverse inclusion,
    which combined with the above gives equality.

    The image is the result of the
    \hyperref[thm:iso-1-mod]{first isomorphism theorem},
    which gives us \(\img\phi = R/\ker\phi\).
\end{proof}
\begin{theorem}[Chinese Remainder Theorem]
    Suppose \(R\) is a ring, and \({\{J_i\}}_{i=1}^n\) is a family of ideals.
    Consider the homomorphism \(\func{\phi}{R}{\prod_i R/J_i}\).
    Then \(\phi\) is surjective if and only if \({\{J_i\}}_{i=1}^n\) are pairwise coprime.
    In that case, \(\prod_i R/J_i \cong R/\prod_i J_i\).
\end{theorem}
\begin{proof}
    Suppose \(\phi\) is surjective.
    Then in particular, there exists \(x \in R\)
    such that \(\phi(x) = (1,0,\hdots,0)\).
    This gives us that \(x \in 1 + J_1\) and \(x \in J_2\).
    Hence there exists \(j_1 \in J_1\) and \(j_2 \in J_2\)
    such that \(1 + j_1 = j_2\),
    which gives us \(J_1 + J_2 = (1)\).
    This proof can be repeated with arbitrary indices replacing 1 and 2.

    Suppose \({\{J_i\}}_{i=1}^n\) are pairwise coprime.
    Then there exists \(x_i \in J_i\) and \(y_i \in J_1\)
    such that \(x_i + y_i = 1\).
    Using the same argument as Theorem~\ref{thm:coprime-product-intersection},
    there exists
    \begin{equation*}
        b_1 = \prod_{i=2}^n x_i = \prod_{i=2}^n (1-y_i) = 1 - y
        \in \prod_{i=2}^n J_i \subseteq \bigcap_{i=2}^n J_i
    \end{equation*}
    with \(y \in J_1\).
    Hence we have found a \(b_1\) such that \(\phi(b_1) = (1,0,\hdots,0)\).
    Repeat this proof with an arbitrary index instead of 1 to obtain all \(b_i\).
    Then any arbitrary \((r_1,\hdots,r_n)\)
    will be mapped from \(\sum_{i=1}^n r_i b_i\).

    Now, if \(\phi\) is surjective,
    from the \hyperref[lem:ideal-projections]{lemma above}
    we have \(\img\phi = \prod_i R/J_i = R/\bigcap_i J_i\),
    and by pairwise coprimality (Theorem~\ref{thm:coprime-product-intersection}),
    \(R/\bigcap_i J_i = R/\prod_i J_i\).
\end{proof}
\begin{remark}
    The Chinese Remainder Theorem is named after ancient China,
    where it was believed to be used to count the number of soldiers remaining after a battle.
    Soldiers were to be instructed line up in rows of 7, 11, and then 13,
    where after each instruction,
    the remainder in the last unfilled row would be counted.
    Then we can see that \(\bZ/(7) \times \bZ/(11) \times \bZ/(13) \cong \bZ/(1001)\),
    which if your force does not exceed a thousand men,
    gives you the exact number of soldiers.
\end{remark}

\subsection*{Quotient Ideals}

\begin{definition}
    Suppose \(R\) is a ring, and \(I,J\) are ideals.
    The quotient ideal is \((I:J) = \{x \in R : xJ \subseteq I\}\).
\end{definition}
\begin{lemma}
    \((I:J)\) indeed forms an ideal.
\end{lemma}
\begin{proof}
    Suppose \(x,y \in (I:J)\).
    Then \((x+y)J = xJ + yJ \subseteq I\),
    so \(x+y \in (I:J)\).

    Consider \(x \in (I:J)\) and \(r \in R\).
    Then we have \((xr)J = x(rJ) \subseteq xJ \subseteq I\),
    so \(xr \in (I:J)\).
\end{proof}
\begin{proposition}
    The following properties hold for quotient ideals:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(I \subseteq (I:J)\);
        \item \((I:J)J \subseteq I\);
        \item \((I:I) = (1)\); and
        \item \(((I:J):K) = (I:JK) = ((I:K):J)\).
    \end{enumerate}
\end{proposition}
\begin{proof}
    For (a), if \(x \in I\), then \(xJ \subseteq xR \subseteq I\),
    so \(x \in (I:J)\).

    For (b), consider \(x \in (I:J)\) and \(y \in J\).
    Then \(xJ \subseteq I\), and in particular \(xy \in I\).

    For (c), we see that \(1I = I\), so \(1 \in (I:I)\).

    For (d), consider \(x \in ((I:J):K)\).
    Then \(xK \in (I:J)\),
    which implies that \(xJK \in I\),
    which exactly follows the definition of \(x \in (I:JK)\).
    This proof is exactly the same in the reverse direction.
\end{proof}

\subsection*{Extensions and Contractions}

\begin{definition}
    Suppose \(\func{\phi}{R}{S}\) is any ring homomorphism.
    Let \(I \subseteq R\) and \(J \subseteq S\) be ideals.
    \(\phi(I) \subseteq S\) might not be an ideal,
    so we define the ideal it generates \((\phi(I)) \subseteq I\)
    to be the extension of \(I\).
\end{definition}
\begin{remark}
    \(\phi^{-1}(J) \subseteq R\) is always an ideal,
    and if \(J\) prime, \(\phi^{-1}(J)\) is also prime.
\end{remark}
\begin{definition}
    Suppose \(\func{\phi}{R}{S}\) is a ring monomorphism.
    If \(J \subseteq S\), we say \(\phi^{-1}(J) = J \cap R\) is the contraction of \(J\).
\end{definition}


\section{Field of Fractions}

\begin{definition}
    Suppose \(R\) is a ring,
    and \(\func{\zeta}{\bZ}{R}\) the canonical homomorphism.
    If \(\zeta\) is injective, % then the kernel is \(\ker\zeta = \{0\}\),
    % so \(\zeta(\bZ) \subseteq R\), and
    we call \(R\) having characteristic 0.
    If on the other hand \(\zeta\) is not injective,
    then the kernel clearly must be some ideal \(\ker\zeta = n\bZ\),
    % so \(R \cong \bZ/n\bZ\),
    and we call \(R\) having characteristic \(n\).
\end{definition}
\begin{proposition}
    Suppose \(p\) prime.
    Then the ring \(\bZ/p\bZ\) must be a field.
\end{proposition}
\begin{proof}
    We first prove that \(p\bZ\) is maximal in \(\bZ\),
    i.e.\ there does not exist an ideal \(n\bZ\),
    such that \(p\bZ \subsetneq n\bZ \subsetneq \bZ\).
    Clearly if \(p\bZ \subseteq n\bZ\),
    then \(n \mid p\), which tells us \(n = 1\) or \(p\),
    which is either \(\bZ\) or \(p\bZ\).
    Hence the only ideals of \(\bZ\) that contain \(p\bZ\)
    are \(\bZ\) and \(p\bZ\).

    By (the contraposition of)
    the \hyperref[thm:iso-3-ring]{third isomorphism theorem},
    then the only ideals of \(\bZ/p\bZ\) are itself and \(p\bZ/p\bZ = \{0\}\),
    so there does not exists proper ideals of \(\bZ/p\bZ\).
    Then by Theorem~\ref{thm:field-props}, \(\bZ/p\bZ\) is a field.
\end{proof}
\begin{definition}
    For some prime \(p\),
    we call \(\bZ/p\bZ = \mathbb{F}_p\)
    the finite field of \(p\) elements.
\end{definition}

\begin{lemma}\label{lem:field-no-zero-divisors}
    Fields do not have nonzero zero divisors.
\end{lemma}
\begin{proof}
    If \(x,y \neq 0\) and \(xy = 0\),
    \(x^{-1}\) exists, so \(x^{-1}xy = y = 0x^{-1} = 0\)
    which imply \(y = 0\), which is a contradiction in itself.
\end{proof}
\begin{lemma}\label{lem:field-prime-kernel}
    Suppose \(F\) is a field,
    and \(\func{\zeta}{\bZ}{F}\) is the canonical homomorphism.
    Then either \(\zeta\) is injective,
    or \(\ker\zeta = p\bZ\) for some prime \(p\).
\end{lemma}
\begin{proof}
    If \(\zeta\) is not injective,
    then clearly the kernel is some \(\ker\zeta = n\bZ\).
    Suppose, by way of contradiction, that \(n = ab\) such that \(1 < a,b < n\).
    Then \(0 = \zeta(n) = \zeta(ab) = \zeta(a)\zeta(b)\),
    and since \(a,b\) are not elements of \(n\bZ\),
    \(\zeta(a),\zeta(b)\) are nonzero,
    and in particular, they are zero divisors.
    But that contradicts the \hyperref[lem:field-no-zero-divisors]{lemma above}.
    Hence \(n\) must be prime.
\end{proof}
\begin{theorem}\label{thm:field-unique-prime-char}
    The characteristic of a field is unique,
    and it is either 0 or a prime \(p\).
\end{theorem}
\begin{proof}
    The \hyperref[lem:field-prime-kernel]{lemma above}
    already proves that the characteristic is either 0 or prime.
    It is now sufficient to prove uniqueness.
    Suppose \(\mathbb{F}_p, \mathbb{F}_q \subseteq F\) some field.
    then \(\mathbb{F}_p \subseteq F\) implies \(p1 = 0\),
    while \(\mathbb{F}_q \subseteq F\) implies \(q1 = 0\),
    But \(\gcd(p,q) = 1\), which implies \(1 = 0\),
    which is considered a trivial ring and not a field.
\end{proof}

\begin{definition}[Universal Property of Field of Fractions]
    Suppose \(R\) is a commutative domain.
    We call \(F\) its field of fractions or its quotient field if:
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item there exists an monomorphism \(\func{\iota}{R}{F}\); and
        \item for any field \(S\), if \(\func{\phi}{R}{S}\) is a monomorphism,
            then there exists a unique \(\func{\bar{\phi}}{F}{S}\)
            such that \(\phi = \bar{\phi}\circ\iota\).
    \end{enumerate}
    This is represented by the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            R \arrow{r}{\phi} \arrow{d}{\iota} & S \\
            F \arrow[dashrightarrow]{ru}[swap]{\exists! \bar{\phi}}
        \end{tikzcd}
    \end{center}
\end{definition}
\begin{theorem}[Existence and Uniqueness of Field of Fractions]
    For any commutative domain \(R\),
    its field of fractions \(F\) exists and is unique up to isomorphism.
\end{theorem}
\begin{proof}
    Again, as with all universal properties,
    we first prove uniqueness assuming existence.
    Suppose, by way of contradiction,
    that \(F\) and \(F'\) are both fields of fractions.
    Then by definition there exists \(\func{\iota}{R}{F}\) injective,
    and for any field \(S\), in particular \(F'\),
    \(\func{\iota'}{R}{F'}\) injective
    such that there exists a unique \(\func{\bar{\phi}}{F}{F'}\)
    where \(\iota' = \bar{\phi}\circ\iota\).
    But same can be said for \(\func{\iota'}{R}{F'}\) injective,
    for any field \(S\), in particular \(F'\),
    \(\func{\iota}{R}{F}\) injective
    such that there exists a unique \(\func{\bar{\phi}'}{F'}{F}\)
    where \(\iota = \bar{\phi}'\circ\iota'\).
    \begin{center}
        \begin{tikzcd}
            R \arrow{r}{\iota'} \arrow{d}[swap]{\iota} &
            F' \arrow[rightharpoondown, shift right=0.25ex]{ld}%
                [xshift=.5ex, yshift=-.5ex, swap]{\bar{\phi}'} \\
            F \arrow[rightharpoondown, shift right=0.25ex]{ru}%
                [xshift=-.5ex, yshift=.5ex, swap]{\bar{\phi}}
        \end{tikzcd}
    \end{center}

    Then following the definitions,
    we come to these two conclusions:
    \begin{align*}
        \bar{\phi}'\circ\bar{\phi}\circ\iota = \bar{\phi}'\circ\iota' = \iota
        &\implies \bar{\phi}'\circ\bar{\phi} = \id_F \\
        \bar{\phi}\circ\bar{\phi}'\circ\iota' = \bar{\phi}\circ\iota = \iota'
        &\implies \bar{\phi}\circ\bar{\phi}' = \id_{F'}
    \end{align*}
    which by definition means \(\bar{\phi}' = \bar{\phi}^{-1}\).
    Hence \(\bar{\phi}\) is a bijection,
    telling us that \(F \cong F'\) are isomorphic.

    Now we prove existence.
    Suppose we have tuples \((x,y) \in R \times (R - \{0\})\),
    i.e.\ the standard condition that the denominator does not equal 0.
    We shall define an equivalence relation
    \((a,b) \sim (c,d)\) if \(ad = bc\).
    We check the three conditions:
    \((a,b) \sim (a,b)\) since \(ab = ab\) (reflexive);
    \((a,b) \sim (c,d)\) implies \(ad = bc\)
    implies \(cb = da\) implies \((c,d) \sim (a,b)\) (symmetric);
    and \((a,b) \sim (c,d)\) and \((c,d) \sim (e,f)\)
    implies \(ad = bc\) and \(cf = de\),
    so \(adcf = bcde\), when cancelling out \(cd\) we have \(af = be\),
    which implies \((a,f) \sim (b,e)\) (transitive).

    We define \(F = (R \times (R - \{0\}))/{\sim}\),
    i.e.\ elements are \((x,y) \in R \times (R - \{0\})\)
    without unique representation.
    We shall denote elements of \(F\) as \(\overline{(x,y)} \in F\).
    This forms a ring with addition and multiplication defined as
    \begin{equation*}
        \overline{(a,b)} + \overline{(c,d)} = \overline{(ad+bc,bd)} \qquad
        \overline{(a,b)} \cdot \overline{(c,d)} = \overline{(ac,bd)}
    \end{equation*}
    The ring axioms are obvious,
    and essentially encompass the operations on \(\bQ\).

    Lastly, we check that it satisfies the morphisms.
    % We can start again by proving uniqueness first,
    % but we presume that the operations on \(\bQ\) are familiar enough
    % that we shall simply construct the morphisms.
    We define \(\vfunc{\iota}{R}{F}{x}{\overline{(x,1)}}\),
    and for some arbitrary \(\func{\phi}{R}{S}\),
    let \(\vfunc{\bar{\phi}}{F}{S}{\overline{(x,y)}}{\phi(x){\phi(y)}^{-1}}\).
    We check that for arbitrary \((x,y) \in R \times (R - \{0\})\),
    \begin{equation*}
        \bar{\phi}(\iota(x)){\bar{\phi}(\iota(y))}^{-1}
        = \bar{\phi}\overline{(x,1)}{\bar{\phi}\overline{(y,1)}}^{-1}
        = \phi(x){\phi(1)}^{-1}{\phi(y)}^{-1}\phi(1)
        = \phi(x){\phi(y)}^{-1}
    \end{equation*}
    so \(\phi = \bar{\phi}\circ\iota\),
    and our definition is one such set of morphisms, proving existence.
    %
    % Now suppose, by way of contradiction,
    % there exists another \(\bar{\phi}'\)
    % such that it satisfies the commutative diagram,
    % i.e.\ \(\bar{\phi}'\overline{(x,y)} \neq \phi(x){\phi(y)}^{-1}\)
    % for some \(x,y\).
    % But then \(\phi(x){\phi(y)}^{-1}
    % \neq \bar{\phi}'(\iota(x)){\bar{\phi}'(\iota(y))}^{-1}\),
    % which implies either \(\phi(x) \neq \bar{\phi}'(\iota(x))\)
    % or \(\phi(y) \neq \bar{\phi}'(\iota(y))\),
    % either of which would lead to a contradiction.
\end{proof}
\begin{remark}
    Notice that this time we use a universal property
    as a definition for an algebraic structure.
    This is a way to generalize constructions
    in terms of morphisms instead of elements,
    which allow us to define things mostly up to isomorphism.
\end{remark}
\begin{remark}
    Because of how we define \(\bar{\phi}\),
    it is common to write \(\overline{(a,b)} = ab^{-1} = a/b\),
    which is the common notation for fractions.
\end{remark}


\section{Polynomial Rings}

\begin{definition}
    For some commutative ring \(R\),
    we call the polynomials with coefficients in \(R\)
    the polynomial ring \(R[x] = \{\sum_{i=0}^n a_i x^i : a_i \in R, n \geq 0\}\).
    This can be represented as an infinite sequence of coefficients
    with finitely many nonzero terms (with finite support),
    i.e.\ \(R[x] \cong \bigoplus_{i=0}^\infty R\).
\end{definition}
\begin{remark}
    Notice that the indeterminate \(x\) is not restricted to any set,
    and can be not in \(R\);
    an analogy is that although we often look at polynomials \(\bR[x]\),
    the solutions (and thereofre the indeterminates \(x\) that we investigate)
    are sometimes in the bigger field \(\bC\).
\end{remark}
\begin{remark}
    We shall again explore more of the direct sum
    in Chapter~\ref{sec:linear-algebra} for linear algebra.
\end{remark}
\begin{proposition}
    \(R[x]\) forms a ring with addition
    \(\sum a_i x_i + \sum b_i x_i = \sum (a_i+b_i) x_i\)
    and multiplication
    \((\sum a_i x_i)(\sum b_j x_j) = \sum_k (\sum_{i+j=k} a_i b_j) x_k\).
\end{proposition}
\begin{proof}
    Addition is term-wise and inherits all properties from \(R\).
    Multiplication is clearly closed, associative, commutative,
    and has an identity \(1 \in R \subseteq R[x]\).
\end{proof}

\begin{theorem}[Universal Property of Polynomial Rings]\label{thm:univ-prop-polynomial}
    Suppose \(R,S\) are commutative rings,
    and \(R[x]\) is a univariate polynomial ring.
    For all homomorphisms \(\func{\phi}{R}{S}\),
    and for all \(u \in S\),
    there exists a unique homomorphism \(\vfunc{\eta}{R[x]}{S}{x}{u}\),
    such that \(\phi = \eta\circ\iota_k\) for all \(k \in \bZ_0^+\).

    This can be represented by the logical statement
    \begin{equation*}
        \forall S \in \mathbf{Ring},\;\forall \phi \in \Hom(R,S),\;
        \forall u \in S,\;\exists!\vfunc{\eta}{R[x]}{S}{x}{u},\;
        \phi = \eta\circ\iota_k
    \end{equation*}
    and the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            R \arrow{r}{\phi} \arrow{d}{\iota_k} & S \\
            R[x] \arrow[dashrightarrow]{ru}[swap]{\exists! \eta}
        \end{tikzcd}
    \end{center}
\end{theorem}
\begin{proof}
    We prove uniqueness first.
    Suppose, by way of contradiction,
    that there exists \(\eta \neq \eta'\) that maps \(R[x] \to S\).
    Since we know \(R[x] \cong \bigoplus_k R\),
    any element \(p \in R[x]\) can be written as \(p = \sum_k \iota_k(a_k)\)
    for some \(a_k \in R\).
    Assume \(\eta\) and \(\eta'\) differ at \(p\).
    Then clearly \(\eta(p) = \sum_k (\eta\circ\iota_k)(a_k)\),
    and same goes for \(\eta'\),
    but that implies \(\eta'\circ\iota_k \neq \eta\circ\iota_k = \phi\)
    at at least one \(a_k \in R\),
    which is a contradiction.

    Now we prove existence.
    Let \(\vfunc{\iota_k}{R}{R[x]}{a}{a_k x^k}\),
    and \(\vfunc{\eta}{R[x]}{S}{b}{\phi(b)},\; x \mapsto u\),
    i.e.\ evaluating the polynomial at \(x = u\).
    We see that the morphisms follow as expected:
    \begin{equation*}
        \sum_k \eta(\iota_k(a_k)) = \sum_k \eta(a_k x^k)
        = \sum_k \eta(a_k){\eta(x)}^k = \sum_k \phi(a_k)u^k
    \end{equation*}
\end{proof}

% TODO: construction of C from R[x]/(x^2+1)?

\begin{definition}
    Suppose \(R\) is a commutative ring.
    We can inductively define the multivariate polynomial ring
    as \(R[x_1,x_2,\hdots,x_n] = R[x_1,x_2,\hdots,x_{n-1}][x_n]\).
\end{definition}
\begin{theorem}\label{thm:polynomial-ring-isomorphism}
    Suppose \(R\) is a commutative ring,
    \(R[x_1,x_2,\hdots,x_n]\) a polynomial ring in \(n\) variables
    and for some permutation of variables
    \(\sigma \in S_n\), \(y_i = x_{\sigma(i)}\),
    \(R[y_1,y_2,\hdots,y_n]\) another polynomial ring in \(n\) variables.
    Then \(R[x_1,x_2,\hdots,x_n] \cong R[y_1,y_2,\hdots,y_n]\),
    that is, polynomial rings with the same number of variables
    are unique up to isomorphism.
\end{theorem}
\begin{proof}
    By the \hyperref[thm:univ-prop-polynomial]{universal property},
    univariate polynomial rings are unique up to isomorphism.
    Hence we have \(R[x_1] \cong R[y_1]\).
    Then by induction,
    suppose \(R[x_1,x_2,\hdots,x_k] \cong R[y_1,y_2,\hdots,y_k]\).
    Then clearly we can write a bijection \(\beta\) between these two,
    and suppose \(a_i \in R[x_1,x_2,\hdots,x_k]\).
    Elements of \(R[y_1,y_2,\hdots,y_{k+1}]\)
    can now be written as \(\sum_i \beta(a_i) y_{k+1}^i\),
    so we have \(R[x_1,x_2,\hdots,x_k][y_{k+1}] \cong R[y_1,y_2,\hdots,y_{k+1}]\).
    By the \hyperref[thm:univ-prop-polynomial]{universal property} again, we have
    \(R[x_1,x_2,\hdots,x_{k+1}] \cong R[x_1,x_2,\hdots,x_k][y_{k+1}]\),
    so \(R[x_1,x_2,\hdots,x_{k+1}] \cong R[y_1,y_2,\hdots,y_{k+1}]\).
\end{proof}

\begin{definition}
    Suppose we have \(R \subseteq S\) two rings,
    and a polynomial ring \(R[x]\).
    Choosing an arbitrary element \(u \in S\),
    we can write the evaluation morphism as
    \(\vfunc{\eta_u}{R[x]}{S}{x}{u}\).
    We call the image the subring generated by \(u\) over \(R\),
    commonly written as \(\eta_u(R[x]) = R[u] \subseteq S\).
\end{definition}
\begin{proposition}
    \(R[u] \cong R[x]/I\), where \(I = \{p(x) \in R[x] : p(u) = 0\}\).
\end{proposition}
\begin{proof}
    A direct consequence of
    the \hyperref[thm:iso-1-ring]{first isomorphism theorem}.
\end{proof}

\begin{definition}
    Suppose \(R \subseteq S\) both fields,
    and \(R[x]\) is a polynomial ring.
    For some arbitrary \(u \in S\),
    consider the evaluation morphism \(\vfunc{\eta_u}{R[x]}{S}{x}{u}\).
    If the kernel is \(\ker\eta_u = \{0\}\),
    then \(R[u] \cong R[x]\), and we call \(u\) transcendental;
    if on the other hand, the kernel \(I = \ker\eta_u \neq \{0\}\),
    then \(R[u] \cong R[x]/I\), and we call \(u\) algebraic,
    in the sense that \(u\) satisfies polynomials,
    in particular the ones in \(I\).
\end{definition}

\begin{theorem}
    Suppose \(R \subseteq S\) both fields,
    and \(R[x]\) a polynomial ring.
    Suppose \(I \subseteq R[x]\) is any ideal.
    If \(I\) contains any constant \(r \in R\), \(r \neq 0\)
    then it is not a substitution kernel,
    i.e.\ it cannot be the kernel for \(\eta_u\) for any \(u \in S\).
\end{theorem}
\begin{proof}
    By way of contradiction, suppose \(r \in I\) and \(r \in R\), \(r \neq 0\),
    and that there exists some \(u \in S\)
    such that \(R[u] \cong R[x]/I\).
    Without loss of generality, let us choose the smallest \(r\).
    Now, this would imply that \(r = 0\) in \(S\).
    Hence \(S\) must have characteristic \(r\).
    Then this tells us \(R\) must also have characteristic \(r\),
    since \(R \subseteq S\).
    But this is a contradiction,
    as this would make \(R \cong S \cong \bZ/r\bZ\);
    as \(R \subseteq R[u] \subseteq R[x] \subseteq S\),
    then the kernel must be \(\{0\}\).
\end{proof}

\begin{definition}
    We define a function \(\func{\deg}{R[x]}{\bZ_0^+}\)
    that gives the degree of a polynomial,
    which is the largest \(n\) such that the coefficient \(a_n \neq 0\).
\end{definition}
\begin{lemma}\label{lem:degree-arithmetic}
    Suppose \(f(x) = \sum_i a_i x^i\) and \(g(x) = \sum_i b_i x^i\)
    both elements of \(R[x]\). Then
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(\deg (f+g) \leq \max\{\deg f, \deg g\}\); and
        \item \(\deg (fg) = \deg f + \deg g\).
    \end{enumerate}
\end{lemma}
\begin{proof}
    If \(\deg f \neq \deg g\), then clearly under addition,
    the leading term remains
    the leading term of the polynomial with the larger degree.
    If \(\deg f = \deg g\),
    if the two leading terms do not cancel out,
    the degree remains the same;
    if they do cancel out,
    then the degree must be smaller.
    This proves statement (a).

    The product of the leading terms \(a_n x^n\) and \(b_m x^m\)
    must be \(a_n b_m x^{n+m}\),
    so the degree is the sum of the leading terms;
    notice all other terms will multiply to a smaller power of \(x\).
    This proves statement (b).
\end{proof}

% \begin{definition}
% \end{definition}
\begin{proposition}\label{prop:ring-polynomial-long-div}
    Suppose \(R\) a commutative ring,
    and \(R[x]\) its polynomial ring.
    Let \(f,g \in R[x]\),
    and we have \(f(x) = \sum_{i=0}^n a_i x^i\),
    \(g(x) = \sum_{i=0}^m b_i x^i\).
    We can define long division on polynomials as
    \(b_m^k f(x) = q(x)g(x) + r(x)\)
    for some arbitrary \(k \in \bN\), and \(\{q,r\} \in R[x]\).
    % The above long division algorithm,
    Along with the degree function,
    this allows for the Euclidean algorithm to apply,
    with the criteria that \(\deg r < \deg g\) or \(r = 0\).
\end{proposition}
\begin{proof}
    We first consider the case that \(\deg f < \deg g\).
    Then let \(b_m^0 = 1\), and \(q(x) = 0\),
    so \(r(x) = f(x)\), and immediately \(\deg r = \deg f < \deg g\).

    We now consider the case that \(\deg f \geq \deg g\).
    We can construct the first term of \(q(x)\) as \(q_1(x) = a_n x^{n-m}\),
    so that
    \begin{equation*}
        f_1(x) = b_m f(x) - a_n x^{n-m} g(x)
        = \sum_{i=0}^n a_i b_m x^i - \sum_{i=n-m}^n a_n b_{i-n+m} x^i
        = \sum_{i=0}^{n-1} a_i b_m x^i - \sum_{i=n-m}^{n-1} a_n b_{i-n+m} x^i
    \end{equation*}
    leaving us with an \(f_1\) that has at least one degree lower than \(f\).
    We can recursively repeat this process
    \begin{equation*}
        f_{i+1}(x) = b_m f_i(x) - q_{i+1}(x)g(x)
        = b_m f_i(x) - a_{\deg f_i} x^{\deg f_i - m} g(x)
    \end{equation*}
    until the \(\deg f_k\) drops below \(\deg g\),
    which allows us to write down \(r(x) = f_k(x)\) at that point,
    and \(q(x) = \sum_{i=1}^k q_i(x)\).
    \begin{equation*}
        q(x)g(x) + r(x) = \sum_{i=1}^k q_i(x)g(x) + f_k(x)
        = \sum_{i=1}^{k-1} q_i(x)g(x) + b_m f_{k-1}(x)
        = \cdots = b_m^k f(x)
    \end{equation*}
    There is at most \(n-m+1\) steps to this induction,
    so \(k \leq n-m+1\).
\end{proof}
\begin{remark}
    Notice that this long division is not unique,
    as we can make \(k\) arbitrarily larger than the choice above,
    and we merely have to multiply \(q(x)\) by that difference.
\end{remark}
\begin{remark}
    We shall discuss more of Euclidean domains
    in Section~\ref{sec:factorial-rings} for unique factorization domains.
\end{remark}

% \begin{definition}
% \end{definition}
\begin{proposition}\label{prop:field-polynomial-long-div}
    Suppose \(F\) a field, and \(F[x]\) its polynomial ring.
    Let \(f,g \in F[x]\),
    and we have \(f(x) = \sum_{i=0}^n a_i x^i\),
    \(g(x) = \sum_{i=0}^m b_i x^i\).
    We can define long division on polynomials as
    \(f(x) = q(x)g(x) + r(x)\), with \(\{q,r\} \in F[x]\).
    Along with the degree function,
    this allows for the Euclidean algorithm to apply,
    with the criteria that \(\deg r < \deg g\) or \(r = 0\).
\end{proposition}
\begin{proof}
    We have the same base case as \hyperref[prop:ring-polynomial-long-div]{above},
    where if \(\deg f < \deg g\), let \(q(x) = 0\),
    so \(r(x) = f(x)\) and \(\deg r = \deg f < \deg g\).

    Now the inductive case is much easier.
    Let \(f(x) = f_0(x)\).
    Using the same process, alternately calculate
    \begin{gather*}
        q_i(x) = \frac{a_{\deg f_{i-1}}}{b_m} x^{\deg f_{i-1} - m} \\
        f_{i+1}(x) = f_i(x) - q_{i+1}(x)g(x)
        = f_i(x) - \frac{a_{\deg f_{i-1}}}{b_m} x^{\deg f_i - m}
        \sum_{i=0}^m b_i x^i
    \end{gather*}
    so that the leading term gets cancelled out every time.
    Repeat this process inductively until \(\deg f_k < \deg g\),
    which is always possible because the degree drops by at least one every step.
    Again let \(f(x) = f_k(x)\) and \(q(x) = \sum_{i=1}^k q_i(x)\).
    \begin{equation*}
        q(x)g(x) + r(x) = \sum_{i=1}^k q_i g(x) + f_k(x)
        = \sum_{i=1}^{k-1} q_i(x)g(x) + f_{k-1}(x)
        = \cdots = f_0(x) = f(x)
    \end{equation*}
\end{proof}
\begin{theorem}
    Long division in fields is unique.
\end{theorem}
\begin{proof}
    Suppose there are \(q \neq q'\) and \(r \neq r'\)
    that both satisfy the long division process.
    Then \(f(x) = q(x)g(x) + r(x) = q'(x)g(x) + r'(x)\).
    Grouping terms together gives us \((q-q')g(x) = r'(x) - r(x)\),
    so this tells us that \(\deg ((q-q')g) = \deg (r'-r)\).
    But the definition of long division requires \(\deg r < \deg g\),
    so Lemma~\ref{lem:degree-arithmetic} gives us
    \(\deg(r'-r) \leq \deg g < \deg(q-q') + \deg g = \deg ((q-q')g)\),
    which contradicts our equality above.
\end{proof}
\begin{corollary}\label{cor:field-polynomial-pid}
    Suppose \(F\) is a field,
    and \(I \subseteq F[x]\) any ideal.
    Then there exists some element \(f \in I\)
    such that it is generated by that element, \(I = (f(x))\),
    i.e.\ all elements of \(I\) are multiples of \(f(x)\).
\end{corollary}
\begin{proof}
    Pick any \(f(x) \in I\) with the lowest degree.
    If \(g \in I\), by \hyperref[prop:field-polynomial-long-div]{long division},
    \(g(x) = q(x)f(x) + r(x)\), which implies \(r(x) = g(x) - q(x)f(x) \in I\).
    But we also know that \(\deg r < \deg f\),
    which tells us that \(r(x) = 0\),
    giving us \(g(x) = q(x)f(x) \in (f(x))\), and hence \(I \subseteq (f(x))\).
    By definition of ideals, \((f(x)) \subseteq I\).
    Therefore \((f(x)) = I\).
\end{proof}

\begin{definition}
    Suppose \(R\) is a commutative domain.
    We call \(R\) a principal ideal domain (PID)
    if every ideal is generated by a single element.
\end{definition}
\begin{remark}
    In fact, what we have just proven \hyperref[cor:field-polynomial-pid]{above}
    is that for any field \(F\), \(F[x]\) is a PID.\@
    More will be discussed on PIDs
    in Section~\ref{sec:factorial-rings} for unique factorization domains.
\end{remark}

\begin{proposition}\label{prop:polynomial-domain}
    Suppose \(R\) is an integral domain.
    Then \(R[x]\) is also an integral domain.
\end{proposition}
\begin{proof}
    Suppose we have nonzero \(\{f(x),g(x)\} \subset R[x]\)
    Then we know \(\deg f = m > 0\), \(\deg g = n > 0\).
    Then \(\deg(fg) = m+n > 0\),
    so \(f(x)g(x) \neq 0\).
\end{proof}

\begin{theorem}
    Suppose \(F\) is a field, and \(f(x) \in F[x]\).
    For some \(u \in F\), if \(f(u) = 0\), then \((x-u) \mid f(x)\);
    and there exists \(r\) distinct elements \({\{u_i\}}_{i=1}^r \subset F\)
    such that \(\prod_{i=1}^r (x-u_i) \mid f(x)\),
    where \(r \leq \deg f\).
\end{theorem}
\begin{proof}
    Let \(\eta_u\) be the evaluation morphism.
    If \(f(u) = 0\), then \(f(x) \in \ker\eta_u\),
    which is an ideal.
    % We use the fact that
    % \hyperref[thm:pid-ufd]{elements in a PID are uniquely factorable}
    % (we will prove this later),
    Since \(F[x]\) is a PID by Corollary~\ref{cor:field-polynomial-pid},
    these ideals are generated by a single element,
    and we claim that it is \((x-u)\).
    Clearly \(u-u = 0\) also \((x-u) \in \ker\eta_u\),
    and it is the lowest degree.

    We can repeat this process until we cannot find elements in a kernel anymore.
    As every time we divide by \((x-u_i)\),
    the degree decreases by one,
    so we can only do this at most \(\deg f\) times.
\end{proof}
\begin{remark}
    Consider the rational polynomials \(f(x) \in \bQ[x]\).
    We can extend \(\bQ\) to a larger field \(K\)
    by adding the solutions of \(f(x)\).
    Let \(G = \Aut(K)\) be the automorphism group over \(K\).
    The study of Galois theory is that there exists a formula
    for the roots of \(f\) if and only if \(G\) is a solvable group.
\end{remark}

\begin{definition}
    If \(F\) is a field, and \(f(x) \in F[x]\) is a polynomial,
    we call \(f(x)\) irreducible
    if \(f(x) = g(x)h(x)\) implies either \(g\) or \(h\) is constant,
    i.e.\ \(f(x)\) cannot decompose into something with a lower degree.
\end{definition}
\begin{theorem}\label{thm:ideal-divisibility}
    Let \(I = (f(x))\) and \(J = (g(x))\) be two ideals in \(F[x]\).
    Then:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(f(x)\) is irreducible if and only if \(I\) is maximal; and
        \item \(I \subseteq J\) if and only if \(g(x) \mid f(x)\).
    \end{enumerate}
\end{theorem}
\begin{proof}
    We first prove statement (b).
    Suppose \(g(x) \mid f(x)\).
    Then there exists \(h(x) \in F[x]\) such that \(f(x) = g(x)h(x)\),
    and hence \(f(x) \in J = (g(x))\),
    and any multiple of \(f(x)\), that is elements of \((f(x))\) is in \(J\);
    therefore \(I \subseteq J\).

    Suppose \(I \subseteq J\).
    As elements of \(J\) are multiples of \(g(x)\),
    we know that in particular elements of \(I \subseteq J\)
    are multiples of \(g(x)\);
    specifically, as \(f(x) \in I \subseteq J\),
    \(g(x) \mid f(x)\).
    This proves statement (b).

    \medskip

    Now suppose \(I\) is reducible.
    Then \(f(x) = g(x)h(x)\) some product of polynomials.
    In particular, \(g(x) \mid f(x)\), \(\deg g < \deg f\),
    so there exists some \(J = (g(x))\) such that \(I \subsetneq J\),
    as it is obvious that \(I\) cannot include elements of degree \(\deg g\).
    But also \(\deg g \geq 1\), so \(J \subsetneq F[x]\),
    so we have found an intermediate ideal,
    and \(I\) not maximal.

    For the reverse direction,
    suppose \(I\) is not maximal,
    and there exists some \(J = (g(x))\)
    such that \(I \subsetneq J \subsetneq F[x]\).
    Then \(g(x) \mid f(x)\),
    and as there are some elements in \(J\) but not in \(I\),
    there exists multiples of \(g(x)\) that is not a multiple of \(f(x)\).
    Hence \(\deg g < \deg f\),
    because otherwise constant multiplies would work if \(\deg g = \deg f\),
    and therefore \(f(x) = g(x)h(x)\),
    where \(\deg h \geq 1\).
    This proves statement (a).
\end{proof}

\begin{remark}
    From \hyperref[thm:ideal-divisibility]{above},
    if \(f(x)\) is irreducible, then \(I\) is maximal in \(F[x]\),
    which by the \hyperref[thm:iso-4-ring]{fourth isomorphism theorem},
    the only ideals of \(F[x]/I\) are itself and \(\{0\}\),
    and Theorem~\ref{thm:field-props} tells us \(F[x]/I \supset F\) is now a field.
    In a sense we are adding the roots of \(f(x)\) to \(F\) to form \(F[x]/I\).

    Suppose \(f(x) = \prod_i g_i(x)\) factors into irreducible polynomials.
    If \(g_i(x) = x-a\) is linear, then \(a \in F\) is a root;
    if \(g_i(x)\) is nonlinear, then there is no root in \(F\),
    since otherwise it would be able to factor this into linear factors.
    Hence we only need to consider irreducible polynomials
    when looking for roots.

    Suppose \(K = F[x]/I\), where \(I = (f(x))\) for some irreducible \(f\).
    Notice that \(x+I \in I\) is always a solution of \(f(x)\) in \(K\),
    as \(x-(x+I) = I\) is in the ideal by definition,
    which is equivalent to \(0 \in K\).
    So if \(\alpha = x+I \in K\),
    we can merely factor out \(x-\alpha\) from \(f(x)\),
    and recursively use this construction to build all the roots of \(f(x)\).
\end{remark}

\subsection*{Polynomial Functions}

\begin{definition}
    Given some polynomial \(f(x) \in F[x]\),
    we can write a function \(\vfunc{f}{F}{F}{u}{f(u)}\),
    which is the familiar notion of polynomials being treated as functions.
    Again, the roots of the polynomial is the preimage of zero.
\end{definition}
\begin{remark}
    In general, two different polynomials can give the same function,
    such as \(x = x^2\) when considering \(\bZ/2\bZ\).
\end{remark}
\begin{theorem}\label{thm:polynomial-distinct-func}
    In infinite fields, distinct polynomials are distinct as functions.
\end{theorem}
\begin{proof}
    It is again sufficient to consider monic polynomials,
    since we know that polynomials that are off by a multiple
    have the same roots,
    and are clearly distinct from each other everywhere else.

    We prove the contrapositive.
    Suppose \(\{f(x),g(x)\} \subset F[x]\).
    If \(f = g\) as functions, then \(f(u) = g(u)\) for all \(u \in F\),
    then \(f(u) - g(u) = 0\), so \(u\) is a root of \(f - g\).
    But since \(F\) has infinitely many roots,
    which implies \(f - g = \prod_{u \in F} a(x-u)\) is not a valid polynomial,
    unless \(a = 0\).
    Hence \(f(x) = g(x)\) as polynomials.
\end{proof}
\begin{corollary}
    If \(F\) is an infinite field,
    and \(f(x) \in F[x]\) some nonzero polynomial,
    then there exists some \(u \in F\) such that \(f(u) \neq 0\).
\end{corollary}
\begin{proof}
    Special case of the \hyperref[thm:polynomial-distinct-func]{theorem above}
    that all polynomials are distinct from 0.
\end{proof}
\begin{corollary}
    If \(F\) is an infinite field,
    and \(f(x_1,x_2,\hdots,x_n) \in F[x_1,x_2,\hdots,x_n]\)
    some nonzero polynomial in \(n\) variables,
    then there exists some \((u_1,u_2,\hdots,u_n) \in F^n\)
    such that \(f(u_1,u_2,\hdots,u_n) \neq 0\).
\end{corollary}
\begin{proof}
    We can perform induction on the number of variables,
    and use the \hyperref[thm:polynomial-distinct-func]{theorem above}
    as our base case.
    Suppose, by way of induction, that our corollary holds for \(k\) variables.
    We attempt to prove the case for \(k+1\) variables.
    Suppose \(f(x_1,\hdots,x_{k+1}) \in F[x_1,\hdots,x_{k+1}]\)
    is a nonzero polynomial in \(k+1\) variables.
    Without loss of generality, assume \(x_{k+1}\) is in the expression,
    since otherwise it is merely a polynomial in \(k\) variables,
    and the inductive hypothesis holds.
    Then picking some \((u_1,\hdots,u_k) \in F^k\), \(u_i \neq 0\),
    \(f(u_1,\hdots,u_k,x_{k+1}) \in F[x_{k+1}]\)
    is a nonzero polynomial in one variable,
    so the \hyperref[thm:polynomial-distinct-func]{theorem above} holds.
\end{proof}
\begin{remark}
    In general, finding nonzeros is easy, but finding zeros is hard.
    \hyperref[thm:nullstellensatz]{Hilbert's Nullstellensatz}
    gives us a criteria for where zeros can be found.
\end{remark}

\subsection*{Symmetric Polynomials}

\begin{definition}
    Suppose \(R\) is a commutative ring (and more often, a field).
    We call a polynomial \(f \in R[x_1,x_2,\hdots,x_n]\) in \(n\) variables
    symmetric if \(f(x_1,x_2,\hdots,x_n)
    = f(x_{\sigma(1)},x_{\sigma(2)},\hdots,x_{\sigma(n)})\)
    for any \(\sigma \in S_n\),
    that is, the polynomial remains unchanged under permutation of variables.
    We denote the set of symmetric polynomials \({R[x_1,x_2,\hdots,x_n]}^{S_n}\).
\end{definition}
\begin{definition}
    We can write down a set of symmetric polynomials called
    the elementary symmetric polynomials \(p_k\), \(1 \leq k \leq n\).
    They include all possible combinations of
    non-repeating products of \(k\) indeterminates.
    \begin{equation*}
        p_k = \sum_{1 \leq i_1 < i_2 < \cdots < i_k \leq n}
        x_{i_1}x_{i_2} \hdots x_{i_k}
        = \sum_{1 \leq i_1 < i_2 < \cdots < i_k \leq n}
        \prod_{j=1}^k x_{i_j}
    \end{equation*}
    In particular we see that \(p_1 = \sum_{i=1}^n x_i\)
    and \(p_n = \prod_{j=1}^n x_j\).
\end{definition}
\begin{proposition}
    The symmetric polynomials \({R[x_1,x_2,\hdots,x_n]}^{S_n}\)
    forms a commutative ring.
    For \(n \geq 2\),
    this is a proper subring \({R[x_1,x_2,\hdots,x_n]}^{S_n}
    \subsetneq R[x_1,x_2,\hdots,x_n]\).
\end{proposition}
\begin{proof}
    This will become obvious once we prove
    the \hyperref[thm:fundamental-sym-polynomial]{following theorem}.
    In the meantime,
    you can convince yourself by checking the axioms.
    The second fact is easy,
    as all univariate polynomials are by definition symmetric,
    and \(f(x_1,\hdots,x_n) = x_1\) is an asymmetric polynomial
    when \(n \geq 2\).
\end{proof}

\begin{definition}
    The total degree of a monomial \(a\prod_{i=1}^n x_i^{k_i}\)
    is defined as \(\sum_{i=1}^n k_i\).
    We call a polynomial \(f \in R[x_1,x_2,\hdots,x_n]\) homogeneous
    if every monomial it contains has the same total degree.
\end{definition}
\begin{lemma}\label{lem:homogeneous-sym-polynomial}
    Any \(f \in R[x_1,x_2,\hdots,x_n]\) is a sum of homogeneous parts.
    \(f\) is symmetric if and only if each homogeneous part is symmetric.
\end{lemma}
\begin{proof}
    The first statement is clear
    because every monomial has a total degree,
    and you can simply group them by total degree.

    The second statement is also easy,
    because under permutation of variables,
    the total degree of a monomial never changes,
    so it preserves each homogeneous part by themselves.
\end{proof}

\begin{definition}
    We define an order on the monomials
    by saying \(x_1^{a_1}x_2^{a_2} \hdots x_n^{a_n}
    < x_1^{b_1}x_2^{b_2} \hdots x_n^{b_n}\)
    when \((a_1,a_2,\hdots,a_n) < (b_1,b_2,\hdots,b_n)\) lexicographically,
    that is, the earliest index \(i\) where \(a_i \neq b_i\),
    and then \(a_i < b_i\).
\end{definition}
\begin{definition}
    For homogeneous polynomials,
    we call the monomial with the largest such order
    the leading term.
\end{definition}

\begin{lemma}\label{lem:leading-term-sym-polynomial}
    \(p_1^{d_1}p_2^{d_2} \hdots p_n^{d_n}\) has a leading term
    \(x_1^{d_1+d_2+\cdots+d_n}x_2^{d_2+\cdots+d_n} \hdots x_n^{d_n}\).
\end{lemma}
\begin{proof}
    We first see that for each \(i\),
    \(p_i^{d_i}\) has a leading term \(x_1^{d_i} \hdots x_i^{d_i}\).
    We now claim that the leading term of a product of monomials
    is the product of the leading terms of the monomials.
    To see that, if \(x_1^{\alpha_1} \hdots x_n^{\alpha_n}
    > x_1^{\beta_1} \hdots x_n^{\beta_n}\)
    and \(x_1^{\gamma_1} \hdots x_n^{\gamma_n}
    > x_1^{\delta_1} \hdots x_n^{\delta_n}\),
    then \(x_1^{\alpha_1+\gamma_1} \hdots x_n^{\alpha_n+\delta_n}
    > x_1^{\beta_1+\delta_1} \hdots x_n^{\beta_n+\delta_n}\),
    since without loss of generality let \(i \leq j\),
    for the first indices where
    \(\alpha_i \neq \beta_i\) and \(\gamma_j \neq \delta_j\),
    and \(\alpha_i > \beta_i\) implies \(\alpha_i+\gamma_i > \beta_i+\delta_i\).
    We have proven that multiplication preserves the order,
    so if the leading terms are the largest out of all monomials,
    the product will still be the largest,
    and hence we can merely multiply the leading terms.
\end{proof}

\begin{theorem}[Fundamental Theorem of Symmetric Polynomials]\label{thm:fundamental-sym-polynomial}
    Every symmetric polynomial \(f \in {R[x_1,x_2,\hdots,x_n]}^{S_n}\)
    can be uniquely represented by a polynomial \(g \in R[p_1,p_2,\hdots,p_n]\)
    with variates being the elementary symmetric polynomials.
\end{theorem}
\begin{proof}
    We provide an algorithm to convert symmetric polynomials
    into polynomials over \(p_i\).
    By the \hyperref[lem:homogeneous-sym-polynomial]{first lemma above},
    it is sufficient to consider \(f\) homogeneous symmetric.
    With the above definitions,
    we can perform steps that are similar to polynomial long division
    by choosing our polynomials correctly.

    We first claim that we can always write the leading term of \(f\)
    as \(ax_1^{k_1}x_2^{k_2} \hdots x_n^{k_n}\)
    where \(k_1 \geq k_2 \geq \cdots \geq k_n\).
    We observe that since \(f\) is symmetric,
    any permutation of the variates for our leading term also exists,
    so we can simply sort the \(k_i\) for our leading monomial
    into a descending sequence,
    % choose the largest possible order,
    % and make that term our leading monomial
    % by permuting all of \(f\) according the sort
    % that we just performed on the leading term.
    and we realize that there exists that permutation of variables
    already in \(f\),
    and we can choose that as our leading term.

    We then construct \(g = ap_1^{k_1-k_2}p_2^{k_2-k_3} \hdots
    p_{n-1}^{k_{n-1}-k_n}p_n^{k_n}\),
    which by the \hyperref[lem:leading-term-sym-polynomial]{second lemma above}
    has a leading term \(ax_1^{k_1}x_2^{k_2} \hdots x_n^{k_n}\),
    as all the sums of exponents are telescoping.
    We can subtract \(f-g = r\) to get a remainder,
    which has a lower leading term,
    allowing us to repeat this process on \(r\).

    We know this process terminates in finite steps,
    because there are only finitely many ways to add \(n\) non-negative numbers
    up to a certain total degree,
    so there are finitely many \(n\)-tuples \((k_1,k_2,\hdots,k_n)\)
    that we need to eliminate.
    This proves the existence of a \(R[p_1,p_2,\hdots,p_n]\) representation.

    \medskip

    Now it suffices to prove uniqueness of such a representation.
    By way of contradiction,
    if there exists an \(f\) with two different representations,
    then we can subtract the two different representations,
    and say that there exists some \(\sum_i a_i \prod_{j=1}^n p_j^{d_{ij}} = 0\).
    In other words, we are proving algebraic independence.

    But the \hyperref[lem:leading-term-sym-polynomial]{second lemma above}
    allows us to see that there is a bijection between product of \(p_j\)
    and the leading terms in \(x_j\).
    Since the \(x_j\) are algebraically independent by definition,
    it is impossible to cancel out the leading term
    of two different products of \(p_j\),
    which imply that \(a_i\) must all be 0.
\end{proof}
\begin{remark}
    It is a good exercise to program yourself
    a symmetric polynomial decomposition calculator,
    simply by following the algorithm as described in the theorem.
\end{remark}
\begin{corollary}
    \({R[x_1,x_2,\hdots,x_n]}^{S_n} \cong R[x_1,x_2,\hdots,x_n]\).
\end{corollary}
\begin{proof}
    The \hyperref[thm:fundamental-sym-polynomial]{theorem above}
    essentially proves \({R[x_1,x_2,\hdots,x_n]}^{S_n} \cong R[p_1,p_2,\hdots,p_n]\).
    But Proposition~\ref{thm:polynomial-ring-isomorphism}
    tells us that \(R[p_1,p_2,\hdots,p_n] \cong R[x_1,x_2,\hdots,x_n]\).
\end{proof}


\section{Factorial Rings}\label{sec:factorial-rings}

\subsection*{Unique Factorization Domains}

\begin{definition}
    Suppose \(R\) is a commutative integral domain.
    For some \(\alpha \in R - \{0\}\),
    if there exists some \(\{\beta,\gamma\} \subset R\) not units
    such that \(\alpha = \beta\gamma\),
    we say \(\alpha\) factors in \(R\).
    Notice the definition excludes units \(u\)
    since it is always possible to write \(\alpha = u(u^{-1}\alpha)\),
    which we do not consider factorization.
    We say in this case that \(\beta\) and \(\gamma\) properly divides \(\alpha\).
\end{definition}
\begin{definition}
    We call \(\alpha\) irreducible if \(\alpha\) is not factorable.
\end{definition}
\begin{proposition}
    Suppose we can factor \(\alpha = \beta\gamma\).
    Then \((\alpha) \subsetneq (\beta)\) and \((\alpha) \subsetneq (\gamma)\).
\end{proposition}
\begin{proof}
    It is sufficient to prove only the case of \(\beta\).
    Clearly \(\alpha \in (\beta)\), so \((\alpha) \subseteq (\beta)\).
    On the other hand, if \((\alpha) = (\beta)\),
    then there exists some unit \(u\) where \(\alpha = \beta u\),
    which tells us in our case, \((\alpha) \subsetneq (\beta)\).
\end{proof}
\begin{remark}
    Because of this relationship between divisibility and ideals,
    every single statement below
    can be translated between these two languages.
    Sometimes it is easy to think of statements in one language or another.
\end{remark}

\begin{definition}
    Suppose \(R\) is a commutative integral domain.
    \(R\) is a unique factorization domain (UFD)
    if all \(\alpha \in R - \{0\}\) can be factorized into
    \(\alpha = \prod_{i=1}^n p_i\) where \(p_i\) are irreducible elements,
    and this factorization is unique up to reordering of elements,
    and multiplication by units.
\end{definition}
\begin{theorem}\label{thm:ufd-conditions}
    \(R\) is a UFD if the following two conditions hold:
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item \textit{Divisor chain.}
            Given any \(a_1 \in R - \{0\}\),
            there exists no infinite proper divisor chain
            such that \(a_{i+1}\) properly divides \(a_i\); and
        \item \textit{\hyperref[lem:euclid]{Primeness.}}
            If \(p\) is irreducible, and \(p \mid ab\),
            then either \(p \mid a\) or \(p \mid b\).
    \end{enumerate}
\end{theorem}
\begin{proof}
    Factorization exists simply based on condition (i),
    since that allows our recursive factorization to terminate.

    Now we prove uniqueness.
    Suppose that we have two factorizations
    \(\alpha = p_1 \hdots p_r = p_1' \hdots p_s'\).
    Since \(p_1 \mid p_1 \hdots p_r\),
    it should also divide \(p_1 \mid p_1' \hdots p_s'\),
    which by condition (ii) it should divide one of \(p_i'\).
    Without loss of generality, let this be \(p_1'\).
    But as \(p_1'\) is irreducible,
    we conclude that \(p_1' = up_1\) for some unit \(u\).
    As we only require factorization up to multiplication by units,
    we can ignore \(u\) in this case,
    and look at the rest of the expression \(p_2 \hdots p_r = p_2' \hdots p_s'\).
    But this is no different from the case of \(p_1\) and \(p_1'\),
    so we recursively repeat that process,
    and see that all terms match up to unit multiplication.
\end{proof}
\begin{remark}
    It is also possible to rephrase condition (i)
    as having no infinite chain of proper ideals.
\end{remark}

\begin{definition}
    Suppose \(R\) is a commutative domain, and \(p \in R - \{0\}\).
    We call \(p\) prime if \((p) \subseteq R\) is a prime ideal.
\end{definition}
\begin{proposition}
    The two notions of primeness are identical.
\end{proposition}
\begin{proof}
    Suppose \(p \mid ab\) implies \(p \mid a\) or \(p \mid b\).
    Then \(ab \in (p)\) implies \(a \in (p)\) or \(b \in (p)\).
    This is by definition,
    since inclusion in an ideal is identical to divisibility.
    The argument can be reversed.
\end{proof}
\begin{lemma}
    In a UFD, elements are prime if and only if they are irreducible.
\end{lemma}
\begin{proof}
    The reverse direction is given by Theorem~\ref{thm:ufd-conditions}.
    For the forward direction,
    if \(p = ab\) is prime, clearly \(p \mid ab\),
    so \(p \mid a\) or \(p \mid b\).
    But \(a \mid p\) and \(b \mid p\),
    so either \(a\) or \(b\) must be a unit.
\end{proof}

\begin{definition}
    Suppose \(R\) is a commutative domain, and \(\{a,b\} \subset R - \{0\}\).
    \(d \in R\) is a greatest common denominator of \(a\) and \(b\),
    denoted \(d = \gcd(a,b)\),
    if \(d \mid a\) and \(d \mid b\),
    and for all \(c \in R\), \(c \mid a\) and \(c \mid b\) implies \(c \mid d\).
    Similarly, \(m \in R\) is a least common multiple of \(a\) and \(b\),
    denoted \(m = \lcm(a,b)\),
    if \(a \mid m\) and \(b \mid m\),
    and for all \(n \in R\), \(a \mid n\) and \(b \mid n\) implies \(m \mid n\).
\end{definition}
% \begin{proposition}
%     For any two elements \(\{a,b\} \in R\) a UFD, \(\gcd(a,b)\) exists.
% \end{proposition}
% \begin{proof}
% \end{proof}
\begin{theorem}
    The primeness condition is equivalent to the following condition:
    \begin{enumerate}[itemsep=0mm]
        \item[(ii)] \textit{GCD.}
        Any two elements have a GCD.\@
    \end{enumerate}
\end{theorem}
\begin{proof}
    Primeness implies GCD is simple.
    Write \(a = p_1^{i_1} \hdots p_r^{i_r}\)
    and \(b = p_1^{j_1} \hdots p_r^{j_r}\).
    Then \(\gcd(a,b) = p_1^{\min(i_1,j_1)} \hdots p_r^{\min(i_r,j_r)}\).

    \medskip

    GCD is commutative and associative
    as in \(\gcd(a,b)\) is also a \(\gcd(b,a)\) simply by definition;
    \(d_n = \gcd(a_1,a_2,\hdots,a_n)\) is defined recursively as
    \(d_1 = a_1\), \(d_{i+1} = \gcd(d_i,a_{i+1})\),
    and clearly that is by definition \(d_n \mid a_i\) for all \(i\),
    and \(c \mid a_i\) implies \(c \mid d_n\).

    We claim that \(c\gcd(a,b)\) is also a \(\gcd(ca,cb)\).
    Suppose \(d = \gcd(a,b)\).
    Then \(d \mid a\) and \(d \mid b\),
    and hence \(cd \mid ca\) and \(cd \mid cb\),
    and \(cd \mid \gcd(ca,cb)\).
    There exists some \(x \in R\) such that \(cdx = \gcd(ca,cb)\),
    and there exists some \(y \in R\) with \(cdxy = y\gcd(ca,cb) = ca\),
    which imply \(cdx = ca\), and hence \(dx \mid a\).
    Without loss of generality \(dx \mid b\) too, so \(dx \mid d\),
    and we know \(x\) is a unit, so \(cd\) is a GCD of \(ca\) and \(cb\).

    From the above we can now prove that if \(a\) coprime with \(b\)
    and \(a\) coprime with \(c\),
    then \(a\) coprime with \(bc\).
    We can show this with \(1 = \gcd(a,c) = \gcd(a,c\gcd(a,b))
    = \gcd(a,ca,cb) = \gcd(a,cb)\).

    GCD implies primeness because if \(p\) is irreducible
    and \(p \nmid a\) and \(p \nmid b\),
    then \(p\) coprime \(a\) and \(p\) coprime \(b\),
    so \(p\) coprime \(ab\),
    and hence \(p \nmid ab\).
\end{proof}

\subsection*{Principal Ideal Domains}

\begin{remark}
    Recall from earlier that principal ideal domains (PID)
    are commutative domains with every ideal generated by a single element.
\end{remark}
\begin{proposition}\label{prop:pid-maximal-is-prime}
    Suppose \(R\) is a PID, \(I \subseteq R\) is an ideal.
    Then \(I\) is maximal if and only if \(I\) is a nonzero prime ideal.
\end{proposition}
\begin{proof}
    Suppose \(I\) is a maximal ideal.
    Then by Theorem~\ref{thm:maximal-quotient-field} \(R/I\) is a field,
    which in particular it must be an integral domain.
    Hence by Proposition~\ref{thm:prime-quotient-domain} \(I\) is a prime ideal.

    Now suppose \(I\) is a nonzero prime ideal.
    We suppose there is an intermediate ideal \(I \subseteq J \subseteq R\).
    But because \(R\) is a PID,
    \(I = \langle x \rangle\), \(J = \langle y \rangle\)
    for some \(\{x,y\} \subset R\).
    But as \(I \subseteq J\), \(x = ay\) for some \(a \in R\);
    but by primality of \(I\), either \(a \in I\) or \(y \in I\).
    If \(y \in I\) then \(I = J\) so \(I\) is maximal.
    If \(a \in I\) then \(a = bx\) for some \(b \in R\),
    so \(x = bxy\), and therefore \(by = 1\).
    \(y\) is a unit, so \(J = R\), and \(I\) is maximal.
\end{proof}

\begin{theorem}\label{thm:pid-ufd}
    If \(R\) is a PID, then \(R\) is a UFD.\@
\end{theorem}
\begin{proof}
    We first prove that it satisfies the divisor chain condition
    via the ideal chain condition.
    Suppose we have an infinite ideal chain
    \((a_1) \subseteq (a_2) \subseteq \cdots \subseteq (a_r) \subseteq \cdots\).
    Then by Proposition~\ref{prop:nested-ideals},
    \(I = \bigcup_{i=1}^\infty (a_i)\) is an ideal.
    Notice that \(I = (b)\) must be generated by some element,
    and since \(b \in I\), there exists some \(r\) that \(b \in (a_r)\).
    However, by definition of the infinite union,
    \((b) \subseteq (a_r) \subseteq (a_{r_1}) \subseteq \cdots \subseteq I = (b)\).
    Hence we are forced to conclude that \((b) = (a_r) = (a_{r+1}) = \cdots = I\),
    and every infinite chain of ideals are only proper up to a finite \(r\).
    There are no infinite chain of proper ideals.

    Now we prove the primeness condition.
    Suppose \(p\) is irreducible, \(p \mid ab\) and \(p \nmid a\).
    \(p\) irreducible means \((p)\) is maximal
    (similar to Theorem~\ref{thm:ideal-divisibility}).
    As \(p \nmid a\), then \(a \notin (p)\),
    so the ideal generated by two elements \((p,a) \supsetneq (p)\),
    and hence \((p,a) = R = (1)\).
    By \hyperref[thm:bezout]{B\'{e}zout's Identity},
    there exists \(u,v\) such that \(up + va = 1\),
    and hence \(upb + vab = b\).
    As \(p \mid upb\) and \(p \mid v(ab)\), \(p \mid b\).

    Then by Theorem~\ref{thm:ufd-conditions}, \(R\) is a UFD.\@
\end{proof}
\begin{remark}
    In general it is difficult to determine whether any ring is a PID.\@
    The Krull dimension is one way to determine PIDs
    via the length of prime ideals.
\end{remark}

\begin{definition}
    Suppose \(R\) is a commutative domain.
    We call \(R\) a Euclidean domain if there exists a long division algorithm
    \(\func{\delta}{R}{\bZ}\),
    such that for all \(a,b \in R - \{0\}\),
    we can find \(q,r \in R\) with \(a = qb + r\),
    where \(\delta(r) < \delta(b)\),
    and \(\delta(x) = 0\) if and only if \(x = 0\).
\end{definition}
\begin{theorem}\label{thm:euclidean-pid}
    If \(R\) is a Euclidean domain,
    then \(R\) is a PID.\@
\end{theorem}
\begin{proof}
    We first look at the ideal of a single element, \(I = \{0\}\);
    clearly this is \(I = (0)\).

    Now suppose \(I\) has more than one element.
    Choose any nonzero element \(x \in I\)
    such that \(\delta(x) \leq \delta(y)\) for all nonzero \(y \in I\);
    in other words, \(\delta(x)\) is minimal.
    Now suppose we have some nonzero \(y \in I\).
    We factor it with \(y = qx + r\),
    which we know \(r = y - qx \in I\);
    and according to the Euclidean function \(\delta(r) < \delta(x)\),
    as \(\delta(x)\) is minimal, \(\delta(r) = 0\), so \(r = 0\).
    Hence all \(y \in I\) is a multiple of \(x\),
    and \(I = (x)\).
\end{proof}
\begin{corollary}\label{cor:euclidean-ufd}
    If \(R\) is a Euclidean domain,
    then \(R\) is a UFD.\@
\end{corollary}
\begin{proof}
    By the \hyperref[thm:euclidean-pid]{theorem above}
    and Theorem~\ref{thm:pid-ufd}.
\end{proof}

\subsection*{Polynomials over UFDs}

\begin{definition}
    Suppose \(R\) is a UFD, and \(f(x) \in R[x]\) a polynomial.
    We define the content of \(f\), a function \(\func{c}{R[x]}{R}\)
    to be the GCD of all nonzero coefficients.
    We call \(f\) primitive if \(c(f)\) is a unit.
    Hence we can write \(f = c(f)f_1\), where \(f_1\) is primitive.
\end{definition}
\begin{lemma}\label{lem:polynomial-primitive-decomp}
    Suppose \(R\) is a UFD, and \(F\) its field of fractions.
    If \(f(x) \in F[x]\),
    there exists a unique factorization up to multiplication of units in \(R\)
    for \(f(x) = \gamma f_1(x)\)
    where \(f_1\) is primitive in \(R[x]\), and \(\gamma \in F\).
\end{lemma}
\begin{proof}
    Since all coefficients of \(f\) are fractions,
    it is sufficient to look at the product of all denominators,
    and multiply by that to obtain some \(g(x) \in R[x]\).
    Hence there exists some \(a \in R\) such that \(af(x) = g(x) \in R[x]\).
    We can then decompose \(g(x) = bg_1(x)\)
    where \(b = c(g)\) and \(g_1\) is primitive.
    Therefore \(af(x) = bg_1(x)\),
    so \(f(x) = \frac{b}{a}g_1(x)\), and there exists \(\gamma = \frac{b}{a}\).

    We now prove uniqueness.
    Suppose \(f(x) = \gamma' g_1'(x)\) for another such combination.
    Then since \(\gamma' \in F\), we write \(\gamma' = b'/a'\),
    and we have \(f(x) = \frac{b'}{a'}g_1'(x) = \frac{b}{a}g_1(x)\).
    So we have \(aa'f(x) = ab'g_1'(x) = a'b g_1(x) \in R[x]\),
    but that tells us \(c(aa'f) = ab' = a'b\) up to unit multiplication.
    Hence \(ab' = ua'b\) for some unit \(u\),
    and \(ub/a = b'/a'\).
\end{proof}
\begin{corollary}\label{cor:polynomial-unit-mult}
    Suppose \(\{f(x),g(x)\} \subset R[x]\) are both primitive.
    If there exists some \(\gamma \in F\) such that \(\gamma f = g\),
    then \(\gamma\) is a unit in \(R\).
\end{corollary}
\begin{proof}
    By the proof of \hyperref[lem:polynomial-primitive-decomp]{lemma above},
    \(\gamma'/\gamma = u\) is a unit.
\end{proof}

\begin{lemma}[Gauss' Lemma for primitivity]\label{lem:gauss-primitive}
    Suppose \(R\) is a UFD.\@
    If \(\{f,g\} \subset R[x]\) are both primitive,
    then \(fg\) is also primitive.
\end{lemma}
\begin{proof}
    We prove the contrapositive.
    Suppose \(fg\) is not primitive.
    Then there exists some prime \(p\) which divides every coefficient of \(fg\).
    Considering \(T = R/(p)\), since \(p\) prime,
    Proposition~\ref{thm:prime-quotient-domain} shows that \(T\) is a domain.
    By Proposition~\ref{prop:polynomial-domain},
    since \(T\) is a domain, \(T[x]\) is also a domain.
    We can write a quotient map \(\vfunc{\pi}{R}{T}{r}{r+(p)}\) the obvious way,
    and extend it to \(\func{\pi}{R[x]}{T[x]}\) by sending \(x \mapsto x\).
    Hence \(\pi(fg) = \pi(f)\pi(g) = (p)\),
    as all the coefficients, and hence all the terms are divisible by \(p\).
    But as \(T[x]\) is a domain, either \(\pi(f) = (p)\) or \(\pi(g) = (p)\),
    which says either \(f\) or \(g\) is not primitive.
\end{proof}
\begin{lemma}[Gauss' Lemma for irreducibility]\label{lem:gauss-irreducible}
    Suppose \(R\) is a UFD, and \(F\) its field of fractions.
    If \(f(x) \in R[x]\) is primitive and irreducible,
    then \(f(x) \in F[x]\) is irreducible.
\end{lemma}
\begin{proof}
    By way of contradiction, suppose \(f(x) = f_1(x)f_2(x)\)
    can be factored in \(F[x]\).
    Then by Lemma~\ref{lem:polynomial-primitive-decomp},
    there exists \(\gamma_i \in F\) and primitive \(g_i \in R[x]\)
    such that \(f(x) = f_1(x)f_2(x) = \gamma_1\gamma_2 g_1(x)g_2(x)\).
    But considering \(f,g_1,g_2\) as primitive elements of \(R[x]\),
    we know that \(\gamma_1\gamma_2\) is a unit in \(R\)
    by Corollary~\ref{cor:polynomial-unit-mult}.
    Then that implies \(f_1,f_2\) are also elements of \(R[x]\),
    so \(f\) factors in \(R[x]\) too,
    which is a contradiction.
\end{proof}
\begin{theorem}
    Suppose \(R\) is a UFD.\@
    Then \(R[x]\) is also a UFD.\@
\end{theorem}
\begin{proof}
    We first prove that a factorization of polynomials exist.
    Suppose \(f(x) \in R[x]\) is nonzero and not a unit.
    Then we can write \(f = c(f)f_1\), where \(f_1\) primitive.
    Clearly \(c(f) \in R\) can factor into irreducible components.
    Now consider \(f_1\).
    If \(f_1\) is irreducible we are done.
    However, if \(f_1\) can be factored such that \(f_1 = g_1 g_2\),
    then we see that \(0 < \deg g_i < \deg f_1\),
    so the degree gets reduced,
    and hence if we perform factorization recursively,
    there are at most \(\deg f_1\) recursions,
    and factorization terminates.
    Hence the factorization of \(f\)
    is the product of the factorization of \(c(f)\)
    and the factorization of \(f_1\).

    \medskip

    We then prove the factorization is unique up to multiplication by units.
    We first consider \(f(x) \in R[x]\) primitive.
    Then \(f(x) = q_1(x)q_2(x) \hdots q_n(x)\) factors into irreducibles,
    with \(\deg q_i > 0\),
    since Corollary~\ref{cor:polynomial-unit-mult} tells us that
    if we have \(q_i = 0\), then \(q_i\) is a unit,
    and by definition we only perform factorization up to units,
    so \(q_i\) should not be in the factorization.

    Suppose we also have another factorization into irreducibles
    \(f(x) = q_1' q_2' \hdots q_m'\), \(\deg q_i' > 0\).
    Since \(q_i\) and \(q_i'\) are all primitive,
    by \hyperref[lem:gauss-irreducible]{Gauss' lemma}
    they are also irreducible in \(F[x]\).
    We have shown earlier in Corollary~\ref{cor:field-polynomial-pid}
    that \(F[x]\) is a PID,
    which by Theorem~\ref{thm:pid-ufd} \(F[x]\) is a UFD.\@
    Then we know that the factorization is equal up to units in \(F[x]\),
    and without loss of generality we pair up the factors
    \(q_i = \gamma q_i'\) for some \(\gamma \in F\).
    But Corollary~\ref{cor:polynomial-unit-mult} once again tells us that
    \(\gamma \in R\) is a unit,
    which is exactly what we desire.

    Lastly we consider \(f(x)\) not primitive.
    Then \(f(x) = c(f)f_1(x)\) where \(f_1(x)\) is primitive.
    \(c(f) = p_1 \hdots p_m\) uniquely factors up to unit multiplication in \(R\),
    and the previous paragraph tells us \(f_1(x)\) does so too.
    This completes the proof for uniqueness.
\end{proof}

\begin{theorem}[Eisenstein's criterion]\label{thm:eisenstein-criterion}
    Suppose \(R\) is an integral domain, \(P \subseteq R\) some prime ideal.
    Let \(f(x) = a_n x^n + a_{n-1}x^{n-1} + \cdots + a_1 x + a_0 \in R[x]\)
    be a polynomial.
    If the coefficients
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(a_i \in P\) for all \(0 \leq i \leq n-1\),
            all coefficients but the leading term in the ideal;
        \item \(a_n \notin P\) leading term not in the ideal; and
        \item \(a_0 \notin P^2\) constant term not in the square,
    \end{enumerate}
    then \(f(x)\) is irreducible in \(R[x]\).
\end{theorem}
\begin{proof}
    Suppose, by way of contradiction, that \(f(x)\) is reducible in \(R[x]\).
    Then we can write \(f(x) = g(x)h(x)\), and we shall denote
    \begin{equation*}
        g(x) = c_k x^k + c_{k-1}x^{k-1} + \cdots + c_1 x + c_0 \qquad
        h(x) = d_\ell x^\ell + d_{\ell-1}x^{\ell-1} + \cdots + d_1 x + d_0
    \end{equation*}
    By condition (a) \& (c),
    only one of \(c_0\) and \(d_0\) is in \(P\),
    so without loss of generality let \(c_0 \in P\) and \(d_0 \notin P\).

    We claim that this implies \(c_i \in P\) for all \(0 \leq i \leq k\).
    We already have our base case \(c_i \in P\) from above,
    so by way of strong induction,
    suppose \(c_0,c_1,\hdots,c_{i-1} \in P\).
    Since \(a_i = \sum_{j+j'=i} c_j d_{j'} = c_i d_0 + c_{i-1}d_1 + \cdots + c_0 d_i \in P\)
    by condition (a),
    and every term in the sum except for the first term is in \(P\) by assumption,
    as \(d_0 \notin P\), we must have \(c_i \in P\).

    But \(c_k \in P\) implies \(a_n = c_k d_\ell \in P\),
    which contradicts condition (b).
    Hence \(f(x)\) is irreducible in \(R[x]\).
\end{proof}

\begin{corollary}[Eisenstein's criterion over integers]\label{cor:eisenstein-criterion}
    Suppose we have a polynomial with integer coefficients
    \(f(x) = a_n x^n + a_{n-1}x^{n-1} + \cdots + a_1 x + a_0 \in \bZ[x]\).
    If there exists a prime \(p\) such that
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(p \mid a_i\) for all \(0 \leq i \leq n-1\),
            \(p\) dividing all coefficients but the leading term;
        \item \(p \nmid a_n\) does not divide the leading term; and
        \item \(p^2 \nmid a_0\) the square does not divide the constant term,
    \end{enumerate}
    then \(f(x)\) is irreducible in \(\bQ[x]\).
\end{corollary}
\begin{proof}
    % Suppose, by way of contradiction,
    % that \(f(x)\) is reducible in \(\bQ[x]\).
    % Then we can write \(f(x) = g(x)h(x)\),
    % and we shall denote
    % \begin{equation*}
    %     g(x) = c_k x^k + c_{k-1}x^{k-1} + \cdots + c_1 x + c_0 \qquad
    %     h(x) = d_\ell x^\ell + d_{\ell-1}x^{\ell-1} + \cdots + d_1 x + d_0
    % \end{equation*}
    % By condition (a) \& (c),
    % only one of \(c_0\) and \(d_0\) are divisible by \(p\),
    % so without loss of generality let \(p \mid c_0\), \(p \nmid d_0\).
    %
    % We claim that this implies \(p \mid c_i\) for all \(0 \leq i \leq k\).
    % We already have our base case \(p \mid c_0\) from above,
    % so by way of strong induction, suppose \(p \mid c_0,c_1,\hdots,c_{i-1}\).
    % Since \(a_i = \sum_{j+j'=i} c_j d_{j'} = c_i d_0 + c_{i-1}d_1 + \cdots + c_0 d_i\),
    % and every term in the sum except for the first term is divisible by \(p\),
    % \(p \nmid d_0\), so \(p \mid c_i\).
    %
    % But \(p \mid c_i\) implies \(p \mid a_n = c_k d_\ell\),
    % so this contradicts condition (b).
    % Hence \(f(x)\) is irreducible in \(\bQ[x]\).
    By the \hyperref[thm:eisenstein-criterion]{theorem above}
    we get irreducibility in \(\bZ[x]\);
    by \hyperref[lem:gauss-irreducible]{Gauss' lemma},
    we get irreducibility in \(\bQ[x]\).
\end{proof}

\begin{corollary}[Irreducibility of the cyclotomic]
    Let \(p\) be prime.
    The cyclotomic polynomial \(\Phi_p(x) = \sum_{i=0}^{p-1} x^i \in \bZ[x]\)
    is irreducible in \(\bQ[x]\).
\end{corollary}
\begin{proof}
    It is sufficient to prove that \(\Phi_p(y+1)\)
    as a polynomial of \(y\) is irreducible.
    By the \hyperref[prop:binom]{binomial theorem}
    \begin{equation*}
        \Phi_p(y+1) = {(y+1)}^{p-1} + {(y+1)}^{p-2} + \cdots + 1 
        = \sum_{i=0}^{p-1} \binom{p}{i} y^{p-1-i}
    \end{equation*}
    Note that by definition of the binomial coefficient,
    only the leading term \(\binom{p}{0} = 1\) is not divisible by \(p\),
    and the constant term \(\binom{p}{p-1} = p\) is not divisible by \(p^2\),
    so by \hyperref[cor:eisenstein-criterion]{Eisenstein's criterion},
    it is irreducible.
\end{proof}
