\chapter{Vector Spaces}\label{sec:linear-algebra}\label{sec:vsp}

\begin{remark}
    We shall resume the following convention as in Section~\ref{sec:matrix-rings}:
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item \(a\) a lowercase symbol denotes a generic element in a set;
        \item \(\vec{a}\) an underlined symbol denotes a vector; and
        \item \(\vb{A}\) a bold symbol (usually uppercase)
            denotes a matrix or tensorial quantity.
    \end{enumerate}
\end{remark}
\begin{remark}
    All homomorphisms in this chapter are assumed to be \(F\)-vector space homomorphisms;
    that is, \(\Hom(U,V) = \Hom_F(U,V)\) for \(U,V\) being \(F\)-vector spaces.
\end{remark}

\section{Basic Definitions}

\begin{definition}
    A left module over a ring \(R\) is a quadruple \((M,+,\cdot,\vec{0})\)
    where \(\vec{0} \in M\) is a set equipped with addition \(+\)
    with an identity \(\vec{0}\)
    and scalar multiplication,
    with the following four properties:
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item \((M,+,\vec{0})\) forms an abelian group;
        \item scalar multiplication
            \(\vfunc{\cdot}{R \times M}{M}{(\alpha,\vec{m})}{\alpha\vec{m}}\),
            and in particular \(1\vec{m} = \vec{m}\);
        \item \(\forall\{\alpha,\beta\} \in R,\, \forall\{\vec{m},\vec{n}\} \subset M\),
            the distributive laws \((\alpha+\beta)(\vec{m}+\vec{n})
            = \alpha\vec{m}+\alpha\vec{n}+\beta\vec{m}+\beta\vec{n}\) hold; and
        \item associativity
            \(\forall\{\alpha,\beta\} \subset R,\,\forall\vec{m} \in M\),
            \((\alpha\beta)\vec{m} = \alpha(\beta\vec{m})\).
    \end{enumerate}
\end{definition}
\begin{definition}
    A vector space over a field \(F\) is similarly defined with \(R = F\).
    It is a quadruple \((V,+,\cdot,\vec{0})\)
    where \(\vec{0} \in V\) is a set equipped with addition \(+\)
    with an identity \(\vec{0}\)
    and scalar multiplication,
    with the following four properties:
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item \((V,+,\vec{0})\) forms an abelian group;
        \item scalar multiplication
            \(\vfunc{\cdot}{F \times V}{M}{(\alpha,\vec{v})}{\alpha\vec{v}}\),
            and in particular \(1\vec{v} = \vec{v}\);
        \item \(\forall\{\alpha,\beta\} \in R,\, \forall\{\vec{u},\vec{v}\} \subset M\),
            the distributive laws \((\alpha+\beta)(\vec{u}+\vec{v})
            = \alpha\vec{u}+\alpha\vec{v}+\beta\vec{u}+\beta\vec{v}\) hold; and
        \item associativity
            \(\forall\{\alpha,\beta\} \subset R,\,\forall\vec{v} \in M\),
            \((\alpha\beta)\vec{v} = \alpha(\beta\vec{v})\).
    \end{enumerate}
\end{definition}
\begin{proposition}
    Suppose \(F\) is a field,
    and \(F^X = \{\func{f}{X}{F}\}\) the set of all functions from \(X\) to \(F\).
    When equipped with pointwise addition and multiplication,
    this forms a vector space over \(F\).
\end{proposition}
\begin{proof}
    Since addition and multiplication is pointwise,
    all proofs essentially start with `for all \(x \in X\) \(f(x) \in F\)',
    so distributive laws and associativity laws are inherited from \(F\).
\end{proof}
\begin{corollary}
    \(F^n\) forms a vector space.
\end{corollary}
\begin{proof}
    Simply consider \(X = \{0,1,\hdots,n-1\}\) an \(n\)-element set.
\end{proof}

\begin{definition}
    Suppose \((V,+,\cdot,0,1)\) is a vector space.
    Then \(W\) is a subspace if \(W \subseteq V\)
    and its additive and multiplicative groups form subgroups.
\end{definition}

\begin{definition}
    For any sequence of vectors \({\{\vec{v}_i\}}_{i=1}^r\),
    finite summation is recursively defined as
    \begin{equation*}
        \sum_{i=1}^0 \vec{v}_i = \vec{0} \qquad
        \sum_{i=1}^{r+1} \vec{v}_i = \vec{v}_{r+i} + \sum_{i=1}^r \vec{v}_i \qquad
        \sum_{i=m}^n \vec{v}_i = \sum_{i=1}^n \vec{v}_i - \sum_{i=1}^{m-1} \vec{v}_i
    \end{equation*}
\end{definition}

\begin{definition}
    Suppose \(V\) is a vector space over \(F\)
    (not necessarily finite-dimensional),
    and \(S \subseteq V\) some subset.
    Then
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item a vector \(\vec{v} \in V\) (linearly) depends on \(S\)
            if there are finite sets \(\exists {\{\vec{w}_i\}}_{i=1}^r \subset V\),
            \(\exists {\{a_i\}}_{i=1}^r \subset F\),
            such that it can written as a linear combination
            \(\sum_{i=1}^r a_i \vec{w}_i\);
        \item the span (or the \(F\)-span), denoted \(\Span(S)\) or \(\Span_F(S)\),
            is the set of all vectors in \(V\) that depend on \(S\);
        \item \(S\) is (linearly) dependent if \(\exists \vec{v} \in S\)
            such that \(\vec{v}\) depends on \(S \setminus \{\vec{v}\}\),
            and it is (linearly) independent otherwise; and
        \item \(S\) forms a basis of \(V\)
            if \(S\) is (linearly) independent and \(V = \Span(S)\).
    \end{enumerate}
\end{definition}
\begin{lemma}\label{lem:intersection-subspace}
    Suppose \(V\) a vector space, and \(W_i \subseteq V\) subspaces.
    Then \(\bigcap_i W_i\) is also a subspace of \(V\).
\end{lemma}
\begin{proof}
    Apply Lemma~\ref{lem:intersection-subgroup} twice
    exactly like Lemma~\ref{lem:intersection-subring}.
\end{proof}
\begin{proposition}\label{prop:subset-generated-subspace}
    \(\Span(S) = \bigcap_{S \subset W \subset V} W\)
    where \(W\) is a subspace of \(V\).
\end{proposition}
\begin{proof}
    Apply Proposition~\ref{prop:subset-generated-subgroup} twice
    exactly like Proposition~\ref{prop:subset-generated-subring}.
\end{proof}

\begin{lemma}
    \(S\) is dependent if and only if 
    there are distinct \({\{\vec{v}_i\}}_{i=1}^r \subset S\)
    and coefficients \({\{a_i\}}_{i=1}^r \subset F\) that are not all 0
    such that \(\sum_{i=1}^r a_i \vec{v}_i = \vec{0}\).
\end{lemma}
\begin{proof}
    \begin{equation*}
        \vec{v} = \sum_{i=1}^r a_i \vec{v}_i
        \iff \vec{0} = -\vec{v} + \sum_{i=1}^r a_i \vec{v}_i
    \end{equation*}
\end{proof}

\begin{definition}
    Suppose \(U,V\) are vector spaces, and \(\func{f}{U}{V}\) a function.
    \(f\) is linear if \(f(a\vec{u}+\vec{v}) = af(\vec{u}) + f(\vec{v})\),
    i.e.\ it preserves both operations.
\end{definition}
\begin{definition}
    Vector space homomorphisms are linear maps.
\end{definition}
\begin{proposition}
    The set of endomorphisms of a vector space, \(\End(V) = \Hom(V,V)\)
    has both a ring structure and a vector space structure.
\end{proposition}
\begin{proof}
    Let \(\func{0}{V}{V}\) map every vector to \(\vec{0}\),
    and \(\func{1}{V}{V}\) be the identity map.
    Clearly if \(\func{f,g}{V}{V}\),
    \(af(\vec{x}) = f(a\vec{x})\) by the fact that it is a linear map,
    and addition is defined the obvious way.
    If we define ring multiplication as function composition,
    then by linearity we have distributivity.
\end{proof}

\begin{proposition}\label{prop:image-spanning}
    Suppose \(B \subset V\) is spanning.
    Then the image of this set under a linear transformation \(\func{T}{V}{W}\) is spanning.
\end{proposition}
\begin{proof}
    Suppose \(\vec{v} \in V\) is a vector.
    Let it be written as \(\vec{v} = \sum a_i\vec{x}_i\).
    Then \(T\vec{v}\) can be written as a linear combination
    \begin{equation*}
        T\vec{v} = T\pqty{\sum a_i\vec{x}_i} = \sum a_i T\vec{x}_i
    \end{equation*}
\end{proof}
\begin{proposition}\label{prop:image-lin-indep}
    Suppose \(B \subset V\) is linearly independent,
    and \(\func{T}{V}{W}\) is a linear transformation.
    If \(T\) is injective, then the image \(T(B)\) is also linearly independent.
\end{proposition}
\begin{proof}
    Suppose, by way of contradiction,
    that \(T(B)\) is not linearly independent.
    Then there exists some nonzero \(a_i\) such that
    \(\sum a_i T\vec{x}_i = \vec{0}\).
    But then that would imply that \(T(\sum a_i\vec{x}_i) = \vec{0}\),
    and by injectivity, \(\sum a_i\vec{x}_i = \vec{0}\),
    which contradicts linear independence.
\end{proof}


\section{General Constructions}

\subsection*{Direct Sum}

\begin{definition}[Universal Property of Direct Sum]
    Suppose \({\{V_i\}}_{i \in I}\) is a family of vector spaces.
    The direct sum of vector spaces \(\bigoplus_{i \in I} V_i\)
    is the categorical coproduct of vector spaces, that is,
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item there exist embeddings \(\func{\iota_i}{V_i}{\bigoplus_{i \in I} V_i}\); and
        \item for any vector space \(X\),
            if \(\func{\phi_i}{V_i}{X}\) are homomorphisms,
            then there exists a unique \(\func{\bar{\phi}}{\bigoplus_{i \in I} V_i}{X}\)
            such that \(\phi_i = \bar{\phi}\circ\iota_i\).
    \end{enumerate}

    This can be represented by the logical statement
    \begin{equation*}
        \forall X\in\mathbf{Vect},\;
        \forall i \in I,\; \forall \phi_i\in\Hom(V_i,X),\;
        \exists! \phi\in\Hom\qty(\bigoplus_{i \in I} V_i,X),\;
        \phi_i = \bar{\phi}\circ\iota_i
    \end{equation*}
    and the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            V_i \arrow{r}{\phi_i} \arrow{d}{\iota_i} & X \\
            \bigoplus_{i \in I} V_i \arrow[dashrightarrow]{ru}[swap]{\exists! \bar{\phi}}
        \end{tikzcd}
    \end{center}
\end{definition}

\begin{definition}
    Suppose \(U,V\) are \(F\)-spaces.
    The external direct sum \(U \oplus V\)
    is a vector space \(U \times V\)
    equipped with coordinate-wise operations.
\end{definition}
\begin{lemma}
    \(U \oplus V\) indeed forms a vector space.
\end{lemma}
\begin{proof}
    The additive groups is a direct sum of (abelian) groups,
    which is (abelian) group.
    Check scalar multiplication by
    \begin{equation*}
        (ab)(\vec{u},\vec{v}) = ((ab)\vec{u},(ab)\vec{v})
        = (a(b\vec{u}),a(b\vec{v})) = a(b\vec{u},b\vec{v})
        = a(b(\vec{u},\vec{v}))
    \end{equation*}
\end{proof}
\begin{lemma}\label{lem:dim-direct-sum}
    \(\dim_F(U \oplus V) = \dim_F U + \dim_F V\).
\end{lemma}
\begin{proof}
    Suppose \(B_U \subset U\) and \(B_V \subset V\) be bases.
    Let \(B = {\{(\vec{u},\vec{0})\}}_{\vec{u} \in B_U} \sqcup
    {\{(\vec{0},\vec{v})\}}_{\vec{v} \in B_V}\) be a disjoint union.
    We wish to prove that this forms a basis.
    \begin{equation*}
        \sum_i a_i(\vec{u}_i,\vec{0}) + \sum_j b_j(\vec{0},\vec{v}_j)
        = (\vec{0},\vec{0})
    \end{equation*}
    invokes linear independence by each of \(U\) and \(V\),
    so all \(a_i\) and \(b_j\) are zero.
    Then clearly \(B\) spans \((\vec{u},\vec{v})\)
    simply by span of \(B_U\) and \(B_V\).
\end{proof}

\begin{theorem}[Existence of Direct Sum]
    The direct sum of vector spaces \(V_i\)
    is a sum of elements \(x_i \in V_i\), with finite support.
    Addition and scalar multiplication respect usual summation rules.
\end{theorem}
\begin{proof}
    We prove that our vector space of finite sums does fulfill the universal property.
    We shall first prove uniqueness of \(\bar{\phi}\), assuming existence.
    Suppose, by way of contradiction,
    that \(\bar{\phi} \neq \bar{\phi}'\) both fulfill this diagram.
    Then there exists some finite sum \(\sum_{i \in I} \iota_i(x_i)\) such that
    \begin{equation*}
        \sum_{i \in I} \phi_i(x_i)
        = \sum_{i \in I} (\bar{\phi}\circ\iota_i)(x_i)
        = \bar{\phi}\pqty{\sum_{i \in I} \iota_ix(x_i)}
        \neq \bar{\phi}'\pqty{\sum_{i \in I} \iota_i(x_i)}
        = \sum_{i \in I} (\bar{\phi}'\circ\iota_i)(x_i)
        = \sum_{i \in I} \phi_i(x_i)
    \end{equation*}
    which is a contradiction.

    We now prove existence.
    We wish to show that we can define \(\bar{\phi}\) as
    \(\sum \iota_i(x_i) \mapsto \sum \phi_i(x_i)\).
    We first check linearity,
    and that we pad our sums below with zeroes,
    so that they are both sums with same number of terms.
    \begin{align*}
        \bar{\phi}\pqty{a\sum \iota_i(x_i) + \sum \iota_i(y_i)}
        &= \bar{\phi}\pqty{\sum (a\iota_i(x_i)+\iota_i(y_i))}
        = \pqty{\sum (\bar{\phi}\circ\iota_i)(ax_i + y_i)}
        = \sum \phi_i(ax_i + y_i) \\
        &= a\sum \phi_i(x_i) + \sum \phi_i(y_i)
        = a\bar{\phi}\pqty{\sum \iota_i(x_i)} + \bar{\phi}\pqty{\sum \iota_i(y_i)}
    \end{align*}
    The fact that \(\bar{\phi}\circ\iota_i = \phi_i\) is by construction.
\end{proof}
\begin{remark}
    In general, when proving a universal property,
    we start by proving uniqueness assuming existence,
    and then proving existence afterwards.
    This is because the details of the proof of existence
    is often revealed while proving uniqueness.
\end{remark}
\begin{theorem}[Uniqueness of Direct Sum]
    The direct sum of vector spaces is unique up to isomorphism.
\end{theorem}
\begin{proof}
    Suppose, by way of contradiction,
    that there exists some \(W\) that also fits the universal property.
    \begin{center}
        \begin{tikzcd}
            V_i \arrow{r}{\iota_i'} \arrow{d}{\iota_i} &
            W \arrow[rightharpoondown, shift right=0.25ex]{ld}%
                [xshift=.5ex, yshift=-.5ex, swap]{\bar{\phi}'} \\
            \bigoplus_{i \in I} V_i \arrow[rightharpoondown, shift right=0.25ex]{ru}%
                [xshift=-.5ex, yshift=.5ex, swap]{\bar{\phi}}
            % F' \arrow[rightharpoondown, shift right=0.25ex]{ld}%
            %     [xshift=.5ex, yshift=-.5ex, swap]{\bar{\phi}'} \\
            % F \arrow[rightharpoondown, shift right=0.25ex]{ru}%
            %     [xshift=-.5ex, yshift=.5ex, swap]{\bar{\phi}}
        \end{tikzcd}
    \end{center}
    % It is obvious from the universal property that
    % the unique inclusions do not get affected by order or bracketing.
    We can see from the diagram that \((\bar{\phi}'\circ\bar{\phi})\circ\iota_i = \iota_i\),
    so by the uniqueness of \(\bar{\phi}\) and \(\bar{\phi}'\) in this diagram,
    \(\bar{\phi}'\circ\bar{\phi} = \id\).
    The same argument can be used to conclude that \(\bar{\phi}\circ\bar{\phi}' = \id\).
    Hence \(\bar{\phi}^{-1} = \bar{\phi}'\),
    and we have isomorphism.
\end{proof}

\begin{proposition}\label{prop:vsp-sum-iso}
    We have the following canonical isomorphisms:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \((U \oplus V) \oplus W \cong U \oplus V \oplus W \cong U \oplus (V \oplus W)\); and
        \item \(U \oplus V \cong V \oplus U\).
    \end{enumerate}
\end{proposition}
\begin{proof}
    Suppose \(\vec{u} \in U\), \(\vec{v} \in V\), and \(\vec{w} \in W\).
    Then we have the following maps:
    \((\vec{u}+\vec{v})+\vec{w} \mapsto \vec{u}+\vec{v}+\vec{w} \mapsto \vec{u}+(\vec{v}+\vec{w})\).
    Both of these maps are isomorphic,
    because the bases are concatenated from the bases of \(U,V,W\).
    % for \(\vec{u}_i,\vec{v}_j,\vec{w}_k\) basis vectors.

    A similar argument can be made for part (b).
    The bases are both dependent on the choice of \(\vec{u}_i,\vec{v}_j\),
    since the bases this time is concatenated from a different order only.
\end{proof}
% \begin{proposition}
%     \(\dim(\bigoplus_{i=1}^n V_i) = \sum_{i=1}^n \dim(V_i)\).
% \end{proposition}
% \begin{proof}
    
% \end{proof}


\subsection*{Direct Product}

\begin{definition}[Universal Property of Direct Product]
    Suppose \({\{V_i\}}_{i \in I}\) is a family of vector spaces.
    The direct product of vector spaces \(\prod_{i \in I} V_i\)
    is the categorical product of vector spaces, that is,
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item there exists projections \(\func{\pi_i}{\prod_{i \in I} V_i}{V_i}\); and
        \item for any vector space \(X\), if \(\func{\phi_i}{X}{V_i}\) are homomorphisms,
            then there exists a unique \(\func{\bar{\phi}}{X}{\prod_{i \in I} V_i}\)
            such that \(\phi_i = \pi_i\circ\bar{\phi}\).
    \end{enumerate}

    This can be represented by the logical statement
    \begin{equation*}
        \forall X \in \mathbf{Vect},\;
        \forall i \in I,\;
        \forall \phi_i \in \Hom(X,V_i),\;
        \exists! \phi \in \Hom\qty(X,\prod_{i \in I} V_i),\;
        \phi_i = \pi_i \circ \bar{\phi}
    \end{equation*}
    and the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            V_i & X \arrow{l}[swap]{\phi_i}%
                \arrow[dashrightarrow]{ld}{\exists! \bar{\phi}} \\
            \prod_{i \in I} V_i \arrow{u}[swap]{\pi_i}
                % \arrow[dashrightarrow]{ru}[swap]{\exists! \bar{\phi}}
        \end{tikzcd}
    \end{center}
\end{definition}

\begin{theorem}[Existence of Direct Product]
    The direct product of vector spaces is a tuple \({(x_i)}_{i \in I}\) indexed over \(I\),
    with each element \(x_i \in V_i\).
\end{theorem}
\begin{proof}
    We prove that our vector space of tuples does fulfill the universal property.
    We shall first prove uniqueness of \(\bar{\phi}\), assuming existence.
    Suppose, by way of contradiction,
    that \(\bar{\phi} \neq \bar{\phi}'\) both fulfill this diagram.
    Then there exists some element \(x \in X\) such that
    \(\bar{\phi}(x) \neq \bar{\phi}'(x)\).
    But we have, for all \(i \in I\)
    \begin{equation*}
        (\pi_i\circ\bar{\phi})(x) = \phi_i(x) = (\pi_i\circ\bar{\phi}')(x)
    \end{equation*}
    so we have two different elements projecting to the same elements in \(V_i\).
    That is a contradiction, since if all projections agree,
    then the tuples must agree at all positions.

    We now prove existence.
    We wish to show that we can define \(\bar{\phi}\) as
    \(x \mapsto {(\phi_i(x))}_{i \in I}\).
    We first check linearity.
    \begin{equation*}
        \bar{\phi}(ax + y) = {(\phi_i(ax+y))}_{i \in I}
        = {(a\phi_i(x) + \phi_i(y))}_{i \in I}
        = a{(\phi_i(x))}_{i \in I} + {(\phi_i(y))}_{i \in I}
        = a\bar{\phi}(x) + \bar{\phi}(y)
    \end{equation*}
    The fact that \(\pi_i\circ\bar{\phi} = \phi_i\) is by construction.
\end{proof}
\begin{theorem}[Uniqueness of Direct Product]
    The direct product of vector spaces is unique up to isomorphism.
\end{theorem}
\begin{proof}
    Suppose, by way of contradiction, that there exists another vector space \(W\)
    that fits this universal property.
    \begin{center}
        \begin{tikzcd}
            V_i & %
                % \arrow[dashrightarrow]{ld}{\exists! \bar{\phi}} \\
            % \prod_{i \in I} V_i \
            W \arrow[rightharpoondown, shift right=0.25ex]{ld}%
                [xshift=.5ex, yshift=-.5ex, swap]{\bar{\phi}'} \arrow{l}[swap]{\pi_i'} \\
            \prod_{i \in I} V_i \arrow[rightharpoondown, shift right=0.25ex]{ru}%
                [xshift=-.5ex, yshift=.5ex, swap]{\bar{\phi}} \arrow{u}[swap]{\pi_i}
        \end{tikzcd}
    \end{center}
    Again, from the universal property,
    \(\pi_i\circ(\bar{\phi}'\circ\bar{\phi}) = \pi_i\),
    and by uniqueness, \(\bar{\phi}'\circ\bar{\phi} = \id\).
    A similar argument gives \(\bar{\phi}\circ\bar{\phi}' = \id\).
    Hence we have isomorphism.
\end{proof}

\begin{theorem}\label{thm:vsp-sum-prod-iso}
    Suppose \({\{V_i\}}_{i=1}^n\) is a finite family of vector spaces.
    Then the direct sum and direct product of finitely many vector spaces are isomorphic.
    \begin{equation*}
        \bigoplus_{i=1}^n V_i \cong \prod_{i=1}^n V_i
    \end{equation*}
\end{theorem}
\begin{proof}
    The following projections and inclusions clearly give a surjection
    from the direct product to the direct sum.
    \begin{center}
        \begin{tikzcd}
            \prod_{i=1}^n V_i \arrow{r}{\pi_i} &
            V_i \arrow{r}{\iota_i} &
            \bigoplus_{i=1}^n V_i
        \end{tikzcd}
    \end{center}
    It suffices to prove that this is also injective.
    But the \hyperref[thm:pigeonhole]{pigeonhole principle}
    gives us that a surjection of finitely many vector spaces
    must also be an injection.
\end{proof}
\begin{remark}
    In infinite indices,
    the direct sum is a smaller space than the direct product.
    Choosing canonical coordinates,
    the direct sum can only have finite support with respect to its basis,
    but the direct product can have infinite support.
\end{remark}

\begin{proposition}
    We have the following canonical isomorphisms:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \((U \times V) \times W \cong U \times V \times W \cong U \times (V \times W)\); and
        \item \(U \times V \cong V \times U\).
    \end{enumerate}
\end{proposition}
\begin{proof}
    Since Theorem~\ref{thm:vsp-sum-prod-iso} tells us we have an isomorphism
    between the finite direct sum and direct product,
    Proposition~\ref{prop:vsp-sum-iso} suffices.
\end{proof}


\subsection*{Quotient}

\begin{proposition}[Universal Property of Quotients]
    Suppose \(V\) is a vector space, and \(U \subseteq V\) is a subspace,
    and \(\func{\pi}{V}{U}\) is the quotient map as groups.
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(\pi\) is a linear map, and there exists a unique vector space structure on \(V/U\);
        \item For any vector space \(Z\), if we have \(f \in \Hom(V,Z)\) and \(\ker(f) \supseteq U\),
            then there exists a unique linear map \(\bar{f} \in \Hom(V/U,Z)\)
            such that \(\bar{f} \circ g = f\).
    \end{enumerate}

    This can be represented by the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            V \arrow{d}{\pi} \arrow{r}{f} & Z \\
            V/U \arrow[dashrightarrow]{ru}[swap]{\exists! \bar{f}}
        \end{tikzcd}
    \end{center}
\end{proposition}
\begin{proof}
    Suppose \(\vec{x} \in V/U\).
    If \(\vec{x} = \vec{v} + U + \vec{v}' + U\),
    then we can see that \(a\vec{v}-a\vec{v}' = a(\vec{v}-\vec{v}') \in U\),
    which allows us to conclude that \(\pi(a\vec{v}) = \pi(a\vec{v}') = a\pi(\vec{v})\)
    is well-defined.
    The additive axioms are clearly satisfied,
    so it suffices to check distributivity.
    \begin{equation*}
        \pi(a\vec{v}+a\vec{w}) = a\pi(\vec{v}) + a\pi(\vec{w})
        = a\pi(\vec{v} +\vec{w})
    \end{equation*}
\end{proof}

\begin{theorem}[First Isomorphism Theorem for Vector Spaces]\label{thm:iso-1-vsp}
    Suppose \(\func{\phi}{V}{W}\) is a vector space homomorphism,
    and \(U = \ker(\phi)\).
    We have:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(U \subseteq V\), the kernel is a subspace;
        \item \(\phi(V) \subseteq W\), the image is a subspace; and
        \item \(\phi(V) \cong V/U\), the image is uniquely isomorphic to the quotient subspace.
    \end{enumerate}
\end{theorem}
\begin{theorem}[Third and Fourth Isomorphism Theorems for Vector Spaces]\label{thm:iso-3-vsp}\label{thm:iso-4-vsp}
    Suppose \(V\) is a vector space, \(U \subseteq V\) some subspace,
    and \(\func{\pi}{V}{V/U}\) is the quotient homomorphism.
    Then \(\pi\) is a bijection between the subspaces of \(V/U\)
    and the subspaces of \(V\) containing \(U\).
    In particular, if \(W\) is one such intermediate subspace,
    then \((V/U)/(W/U) \cong V/W\).
\end{theorem}
\begin{remark}
    Observe that vector spaces are modules over fields,
    so these isomorphism theorems are direct consequences of
    \hyperref[thm:iso-1-module]{isomorphism theorems on modules},
    which we will give a treatment in Chapter~\ref{sec:modules}.
    However, as the reader might be observed
    from the proofs of the isomorphism theorems
    for \hyperref[thm:iso-1-group]{groups} and \hyperref[thm:iso-1-ring]{rings},
    the direct proof of these theorems follow the exact same line of reasoning.
\end{remark}
% EDIT WHEN UNIVERSAL ALGEBRA WRITTEN


\section{Duality}

\begin{definition}
    Suppose \(V\) is an \(F\)-vector space.
    The dual vector space is \(V' = \Hom_F(V,F)\)
    the set of all homomorphisms into the base field.
    In some other fields of math the dual is sometimes denoted \(V^\ast\).
\end{definition}
\begin{lemma}
    Suppose \(W\) and \(V\) are \(F\)-vector spaces. Then
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(W^V\), the set of all functions \(V \to W\),
            is a vector space under coordinate-wise operations; and
        \item \(\Hom_F(W,V) \subseteq W^V\) is a subspace.
    \end{enumerate}
\end{lemma}
\begin{proof}
    \(W^V = \prod_{\vec{v} \in V} W\),
    which by the universal property of the direct product is a vector space.

    The homomorphisms are clearly a subset,
    so it suffices to prove that it is a vector space.
    The zero morphism is trivially linear,
    so we show that if \(f,g\) are linear,
    \begin{align*}
        (af+g)(b\vec{u}+\vec{v}) &= af(b\vec{u}+\vec{v}) + g(b\vec{u}+\vec{v})
        = baf(\vec{u}) + af(\vec{v}) + bg(\vec{u}) + g(\vec{v}) \\
        &= b(af(\vec{u}) + g(\vec{u})) + af(\vec{v}) + g(\vec{v})
        = b(af+g)(\vec{u}) + (af+g)(\vec{v})
    \end{align*}
\end{proof}
\begin{proposition}
    \({(V/U)}' = \{\phi \in V' : \phi\vert_U = \vec{0}\}\).
\end{proposition}
\begin{proof}
    Suppose \(\phi \in {(V/U)}'\).
    Then we can extend this to \(\phi\circ\pi\),
    which since it factors through \(V/U\),
    the kernel contains \(U\),
    so \({(V/U)}' \subseteq \{\phi \in V' : \phi\vert_U = \vec{0}\}\).
    On the other hand, if \(\phi\vert_U = \vec{0}\),
    by the universal property of quotients,
    we can find a corresponding morphism \(V/U \to F\),
    i.e.\ in \({(V/U)}'\).
\end{proof}

\begin{definition}
    Suppose \(B \subset V\) is a basis.
    The dual basis \(B' \subset V'\) is a set where for each \(\vec{u} \in B\),
    there is a corresponding \(\phi_{\vec{u}} \in B'\)
    such that \(\phi_{\vec{u}}(\vec{v}) = \delta_{\vec{u},\vec{v}}\)
    for all \(\vec{v} \in B\),
    where \(\delta\) is the Kronecker delta.
\end{definition}
\begin{lemma}
    Suppose \(B = {\{\vec{v}_i\}}_{i \in I}\),
    and \(B' = {\{\phi_i\}}_{i \in I}\),
    where \(\phi_i(\vec{v}_j) = \delta_{ij}\).
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(\phi_i(\sum_{j \in I} a_j \vec{v}_j) = a_i\); and
        \item Each \(\phi_i \in B'\) is uniquely defined by choice of \(B\).
    \end{enumerate}
\end{lemma}
\begin{proof}
    \(\phi_i\) are linear functionals, so we have
    \begin{equation*}
        \phi_i\pqty{\sum_{j \in I} a_j \vec{v}_j}
        = \sum_{j \in I} a_j \phi_i(\vec{v}_j)
        = \sum_{j \in I} a_j \delta_{ij}
        = a_i
    \end{equation*}

    Suppose \(\phi_i(\vec{v}_j) = \phi_{i'}(\vec{v}_j) = \delta_{ij}\).
    Since they agree on the basis \(B \subset V\),
    they must also agree on \(V\),
    so \(\phi_i = \phi_{i'}\).
\end{proof}

\begin{lemma}
    The dual basis \(B' \subset V'\) is linearly independent.
\end{lemma}
\begin{proof}
    Suppose \(B'\) not linearly independent.
    Then there exists some \(a_i\) such that \(\sum_{i \in I} a_i \phi_i = 0\).
    But we can evaluate at \(\vec{v}_j \in B\) to conclude that
    \(\sum_{i \in I} a_i \phi_i(\vec{v}_j) = a_j = 0\) for all \(j \in I\).
\end{proof}
\begin{proposition}
    If \(V\) is finite dimensional,
    then \(B' \subset V'\) is a basis.
\end{proposition}
\begin{proof}
    It suffices to prove that it is spanning.
    Suppose \(B = {\{\vec{v}_i\}}_{i=1}^n\),
    and \(\phi \in V'\) some functional.
    For any \(\vec{x} \in V\),
    \(\vec{x} = \sum_{i=1}^n a_i \vec{v}_i\),
    so
    \begin{equation*}
        \phi(\vec{x}) = \sum_{i=1}^n a_i \phi(\vec{v}_i)
        = \sum_{i=1}^n \phi(\vec{v}_i) \phi_i(\vec{x})
    \end{equation*}
    Hence \(\phi = \sum_{i=1}^n \phi(\vec{v}_i) \phi_i\).
\end{proof}
\begin{corollary}
    If \(V\) is finite dimensional,
    then \(\dim V = \dim V'\).
\end{corollary}
\begin{proof}
    The bases are the same size.
\end{proof}
\begin{remark}
    Note that `dual basis' is kind of a misnomer,
    since it does not form a basis in the case of infinite dimensions.
    Rather, the dual basis is the basis for the subspace of \(V'\)
    with finite support.
\end{remark}

\begin{proposition}
    \(\pqty{\bigoplus_i V_i}' \cong \prod_i V_i'\).
\end{proposition}
\begin{proof}
    Suppose \(\phi \in \pqty{\bigoplus_i V_i}'\).
    If we consider the usual restriction of \(\phi\) to each individual \(V_i\),
    and denote each of them \(\phi_i\),
    then the direct product of each of the \(\phi_i\)
    gives us a natural homomorphism
    from \(\pqty{\bigoplus_i V_i}' \to \prod_i V_i'\),
    \(\phi \mapsto (\phi_i)\).

    We shall explicitly construct the inverse as to show that it is an isomorphism.
    Consider \((\psi_i) \in \prod_i V_i'\).
    We can map this to some element \(\sum_i \psi_i\).
    Observe that although there might be infinitely many nonzero \(\psi_i\),
    when applied to \(\vec{x}_i \in V_i\), there are only finitely many nonzero \(\psi_i(\vec{x}_i)\) terms.

    We first check that these two homomorphisms are indeed homomorphisms,
    by checking linearity.
    The forward direction is almost by definition:
    \(a\phi + \chi\) restricts to \(a\phi_i + \chi_i\),
    so \(a\phi + \chi \mapsto (a\phi_i + \chi_i) = a(\phi_i) + (\chi_i)\).
    The reverse direction is similar:
    \(a(\psi_i) + (\omega_i) = (a\psi_i + \omega_i) \mapsto
    \sum_i a\psi_i + \omega_i = a\sum_i \psi_i + \sum_i \omega_i\).

    We then check that it is indeed an inverse.
    \begin{equation*}
        \phi\pqty{\sum_i \vec{x}_i} \overset{f}{\mapsto} (\phi_i(\vec{x}_i))
        = \sum_i \phi_i(\vec{x}_i) = \phi\pqty{\sum_i \vec{x}_i}
    \end{equation*}
\end{proof}

\begin{theorem}
    Suppose \(\vfunc{f}{V}{V''}{\vec{v}}{(\phi\mapsto\phi(\vec{v})) = \delta_{\vec{v}}}\),
    where \(\delta_{\vec{v}}\) is the evaluation map.
    This homomorphism is injective;
    furthermore, it is an isomorphism if and only if \(\dim_F(V) < \infty\).
\end{theorem}
\begin{proof}
    Suppose, by way of contradiction, that there is some \(\vec{v} \neq \vec{0}\)
    such that \(f(\vec{v}) = \delta_{\vec{v}} = 0\);
    this tells us there is some vector \(\vec{v} \in V\)
    such that any \(\phi \in V'\) evaluates it to \(\phi(\vec{v}) = 0\).
    But then, if we choose a basis for \(V\), such that it includes \(\vec{v}\),
    the corresponding dual basis would include some \(\phi_{\vec{v}}\)
    such that \(\phi_{\vec{v}}(\vec{v}) = 1\), which is a contradiction.

    Suppose \(V\) is a finite-dimensional vector space.
    Then it suffices to prove that \(f\) is a surjection.
    Let \({\{\vec{v}_i\}}_{i=1}^n \subset V\) be a basis,
    and \({\{\phi_i\}}_{i=1}^n \subset V'\) be its dual basis.
    Consider any \(\delta \in V''\).
    If we let \(a_i = \delta(\phi_i)\),
    and \(\vec{v} = \sum_{i=1}^n a_i\vec{v}_i\),
    then we see that
    \begin{equation*}
        f(\vec{v}) = \sum_{i=1}^n a_i f(\vec{v}_i)
        = \sum_{i=1}^n \delta(\phi_i) \delta_{\vec{v}_i}
        = \delta
    \end{equation*}
    Hence we have surjection.

    Now suppose \(V\) is an infinite-dimensional vector space.
    Then if \({\{\vec{v}_i\}}_{i \in I} \subset V\) is a basis,
    then \(V = \bigoplus_{i \in I} F\),
    and \(V' = \prod_{i \in I} F\).
    But we can immediately see that in infinite dimensions the dual basis does not form a basis,
    and hence there is no surjection from \(V \to V'\).
    Analogously there is no surjection from \(V' \to V''\).
    Hence there exists no surjection \(V \to V''\).
\end{proof}


\section{Tensor Product}

\subsection*{Bilinearity}

\begin{definition}
    Suppose \(U,V,W\) are vector spaces,
    and \(\func{f}{U \times V}{W}\) is a function.
    We say \(f\) is bilinear if it is linear in each variable.
    Similarly, if \(\func{f}{\prod_{i=1}^n V_i}{W}\) is a function,
    \(f\) is \(n\)-linear if it is linear in each variable.
\end{definition}
\begin{definition}
    We can generalize the inner product of vectors.
    Suppose \(U,V\) are vector spaces,
    with \({\{\vec{u}_i\}}_{i=1}^m,{\{\vec{v}_j\}}_{j=1}^n\) their respective bases.
    Then we can define a Gram matrix to be \(b_{ij} = \ip{\vec{u}_i}{\vec{v}_j}\),
    for some bilinear form \(\func{\ip{\cdot}{\cdot}}{U \times V}{F}\).
    Then the pairing becomes \(\ip{\vec{u}}{\vec{v}} = \vec{u}^T \vb{B} \vec{v}\),
    where \(\vb{B}\) is the Gram matrix.
\end{definition}
\begin{theorem}
    There is a bijection between the set of bilinear maps \(U \times V \to W\)
    and \(\Hom(U,\Hom(V,W))\).
\end{theorem}
\begin{proof}
    Consider a bilinear function \(\func{f}{U \times V}{W}\).
    If we fix any \(\vec{u} \in U\),
    we can define \(\func{f_{\vec{u}} = f(\vec{u},\cdot)}{V}{W}\).
    Then clearly \(f_{\vec{u}} \in \Hom(V,W)\),
    and \(\vec{u} \mapsto f_{\vec{u}}\) is in \(\Hom(U,\Hom(V,W))\).

    Now consider any arbitrary map \(\Hom(U,\Hom(V,W))\).
    Then for each \(\vec{u} \in U\) we have a corresponding \(f_{\vec{u}} \in \Hom(V,W)\),
    and this map is linear in \(U\).
    But \(f_{\vec{u}}\) is exactly the set of linear maps from \(V \to W\),
    so we have linearity in both \(U\) and \(V\) as desired.
\end{proof}
\begin{corollary}
    There is a bijection between the set of pairings \(\func{f}{U \times V}{F}\)
    and \(\Hom(U,V')\).
\end{corollary}
\begin{proof}
    \(\Hom(U,\Hom(V,F)) = \Hom(U,V')\).
\end{proof}
\begin{definition}
    A paring \(U \times V \to F\) is nondegenerate
    if the associated maps \(U \mapsto V'\) and \(V \mapsto U'\) are both embeddings.
\end{definition}

\begin{definition}
    Suppose \(T \in \Hom(U,V)\).
    We define the dual map to be \(T' \in \Hom(V',U')\),
    where we map \(\phi \mapsto \phi \circ T\).
\end{definition}
\begin{proposition}
    We have the following properties for dual maps:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(T \mapsto T'\) is a linear map from \(\Hom(U,V) \to \Hom(V',U')\);
        \item \({(ST)}' = T'S'\); and
        \item if \(\vb{A}\) is the matrix of \(T\), then \(\vb{A}^T\) is the matrix of \(T'\).
    \end{enumerate}
\end{proposition}
\begin{proof}
    Suppose \(S,T \in \Hom(U,V)\).
    Then for any \(\phi \in V'\),
    \begin{equation*}
        (aS + T)'\phi = \phi \circ (aS + T) = a\phi \circ S + \phi \circ T
        = aS' + T'
    \end{equation*}
    This proves statement (a).

    Suppose \(T \in \Hom(U,V)\) and \(S \in \Hom(V,W)\).
    Then for any \(\phi \in W'\),
    \begin{equation*}
        {(ST)}'\phi = \phi \circ ST = (\phi \circ S) \circ T
        = T'(\phi \circ S) = T'S'\phi
    \end{equation*}
    This proves statement (b).

    Suppose \(T \in \Hom(U,V)\), and \(\vb{A}\) is its matrix representation.
    Let \(\phi \in V'\) be a linear functional,
    with a representation \(\phi_i\) as a horizontal covector.
    % and we have bases \({\{\vec{u}_i\}}_{i=1}^m\) and \({\{\vec{v}_j\}}_{j=1}^n\).
    % Let \(a_{ij}\) be the matrix elements of \(\vb{A}\),
    % and \(\vec{u} = \sum_{k=1}^m b_k\vec{u}_k\).
    % We have
    % \begin{equation*}
    %     T\vec{u} =  \sum_{i,j} a_{ij}b_j\vec{v}_i
    % \end{equation*}
    Then we have
    \begin{equation*}
        \pqty{\phi\vb{A}}^T
        = \vb{A}^T\phi^T
    \end{equation*}
    where the left multiplication is by \(\vb{A}^T\).
    This proves statement (c).
\end{proof}
\begin{remark}
    Although we have not proven all the corresponding properties,
    we can see intuitively that taking the dual is a contravariant functor.
    Functoriality of \(\Hom(\cdot,F)\) will be discussed in more generality
    in Section~\ref{sec:comm-modules} for modules over commutative rings.
\end{remark}
% \begin{proposition}
%     Suppose \(T \in \End(V,V)\) is arbitrary.
%     Then there exists a unique nondegenerate pairing \(\func{\ip{\cdot}{\cdot}}{V' \times V}{F}\)
%     such that \(\ip{\phi}{T\vec{v}} = \ip{T'\phi}{\vec{v}}\) holds
%     for all \(\vec{v} \in V\) and \(\phi \in V'\).
% \end{proposition}
% \begin{proof}
    
% \end{proof}

\subsection*{Tensor Product}

\begin{definition}[Universal Property of Tensor Product]
    Suppose \({\{V_i\}}_{i \in I}\) is a family of vector spaces.
    The tensor product of vector spaces \(\bigotimes_{i \in I} V_i\)
    is the universal construction for all \(I\)-linear maps, that is,
    \begin{enumerate}[label={(\roman*)}, itemsep=0mm]
        \item there exists a bilinear embedding \(\func{\iota}{\prod_{i \in I} V_i}{\bigotimes_{i \in I} V_i}\); and
        \item for any vector space \(X\),
            if \(\func{\phi}{\prod_{i \in I} V_i}{X}\) is a bilinear map,
            then there exists a unique \(\func{\bar{\phi}}{\bigotimes_{i \in I} V_i}{X}\)
            such that \(\phi = \bar{\phi}\circ\iota\).
    \end{enumerate}

    This can be represented by the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            \prod_{i \in I} V_i \arrow{r}{\phi} \arrow{d}{\iota} & X \\
            \bigotimes_{i \in I} V_i \arrow[dashrightarrow]{ru}[swap]{\exists! \bar{\phi}}
        \end{tikzcd}
    \end{center}
\end{definition}

\begin{theorem}[Uniqueness of Tensor Product]
    The tensor product is unique up to isomorphism.
\end{theorem}
\begin{proof}
    Suppose, by way of contradiction, that there exists another vector space \(W\)
    that fits that universal property.
    \begin{center}
        \begin{tikzcd}
            \prod_{i \in I} V_i \arrow{r}{\iota'} \arrow{d}{\iota} &
            W \arrow[rightharpoondown, shift right=0.25ex]{ld}%
                [xshift=.5ex, yshift=-.5ex, swap]{\bar{\phi}'} \\
            \bigotimes_{i \in I} V_i \arrow[rightharpoondown, shift right=0.25ex]{ru}%
                [xshift=-.5ex, yshift=.5ex, swap]{\bar{\phi}}
        \end{tikzcd}
    \end{center}
    A standard argument via diagram chase gives us
    \begin{align*}
        (\bar{\phi}'\circ\bar{\phi})\circ\iota = \iota &\implies \bar{\phi}'\circ\bar{\phi} = \id \\
        (\bar{\phi}\circ\bar{\phi}')\circ\iota' = \iota' &\implies \bar{\phi}\circ\bar{\phi}' = \id
    \end{align*}
    by uniqueness of \(\bar{\phi},\bar{\phi}'\).
    This gives us isomorphism.
\end{proof}

\begin{theorem}[Existence of Tensor Product]
    Suppose \({\{V_i\}}_{i \in I}\) is a family of vector spaces,
    Consider the vector space \(T\) of formal linear combinations of vectors
    \({\{{(\vec{v}_i)}_{i \in I} : \vec{v}_i \in V_i\}}\).
    We then construct the following equivalence classes,
    where the vectors below only differ in the \(i\)th index.
    \begin{equation*}
        (\hdots,\vec{u}_i+\vec{v}_i,\hdots) = (\hdots,\vec{u}_i,\hdots) + (\hdots,\vec{v}_i,\hdots) \qquad
        (\hdots,a\vec{v}_i,\hdots) = a(\hdots,\vec{v}_i,\hdots)
    \end{equation*}
    Then \(T/{\sim}\) is the tensor product,
    with the image of \({(\vec{v}_i)}_{i \in I}\) written as \(\bigotimes_{i \in I} \vec{v}_i\).
\end{theorem}
\begin{proof}
    We first remember that the equivalence class \(\sim\) indeed forms a vector space,
    since by construction,
    all linear combinations of elements will be in the equivalence class.
    Hence \(T/{\sim}\) is indeed a valid quotient vector space.

    We now show that \(T/{\sim}\) indeed fulfills our universal property.
    First off, we define our embedding \(\iota\) as
    \({(\vec{v}_i)}_{i \in I} \mapsto \bigotimes_{i \in I} \vec{v}_i\).
    By definition of our equivalence class, \(\iota\) is \(I\)-linear.
    We can also see that since \(\{{(\vec{v}_i)}_{i \in I} : \vec{v}_i \in V_i\}\) span \(T\),
    its image must also span \(\{\bigotimes_{i \in I} \vec{v}_i : \vec{v}_i \in V_i\}\) span \(T/{\sim}\)
    We show that if such a \(\bar{\phi}\) exists, it must be unique.
    Suppose, by way of contradiction,
    that \(\bar{\phi}\neq\bar{\phi}'\) both fit our universal property.
    Then we know that there exists some \(\vec{w} \in T/{\sim}\)
    that \(\bar{\phi}\) and \(\bar{\phi}'\) disagree on;
    in particular, if we write \(\vec{w}\) as a sum of some spanning tensors,
    \(\bar{\phi}\) and \(\bar{\phi}'\) disagree on at least one of those tensors.
    Hence
    \begin{equation*}
        \phi{(\vec{v}_i)}_{i \in I}
        = (\bar{\phi}\circ\iota){(\vec{v}_i)}_{i \in I}
        = \bar{\phi}\pqty{\bigotimes_{i \in I} \vec{v}_i}
        \neq \bar{\phi}'\pqty{\bigotimes_{i \in I} \vec{v}_i}
        = (\bar{\phi}'\circ\iota){(\vec{v}_i)}_{i \in I}
        = \phi{(\vec{v}_i)}_{i \in I}
    \end{equation*}
    is a contradiction.

    We then show existence of such a \(\bar{\phi}\).
    We define \(\bar{\phi}\) (on a spanning set) to be
    \(\bigotimes_{i \in I} \vec{v}_i \mapsto \phi{(\vec{v}_i)}_{i \in I}\),
    and any sums of that to sums of the image.
    We check that this is well-defined,
    by checking that the elements in the kernel \(\sim\) must go to 0.
    \begin{align*}
        \bar{\phi}(\iota(\hdots,\vec{u}_i+\vec{v}_i,\hdots))
        &= \phi(\hdots,\vec{u}_i+\vec{v}_i,\hdots)
        = \phi(\hdots,\vec{u}_i,\hdots) + \phi(\hdots,\vec{u}_i,\hdots) \\
        &= \bar{\phi}(\iota(\hdots,\vec{u}_i,\hdots))
        + \bar{\phi}(\iota(\hdots,\vec{v}_i,\hdots)) \\
        \bar{\phi}(\iota(\hdots,a\vec{v}_i,\hdots))
        &= \phi(\hdots,a\vec{v}_i,\hdots)
        = a\phi(\hdots,\vec{v}_i,\hdots)
        = a\bar{\phi}(\iota(\hdots,\vec{v}_i,\hdots))
    \end{align*}
    We show linearity.
    \begin{equation*}
        \bar{\phi}\pqty{a\sum_j \bigotimes_{i \in I} \vec{v}_{ij}}
        % = \bar{\phi}\pqty{\sum_j a\bigotimes_{i \in I} \vec{v}_{ij}}
        = \sum_j \bar{\phi}\pqty{a\bigotimes_{i \in I} \vec{v}_{ij}}
        = \sum_j a\phi{(\vec{v}_{ij})}_{i \in I}
        = a \sum_j \bar{\phi}\pqty{\bigotimes_{i \in I} \vec{v}_{ij}}
        = a \bar{\phi}\pqty{\sum_j \bigotimes_{i \in I} \vec{v}_{ij}}
    \end{equation*}
    The sum part of linearity is true by definition.
    Lastly, we see that our \(\bar{\phi}\) fulfills the diagram by construction.
\end{proof}

\begin{proposition}\label{prop:tensor-basis}
    Suppose \(B_U \subset U\) and \(B_V \subset V\) are bases.
    Then \(\{\vec{u}\otimes\vec{v}: \vec{u} \in B_U, \vec{v} \in B_V\}\) is a basis.
\end{proposition}
\begin{proof}
    By construction, this is already spanning,
    because the only possible pure tensors are selected from \(\vec{u}\otimes\vec{v}\),
    where \(\vec{u} \in U\) and \(\vec{v} \in V\),
    and each of these can be expressed as a linear combination of the bases vectors.
    We now see that this is also linearly independent,
    because if not,
    then it implies that there are some linear combination of tensors
    that bilinear forms will always evaluate to zero,
    which is strictly untrue.
\end{proof}
\begin{corollary}\label{cor:dim-tensor-product}
    \(\dim(U \otimes V) = \dim(U)\dim(V)\).
\end{corollary}
\begin{proof}
    Simply take the cardinality of the basis.
\end{proof}


\section{Symmetry and Antisymmetry}

\begin{remark}
    In the following section,
    suppose all fields are of characteristic not 2,
    unless otherwise noted.
\end{remark}

\begin{definition}
    Suppose \(T \in \End(V)\) is a linear operator.
    We say \(\lambda\) is an eigenvalue of \(T\)
    if there exists some \(\vec{v} \neq \vec{0}\) such that \(T\vec{v} = \lambda\vec{v}\).
    The set of all eigenvalues of \(T\) is its spectrum, \(\Spec(T)\).
\end{definition}
\begin{definition}
    Suppose \(T \in \End(V)\) is a linear operator.
    If there exists some \(n > 0\) such that \(T^n = 0\),
    then we say that \(T\) is nilpotent of index \(n\).
    Similarly, if there exists some \(n > 0\) such that \(T^m = \id\),
    then we say that \(T\) is idempotent of index \(m\).
\end{definition}

\begin{lemma}\label{lem:direct-sum-idempotent}
    Suppose \(T \in \End(V)\) is a linear operator,
    with \(T \neq \id\), and \(T^2 = \id\).
    Then \(\Spec(T) = \{\pm 1\}\), and \(V = V_+ \oplus V_-\),
    where \(V_+ = \{\vec{v}: T\vec{v} = \vec{v}\}\)
    and \(V_- = \{\vec{v}: T\vec{v} = -\vec{v}\}\).
\end{lemma}
\begin{proof}
    Let \(P_+ = \frac{1}{2}(T+I)\).
    Then \(TP_+ = \frac{1}{2}(T^2+T) = \frac{1}{2}(I+T) = P_+\),
    and \(P_+^2 = \frac{1}{4}(T^2+2T+I) = \frac{1}{4}(2T+2I) = P_+\).
    Hence \(P_+\) is a projection operator.
    In particular, if \(\vec{v} \in \img(P_+)\),
    then there exists \(\vec{u}\) such that \(P_+\vec{u} = \vec{v}\);
    we have \(\vec{v} = P_+\vec{u} = TP_+\vec{u} = T\vec{v}\)
    so \(\vec{v} \in V_+\).
    Therefore \(\img(P_+) \subseteq V_+\).

    Similarly, let \(P_- = \frac{1}{2}(I-T)\).
    Then \(TP_- = \frac{1}{2}(T-T^2) = \frac{1}{2}(T-I) = -P_-\),
    and \(P_-^2 = \frac{1}{4}(I-2T+T^2) = \frac{1}{4}(2I-2T) = P_-\).
    Hence \(P_-\) is also a projection operator.
    In particular, if \(\vec{v} \in \img(P_-)\),
    then there exists \(\vec{u}\) such that \(P_-\vec{u} = \vec{v}\);
    we have \(\vec{v} = P_-\vec{u} = -TP_-\vec{u} = -T\vec{v}\),
    so \(\vec{v} \in V_-\).
    Therefore \(\img(P_-) \subseteq V_-\).

    Observe that \(P_+ = \id - P_-\).
    Now, we see that for any \(\vec{v} \in V\),
    \(\vec{v} = P_+\vec{v} + (\id - P_+)\vec{v} = P_+\vec{v} + P_-\vec{v}\),
    so we have \(V \subseteq \img(P_+) \oplus \img(P_-)\)
    (we cannot be sure that they are equal,
    since we have not proven whether this representation is unique or not).
    But then we see that
    \(V \subseteq \img(P_+) \oplus \img(P_-) \subseteq V_+ \oplus V_- \subseteq V\).
    We are sure that \(V_+,V_-\) are both nonzero
    because by defintion, \(P_+,P_-\) are nonzero.
\end{proof}
\begin{definition}
    Suppose \(U\) is a vector space,
    and \(\sigma \in \End(U \otimes U)\) is an involution,
    i.e.\ it maps \(\vec{u}\otimes\vec{v} \mapsto \vec{v}\otimes\vec{u}\).
    Then the symmetric product is
    \(\Sym^2 U = \{\vec{t} \in U \otimes U: \sigma(\vec{t}) = \vec{t}\}\),
    and the antisymmetric product is
    \(\Asym^2 U = \{\vec{t} \in U \otimes U: \sigma(\vec{t}) = -\vec{t}\}\).
\end{definition}
\begin{proposition}\label{prop:sum-sym-wedge}
    \(U \otimes U \cong \Sym^2 U \oplus \Asym^2 U\).
\end{proposition}
\begin{proof}
    Simple application of Lemma~\ref*{lem:direct-sum-idempotent}.
\end{proof}

\begin{proposition}
    Suppose \(B \subset U\) is a basis.
    Then \({\{\frac{1}{2}(\vec{u}\otimes\vec{v} + \vec{v}\otimes\vec{u})\}}_{\vec{u},\vec{v} \in B}\)
    spans \(\Sym^2 U\),
    and \({\{\frac{1}{2}(\vec{u}\otimes\vec{v} - \vec{v}\otimes\vec{u})\}}_{\vec{u},\vec{v} \in B}\)
    spans \(\Asym^2 U\).
\end{proposition}
\begin{proof}
    Consider any \(\vec{t} \in \Sym^2 U\).
    Write \(\vec{t} = \sum_i a_i\vec{u}_i\otimes\vec{v}_i\),
    where \(\{\vec{u}_i,\vec{v}_i\} \subset B\).
    But we have
    \begin{equation*}
        \sigma(\vec{t}) = \sum_i a_i\vec{v}_i\otimes\vec{u}_i
        = \sum_i a_i\vec{u}_i\otimes\vec{v}_i = \vec{t}
    \end{equation*}
    Hence we can write
    \begin{equation*}
        \vec{t} = \frac{1}{2} \sum_i a_i(\vec{u}_i\otimes\vec{v}_i + \vec{v}_i\otimes\vec{u}_i)
    \end{equation*}

    Similarly, if \(\vec{t} \in \Asym^2 U\), we have
    \begin{equation*}
        \sigma(\vec{t}) = -\sum_i a_i\vec{v}_i\otimes\vec{u}_i
        = \sum_i a_i\vec{u}_i\otimes\vec{v}_i = \vec{t}
    \end{equation*}
    Hence we can write
    \begin{equation*}
        \vec{t} = \frac{1}{2} \sum_i a_i(\vec{u}_i\otimes\vec{v}_i - \vec{v}_i\otimes\vec{u}_i)
    \end{equation*}
\end{proof}
\begin{corollary}\label{cor:dim-sym-wedge}
    If \(\dim U = n\),
    then \(\dim \Sym^2 U = \frac{1}{2}n(n+1)\),
    and \(\dim \Asym^2 U = \frac{1}{2}n(n-1)\).
\end{corollary}
\begin{proof}
    Let \({\{\vec{u}_i\}}_{i=1}^n \subset U\) be a basis
    Just by counting the cardinality of the spanning set,
    we know that for \(\Sym^2 U\),
    since swapping \(i,j\) gives us the same vector,
    \(\vec{u}_1\) gives us \(n\) choices of \(\vec{v}_j\),
    \(\vec{u}_2\) gives us \(n-1\) choices of \(\vec{v}_j\),
    and so on;
    hence \(\dim \Sym^2 U \leq 1 + 2 + \cdots + n = \frac{1}{2}n(n+1)\).
    For \(\Asym^2 U\),
    the same holds true, but additionally, we cannot have \(i = j\),
    since that tensor evaluates to zero;
    hence \(\dim \Asym^2 U \leq 1 + 2 + \cdots + (n-1) + n - n
    = 1 + 2 + \cdots + (n-1) = \frac{1}{2}(n-1)(1+n-1) = \frac{1}{2}n(n-1)\).
    But Proposition~\ref{prop:sum-sym-wedge} tells us that
    \(U \otimes U \cong \Sym^2 U \oplus \Asym^2 U\)
    so by Lemma~\ref{lem:dim-direct-sum} and Corollary~\ref{cor:dim-tensor-product},
    \begin{equation*}
        n^2 = \dim(U \otimes U) = \dim \Sym^2 U + \dim \Asym^2 U
        \leq \frac{1}{2}(n(n+1) + n(n-1)) = \frac{1}{2}(2n^2 + n - n) = n^2
    \end{equation*}
    and we have found exactly the minimal spanning set.
\end{proof}
\begin{corollary}
    If \(U\) is a finite-dimensional vector space,
    and \({\{\vec{u}_i\}}_{i=1}^n \subset U\) is a basis,
    then \(\{\frac{1}{2}(\vec{u}_i\otimes\vec{u}_j+\vec{u}_j\otimes\vec{u}_i)\}_{i \leq j} \subset \Sym^2 U\)
    and \(\{\frac{1}{2}(\vec{u}_i\otimes\vec{u}_j-\vec{u}_j\otimes\vec{u}_i)\}_{i<j} \subset \Asym^2 U\)
    are bases.
\end{corollary}
\begin{proof}
    Restatement of minimality of spanning set from the
    \hyperref[cor:dim-sym-wedge]{corollary above}.
\end{proof}
\begin{remark}
    The cross product is an antisymmetric product of two vectors in \(\bR^3\).
    Observe that \(\dim \Asym^2 \bR^3 = 3\),
    which is exactly why the cross product can only be defined,
    for two vectors, in three dimensions;
    it is a coincidence.
\end{remark}

\begin{lemma}
    Suppose \(U\) is a vector space.
    We can define an action of \(S_n\) on \(\bigotimes_{i=1}^n U\),
    via \(\sigma(\bigotimes_{i=1}^n \vec{u}_i) = \bigotimes_{i=1}^n \vec{u}_{\sigma(i)}\),
    where \(\sigma \in S_n\).
\end{lemma}
\begin{proof}
    Identity, inverse, and associativity are obvious.
    Group multiplication is given by
    \begin{equation*}
        \sigma\pqty{\tau\pqty{\bigotimes_{i=1}^n \vec{u}_i}}
        = \sigma\pqty{\bigotimes_{i=1}^n \vec{u}_{\tau(i)}}
        = \sigma\pqty{\bigotimes_{i=1}^n \vec{u}_{\sigma\tau(i)}}
        = (\sigma\tau)\pqty{\bigotimes_{i=1}^n \vec{u}_i}
    \end{equation*}
\end{proof}
\begin{definition}
    We can generalize the symmetric product to be
    \begin{equation*}
        \Sym^n U = \Bqty{\vec{t} \in \bigotimes_{i=1}^n U: \forall\sigma \in S_n,\,\sigma(\vec{t}) = \vec{t}}
    \end{equation*}
    Similarly, we can generalize the antisymmetric product to be
    \begin{equation*}
        \Asym^n U = \Bqty{\vec{t} \in \bigotimes_{i=1}^n U: \forall\sigma \in S_n,\,\sigma(\vec{t}) = \sgn(\sigma)\vec{t}}
    \end{equation*}
\end{definition}
\begin{definition}
    Given the symmetric and antisymmetric products, \(\Sym^n U\) and \(\Asym^n U\),
    we can define the associated projection operators
    \begin{equation*}
        P_+\vec{t} = \frac{1}{n!} \sum_{\sigma \in S_n} \sigma(\vec{t}) \qquad
        P_-\vec{t} = \frac{1}{n!} \sum_{\sigma \in S_n} \sgn(\sigma) \sigma(\vec{t})
    \end{equation*}
    These are often called the symmetrization and the antisymmetrization of \(\vec{t}\).
\end{definition}
\begin{proposition}\label{prop:symmetrization-prop}
    We have the following properties:
    \begin{enumerate}[label={(\alph*)}, itemsep=0mm]
        \item \(P_+\) and \(P_-\) are both projection operators,
            i.e.\ \(P_+^2 = P_+\) and \(P_-^2 = P_-\);
        \item \(\img(P_+) \subseteq \Sym^n U\); and
        \item \(\img(P_-) \subseteq \Asym^n U\).
    \end{enumerate}
\end{proposition}
\begin{proof}
    We first observe that, for any \(\tau \in S_n\),
    \begin{equation*}
        \tau P_+ = \frac{1}{n!} \sum_{\sigma \in S_n} \tau\sigma
        = \frac{1}{n!} \sum_{\tilde{\sigma} \in S_n} \tilde{\sigma}
        = P_+
    \end{equation*}
    after a change of variables \(\tilde{\sigma} = \tau\sigma\).
    Similarly, we have
    \begin{equation*}
        \tau P_- = \frac{1}{n!} \sum_{\sigma \in S_n} \sgn(\sigma) \tau\sigma
        = \frac{1}{n!} \sum_{\tilde{\sigma} \in S_n} \sgn(\tau^{-1}\tilde{\sigma}) \tilde{\sigma}
        = \sgn(\tau^{-1}) \frac{1}{n!} \sum_{\tilde{\sigma} \in S_n} \sgn(\tilde{\sigma}) \tilde{\sigma}
        = \sgn(\tau) P_-
    \end{equation*}
    We can then write the symmetrization as
    \begin{equation*}
        P_+^2 = \frac{1}{n!} \sum_{\sigma \in S_n} \sigma P_+
        = \frac{1}{n!} \sum_{\sigma \in S_n} P_+ = P_+
    \end{equation*}
    and the antisymmetrization as
    \begin{equation*}
        P_-^2 = \frac{1}{n!} \sum_{\sigma \in S_n} \sgn(\sigma) \sigma P_-
        = \frac{1}{n!} \sum_{\sigma \in S_n} {\sgn(\sigma)}^2 P_- = P_-
    \end{equation*}
    This proves statement (a).

    Consider \(\vec{t} \in \img(P_+)\).
    Then there exists some \(\vec{u}\) such that \(\vec{t} = P_+\vec{u}\).
    For any \(\sigma \in S_n\),
    \begin{equation*}
        \sigma(\vec{t}) = (\sigma P_+)\vec{u} = P_+\vec{u} = \vec{t}
    \end{equation*}
    so \(\vec{t} \in \Sym^n U\).
    This proves statement (b).

    Consider \(\vec{t} \in \img(P_-)\).
    Then there exists some \(\vec{u}\) such that \(\vec{t} = P_-\vec{u}\).
    For any \(\sigma \in S_n\),
    \begin{equation*}
        \sigma(\vec{t}) = (\sigma P_-)\vec{u} = \sgn(\sigma) P_-\vec{u} = \sgn(\sigma)\vec{t}
    \end{equation*}
    so \(\vec{t} \in \Asym^n U\).
    This proves statement (c).
\end{proof}
\begin{remark}
    Even though \(\bigotimes_{i=1}^m (\bigotimes_{i=1}^n U) \cong \bigotimes_{i=1}^{mn} U\),
    \(\Sym^2 \Sym^2 U \ncong \Sym^4 U\),
    since the group associated with \(\Sym^2 \Sym^2\) is \(V_4\), not \(S_4\).
\end{remark}

\begin{lemma}\label{lem:antisym-same-zero}
    \(P_-(\vec{u}_1 \otimes \cdots \otimes \vec{u}_n) = \vec{0}\)
    if \(\vec{u}_i = \vec{u}_j\) for some \(i \neq j\).
\end{lemma}
\begin{proof}
    Consider the transposition \(\tau = (ij)\), with \(\sgn(\tau) = -1\).
    Then we have, from Proposition~\ref{prop:symmetrization-prop}a,
    \begin{equation*}
        -P_-(\vec{u}_1 \otimes \cdots \otimes \vec{u}_n)
        = (\tau P_-)(\vec{u}_1 \otimes \cdots \otimes \vec{u}_n)
        = (P_- \tau)(\vec{u}_1 \otimes \cdots \otimes \vec{u}_n)
        = P_- (\vec{u}_1 \otimes \cdots \otimes \vec{u}_n)
    \end{equation*}
    since \(\tau P_- = P_- \tau^{-1} = P_- \tau\) by Proposition~\ref{prop:sn-conjugation}.
\end{proof}
\begin{theorem}\label{thm:antisym-lin-dep-zero}
    Suppose \({\{\vec{u}_i\}}_{i=1}^n\) is a linearly dependent set of vectors.
    Then \(P_-(\vec{u}_1 \otimes \cdots \otimes \vec{u}_n) = \vec{0}\).
\end{theorem}
\begin{proof}
    We write \(\vec{u}_n = \sum_{i=1}^{n-1} a_i\vec{u}_i\).
    Then
    \begin{equation*}
        P_-(\vec{u}_1 \otimes \cdots \otimes \vec{u}_n)
        = \sum_{i=1}^{n-1} a_i P_-(\vec{u}_1 \otimes \cdots \otimes \vec{u}_{n-1} \otimes \vec{u}_i)
        = \vec{0}
    \end{equation*}
    by the \hyperref[lem:antisym-same-zero]{lemma above}.
\end{proof}

\begin{lemma}\label{lem:tensor-n-basis}
    Suppose \({\{\vec{v}_i\}}_{i \in I} \subset V\) is a basis.
    Then we can define \(\vec{v}_f = \vec{v}_{f(1)} \otimes \cdots \otimes \vec{v}_{f(n)}\)
    for any \(\func{f}{\{1,\hdots,n\}}{I}\),
    and \({\{\vec{v}_f: f \in I^{\{1,\hdots,n\}}\}} \subset \bigotimes_{i=1}^n V\)
    forms a basis.
\end{lemma}
\begin{proof}
    The case of \(n = 2\) is given by Proposition~\ref{prop:tensor-basis}.
    We now proceed inductively,
    supposing that the case of \(n = k\) holds true.
    Applying Proposition~\ref{prop:tensor-basis} again,
    since we can choose any \(f \in I^{\{1,\hdots,k\}}\) for the first \(k\) indices,
    and we also have complete freedom in the choice of the last index \(f \in I^{\{k+1\}}\),
    we have the case of \(k+1\).
\end{proof}
\begin{theorem}\label{thm:sym-antisym-basis}
    Suppose \({\{\vec{v}_i\}}_{i \in I} \subset V\) is a basis.
    Then \(\{P_+(\vec{v}_f): f \in I^{\{1,\hdots,n\}},\,f(1) \leq \cdots \leq f(n)\} \subset \Sym^n V\)
    and \(\{P_-(\vec{v}_f): f \in I^{\{1,\hdots,n\}},\,f(1) < \cdots < f(n)\} \subset \Asym^n V\)
    are bases.
\end{theorem}
\begin{proof}
    From the \hyperref[lem:tensor-n-basis]{lemma above},
    we already know that \(\{P_+(\vec{v}_f): f \in I^{\{1,\hdots,n\}}\} \subset \Sym^n V\)
    and \(\{P_-(\vec{v}_f): f \in I^{\{1,\hdots,n\}}\} \subset \Asym^n V\) are spanning
    (Proposition~\ref{prop:image-spanning}).
    Moreover, if the images under \(P_\pm\) are different,
    we will have linear independence (Proposition~\ref{prop:image-lin-indep});
    so it suffices for us to remove all the duplicates.
    
    Consider any \(f\) that is not non-decreasing.
    Then we can rearrange the image of \(f\) into a non-decreasing one,
    and call that function \(f'\).
    Since by definition, \(P_\pm\) includes all possible rearrangements in the sum,
    \(P_\pm(\vec{v}_f) = \pm P_\pm(\vec{v}_{f'})\).
    Now consider any two \(f \neq f'\) that are both non-decreasing.
    The multiset \({\{\vec{v}_{f(i)}\}}_{i=1}^n\) images are different,
    so the images under \(P_\pm\) must also differ.

    Lastly, we remember Lemma~\ref{lem:antisym-same-zero} holds,
    which gives us that only the increasing ones are nonzero for \(P_-\).
\end{proof}
\begin{corollary}
    Suppose \(\dim V = n\).
    Then \(\dim \Asym^k V = \binom{n}{k}\) and \(\dim \Sym^k V = \binom{n+k-1}{k}\).
\end{corollary}
\begin{proof}
    Since there are no duplicates in the multiset images of \(P_-\),
    we are simply choosing \(k\) distinct elements.

    On the other hand, we need to count duplicates in \(P_+\).
    This is equivalent to the number of multisets of size \(k\),
    chosen from a set of \(n\) elements.
    Let \(a_i\) be the number of occurences of \(i \in \{1,\hdots,n\}\).
    We want to count the number of solutions to the equation \(\sum a_i = k\).
    Organize these multisets in ascending order,
    and imagine us putting a divider every time the element changes,
    as in there are \(n-1\) dividers.
    Then we can now view this as a sequence of \(k\) numeric elements and \(n-1\) dividers,
    with a total of \(n+k-1\) things.
    The number of ways to put in these \(n-1\) dividers and \(k\) numeric elements
    is exactly \(\binom{n+k-1}{k}\).
\end{proof}
\begin{remark}
    If \(k > n\), we have \(\Asym^k V = \{\vec{0}\}\).
    Moreover, \(\dim \Asym^n V = 1\),
    and hence \(\dim \End(\Asym^n V) = 1\).
\end{remark}

\begin{definition}
    Suppose \(\dim V = n\), and \(T \in \End(V)\).
    Then \(\Asym^k T \in \End(\Asym^k V)\),
    which is defined to be an element-wise application of \(T\).
\end{definition}
\begin{definition}
    Consider the natural isomorphism \(\func{\phi}{\End(\Asym^n V)}{F}\).
    Then we define the determinant to be \(\det T = \phi(\Asym^n T)\).
\end{definition}
\begin{proposition}
    This definition of the determinant coincides
    with the universal property in Section~\ref{sec:matrix-rings}.
\end{proposition}
\begin{proof}
    Let \({\{\vec{v}_i\}}_{i=1}^n \subset V\) be a basis.
    First off, \(\det \id = 1\),
    since \(\id(\vec{v}_1 \wedge \cdots \wedge \vec{v}_n) = \vec{v}_1 \wedge \cdots \wedge \vec{v}_n\).
    Secondly, by definition of the antisymmetric product,
    \((\sigma T)(\vec{v}_1 \wedge \cdots \wedge \vec{v}_n) = \sgn(\sigma) T(\vec{v}_1 \wedge \cdots \wedge \vec{v}_n)\),
    so \(\det(\sigma T) = \sgn(\sigma)\det(T)\).
    Lastly, \(n\)-linearity is given by the definition of the tensor product.
\end{proof}


\section{Minimal Polynomial}


\section{Jordan Canonical Form}
